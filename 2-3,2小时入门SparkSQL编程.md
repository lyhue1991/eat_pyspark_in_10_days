# 2-3,2å°æ—¶å…¥é—¨SparkSQLç¼–ç¨‹


<!-- #region -->
æœ¬èŠ‚å°†ä»‹ç»SparkSQLç¼–ç¨‹åŸºæœ¬æ¦‚å¿µå’ŒåŸºæœ¬ç”¨æ³•ã€‚

ä¸åŒäºRDDç¼–ç¨‹çš„å‘½ä»¤å¼ç¼–ç¨‹èŒƒå¼ï¼ŒSparkSQLç¼–ç¨‹æ˜¯ä¸€ç§å£°æ˜å¼ç¼–ç¨‹èŒƒå¼ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡SQLè¯­å¥æˆ–è€…è°ƒç”¨DataFrameçš„ç›¸å…³APIæè¿°æˆ‘ä»¬æƒ³è¦å®ç°çš„æ“ä½œã€‚

ç„¶åSparkä¼šå°†æˆ‘ä»¬çš„æè¿°è¿›è¡Œè¯­æ³•è§£æï¼Œæ‰¾åˆ°ç›¸åº”çš„æ‰§è¡Œè®¡åˆ’å¹¶å¯¹å…¶è¿›è¡Œæµç¨‹ä¼˜åŒ–ï¼Œç„¶åè°ƒç”¨ç›¸åº”åŸºç¡€å‘½ä»¤è¿›è¡Œæ‰§è¡Œã€‚

æˆ‘ä»¬ä½¿ç”¨pysparkè¿›è¡ŒRDDç¼–ç¨‹æ—¶ï¼Œåœ¨Excutorä¸Šè·‘çš„å¾ˆå¤šæ—¶å€™å°±æ˜¯Pythonä»£ç ï¼Œå½“ç„¶ï¼Œå°‘æ•°æ—¶å€™ä¹Ÿä¼šè·‘javaå­—èŠ‚ç ã€‚

ä½†æˆ‘ä»¬ä½¿ç”¨pysparkè¿›è¡ŒSparkSQLç¼–ç¨‹æ—¶ï¼Œåœ¨Excutorä¸Šè·‘çš„å…¨éƒ¨æ˜¯javaå­—èŠ‚ç ï¼Œpysparkåœ¨Driverç«¯å°±å°†ç›¸åº”çš„Pythonä»£ç è½¬æ¢æˆäº†javaä»»åŠ¡ç„¶åæ”¾åˆ°Excutorä¸Šæ‰§è¡Œã€‚


å› æ­¤ï¼Œä½¿ç”¨SparkSQLçš„ç¼–ç¨‹èŒƒå¼è¿›è¡Œç¼–ç¨‹ï¼Œæˆ‘ä»¬èƒ½å¤Ÿå–å¾—å‡ ä¹å’Œç›´æ¥ä½¿ç”¨scala/javaè¿›è¡Œç¼–ç¨‹ç›¸å½“çš„æ•ˆç‡(å¿½ç•¥è¯­æ³•è§£ææ—¶é—´å·®å¼‚)ã€‚æ­¤å¤–SparkSQLæä¾›äº†éå¸¸æ–¹ä¾¿çš„æ•°æ®è¯»å†™APIï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å®ƒå’ŒHiveè¡¨ï¼ŒHDFSï¼Œmysqlè¡¨ï¼ŒCassandraï¼ŒHbaseç­‰å„ç§å­˜å‚¨åª’ä»‹è¿›è¡Œæ•°æ®äº¤æ¢ã€‚

ç¾ä¸­ä¸è¶³çš„æ˜¯ï¼ŒSparkSQLçš„çµæ´»æ€§ä¼šç¨å·®ä¸€äº›ï¼Œå…¶é»˜è®¤æ”¯æŒçš„æ•°æ®ç±»å‹é€šå¸¸åªæœ‰ Int,Long,Float,Double,String,Boolean ç­‰è¿™äº›æ ‡å‡†SQLæ•°æ®ç±»å‹, ç±»å‹æ‰©å±•ç›¸å¯¹ç¹çã€‚å¯¹äºä¸€äº›è¾ƒä¸ºSQLä¸­ä¸ç›´æ¥æ”¯æŒçš„åŠŸèƒ½ï¼Œé€šå¸¸å¯ä»¥å€ŸåŠ©äºç”¨æˆ·è‡ªå®šä¹‰å‡½æ•°(UDF)æ¥å®ç°ï¼Œå¦‚æœåŠŸèƒ½æ›´åŠ å¤æ‚ï¼Œåˆ™å¯ä»¥è½¬æˆRDDæ¥è¿›è¡Œå®ç°ã€‚

æœ¬èŠ‚æˆ‘ä»¬å°†ä¸»è¦ä»‹ç»ä»¥ä¸‹ä¸»è¦å†…å®¹ï¼š

* RDDå’ŒDataFrameçš„å¯¹æ¯”

* åˆ›å»ºDataFrame

* DataFrameä¿å­˜æˆæ–‡ä»¶

* DataFrameçš„APIäº¤äº’

* DataFrameçš„SQLäº¤äº’

<!-- #endregion -->

```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark.sql import SparkSession

#SparkSQLçš„è®¸å¤šåŠŸèƒ½å°è£…åœ¨SparkSessionçš„æ–¹æ³•æ¥å£ä¸­

spark = SparkSession.builder \
        .appName("test") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext


```

### ä¸€ï¼ŒRDDï¼ŒDataFrameå’ŒDataSetå¯¹æ¯”


DataFrameå‚ç…§äº†Pandasçš„æ€æƒ³ï¼Œåœ¨RDDåŸºç¡€ä¸Šå¢åŠ äº†schmaï¼Œèƒ½å¤Ÿè·å–åˆ—åä¿¡æ¯ã€‚

DataSetåœ¨DataFrameåŸºç¡€ä¸Šè¿›ä¸€æ­¥å¢åŠ äº†æ•°æ®ç±»å‹ä¿¡æ¯ï¼Œå¯ä»¥åœ¨ç¼–è¯‘æ—¶å‘ç°ç±»å‹é”™è¯¯ã€‚

DataFrameå¯ä»¥çœ‹æˆDataSet[Row]ï¼Œä¸¤è€…çš„APIæ¥å£å®Œå…¨ç›¸åŒã€‚

DataFrameå’ŒDataSetéƒ½æ”¯æŒSQLäº¤äº’å¼æŸ¥è¯¢ï¼Œå¯ä»¥å’Œ Hiveæ— ç¼è¡”æ¥ã€‚

DataSetåªæœ‰Scalaè¯­è¨€å’ŒJavaè¯­è¨€æ¥å£ä¸­æ‰æ”¯æŒï¼Œåœ¨Pythonå’ŒRè¯­è¨€æ¥å£åªæ”¯æŒDataFrameã€‚

DataFrameæ•°æ®ç»“æ„æœ¬è´¨ä¸Šæ˜¯é€šè¿‡RDDæ¥å®ç°çš„ï¼Œä½†æ˜¯RDDæ˜¯ä¸€ç§è¡Œå­˜å‚¨çš„æ•°æ®ç»“æ„ï¼Œè€ŒDataFrameæ˜¯ä¸€ç§åˆ—å­˜å‚¨çš„æ•°æ®ç»“æ„ã€‚




### äºŒï¼Œåˆ›å»ºDataFrame


**1ï¼Œé€šè¿‡toDFæ–¹æ³•è½¬æ¢æˆDataFrame**

å¯ä»¥å°†RDDç”¨toDFæ–¹æ³•è½¬æ¢æˆDataFrame


```python
#å°†RDDè½¬æ¢æˆDataFrame
rdd = sc.parallelize([("LiLei",15,88),("HanMeiMei",16,90),("DaChui",17,60)])
df = rdd.toDF(["name","age","score"])
df.show()
df.printSchema()
```

```
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 15|   88|
|HanMeiMei| 16|   90|
|   DaChui| 17|   60|
+---------+---+-----+

root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
 |-- score: long (nullable = true)
```

```python

```

**2, é€šè¿‡createDataFrameæ–¹æ³•å°†Pandas.DataFrameè½¬æ¢æˆpysparkä¸­çš„DataFrame**

```python
import pandas as pd 

pdf = pd.DataFrame([("LiLei",18),("HanMeiMei",17)],columns = ["name","age"])
df = spark.createDataFrame(pdf)
df.show()
```

```
+---------+---+
|     name|age|
+---------+---+
|    LiLei| 18|
|HanMeiMei| 17|
+---------+---+
```

```python

```

```python
# ä¹Ÿå¯ä»¥å¯¹åˆ—è¡¨ç›´æ¥è½¬æ¢
values = [("LiLei",18),("HanMeiMei",17)]
df = spark.createDataFrame(values,["name","age"])
df.show()
```

```
+---------+---+
|     name|age|
+---------+---+
|    LiLei| 18|
|HanMeiMei| 17|
+---------+---+
```

```python

```

**4, é€šè¿‡createDataFrameæ–¹æ³•æŒ‡å®šschemaåŠ¨æ€åˆ›å»ºDataFrame**

å¯ä»¥é€šè¿‡createDataFrameçš„æ–¹æ³•æŒ‡å®šrddå’Œschemaåˆ›å»ºDataFrameã€‚

è¿™ç§æ–¹æ³•æ¯”è¾ƒç¹çï¼Œä½†æ˜¯å¯ä»¥åœ¨é¢„å…ˆä¸çŸ¥é“schemaå’Œæ•°æ®ç±»å‹çš„æƒ…å†µä¸‹åœ¨ä»£ç ä¸­åŠ¨æ€åˆ›å»ºDataFrame.


```python
from pyspark.sql.types import *
from pyspark.sql import Row
from datetime import datetime

schema = StructType([StructField("name", StringType(), nullable = False),
                     StructField("score", IntegerType(), nullable = True),
                     StructField("birthday", DateType(), nullable = True)])

rdd = sc.parallelize([Row("LiLei",87,datetime(2010,1,5)),
                      Row("HanMeiMei",90,datetime(2009,3,1)),
                      Row("DaChui",None,datetime(2008,7,2))])

dfstudent = spark.createDataFrame(rdd, schema)

dfstudent.show()
```

```
+---------+-----+----------+
|     name|score|  birthday|
+---------+-----+----------+
|    LiLei|   87|2010-01-05|
|HanMeiMei|   90|2009-03-01|
|   DaChui| null|2008-07-02|
+---------+-----+----------+

```

```python

```

**4ï¼Œé€šè¿‡è¯»å–æ–‡ä»¶åˆ›å»º**

å¯ä»¥è¯»å–jsonæ–‡ä»¶ï¼Œcsvæ–‡ä»¶ï¼Œhiveæ•°æ®è¡¨æˆ–è€…mysqlæ•°æ®è¡¨å¾—åˆ°DataFrameã€‚

```python
#è¯»å–jsonæ–‡ä»¶ç”ŸæˆDataFrame
df = spark.read.json("data/people.json")
df.show()
```

```
+----+-------+
| age|   name|
+----+-------+
|null|Michael|
|  30|   Andy|
|  19| Justin|
+----+-------+
```

```python
#è¯»å–csvæ–‡ä»¶
df = spark.read.option("header","true") \
 .option("inferSchema","true") \
 .option("delimiter", ",") \
 .csv("data/iris.csv")
df.show(5)
df.printSchema()
```

```
+-----------+----------+-----------+----------+-----+
|sepallength|sepalwidth|petallength|petalwidth|label|
+-----------+----------+-----------+----------+-----+
|        5.1|       3.5|        1.4|       0.2|    0|
|        4.9|       3.0|        1.4|       0.2|    0|
|        4.7|       3.2|        1.3|       0.2|    0|
|        4.6|       3.1|        1.5|       0.2|    0|
|        5.0|       3.6|        1.4|       0.2|    0|
+-----------+----------+-----------+----------+-----+
only showing top 5 rows

root
 |-- sepallength: double (nullable = true)
 |-- sepalwidth: double (nullable = true)
 |-- petallength: double (nullable = true)
 |-- petalwidth: double (nullable = true)
 |-- label: integer (nullable = true)
```

```python
#è¯»å–csvæ–‡ä»¶
df = spark.read.format("com.databricks.spark.csv") \
 .option("header","true") \
 .option("inferSchema","true") \
 .option("delimiter", ",") \
 .load("data/iris.csv")
df.show(5)
df.printSchema()
```

```
+-----------+----------+-----------+----------+-----+
|sepallength|sepalwidth|petallength|petalwidth|label|
+-----------+----------+-----------+----------+-----+
|        5.1|       3.5|        1.4|       0.2|    0|
|        4.9|       3.0|        1.4|       0.2|    0|
|        4.7|       3.2|        1.3|       0.2|    0|
|        4.6|       3.1|        1.5|       0.2|    0|
|        5.0|       3.6|        1.4|       0.2|    0|
+-----------+----------+-----------+----------+-----+
only showing top 5 rows

root
 |-- sepallength: double (nullable = true)
 |-- sepalwidth: double (nullable = true)
 |-- petallength: double (nullable = true)
 |-- petalwidth: double (nullable = true)
 |-- label: integer (nullable = true)
```

```python

```

```python
#è¯»å–parquetæ–‡ä»¶
df = spark.read.parquet("data/users.parquet")
df.show()
```

```
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+

```

```python


```

```python
#è¯»å–hiveæ•°æ®è¡¨ç”ŸæˆDataFrame

spark.sql("CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive")
spark.sql("LOAD DATA LOCAL INPATH 'data/kv1.txt' INTO TABLE src")
df = spark.sql("SELECT key, value FROM src WHERE key < 10 ORDER BY key")
df.show(5)

```

```
+---+-----+
|key|value|
+---+-----+
|  0|val_0|
|  0|val_0|
|  0|val_0|
|  0|val_0|
|  0|val_0|
+---+-----+
only showing top 5 rows
```

```python
#è¯»å–mysqlæ•°æ®è¡¨ç”ŸæˆDataFrame
"""
url = "jdbc:mysql://localhost:3306/test"
df = spark.read.format("jdbc") \
 .option("url", url) \
 .option("dbtable", "runoob_tbl") \
 .option("user", "root") \
 .option("password", "0845") \
 .load()\
df.show()
"""

```

### å››ï¼ŒDataFrameä¿å­˜æˆæ–‡ä»¶


å¯ä»¥ä¿å­˜æˆcsvæ–‡ä»¶ï¼Œjsonæ–‡ä»¶ï¼Œparquetæ–‡ä»¶æˆ–è€…ä¿å­˜æˆhiveæ•°æ®è¡¨

```python
#ä¿å­˜æˆcsvæ–‡ä»¶
df = spark.read.format("json").load("data/people.json")
df.write.format("csv").option("header","true").save("data/people_write.csv")
```

```python
#å…ˆè½¬æ¢æˆrddå†ä¿å­˜æˆtxtæ–‡ä»¶
df.rdd.saveAsTextFile("data/people_rdd.txt")
```

```python
#ä¿å­˜æˆjsonæ–‡ä»¶
df.write.json("data/people_write.json")
```

```python
#ä¿å­˜æˆparquetæ–‡ä»¶, å‹ç¼©æ ¼å¼, å ç”¨å­˜å‚¨å°, ä¸”æ˜¯sparkå†…å­˜ä¸­å­˜å‚¨æ ¼å¼ï¼ŒåŠ è½½æœ€å¿«
df.write.partitionBy("age").format("parquet").save("data/namesAndAges.parquet")
df.write.parquet("data/people_write.parquet")
```

```python
#ä¿å­˜æˆhiveæ•°æ®è¡¨
df.write.bucketBy(42, "name").sortBy("age").saveAsTable("people_bucketed")

```

```python

```

### äº”ï¼ŒDataFrameçš„APIäº¤äº’

```python
from pyspark.sql import Row
from pyspark.sql.functions import * 

df = spark.createDataFrame(
    [("LiLei",15,"male"),
     ("HanMeiMei",16,"female"),
     ("DaChui",17,"male")]).toDF("name","age","gender")

df.show()
df.printSchema()

```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
+---------+---+------+

root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
```


**1ï¼ŒActionæ“ä½œ**


DataFrameçš„Actionæ“ä½œåŒ…æ‹¬show,count,collect,,describe,take,head,firstç­‰æ“ä½œã€‚

```python
#show
df.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
+---------+---+------+
```

```python
#show(numRows: Int, truncate: Boolean) 
#ç¬¬äºŒä¸ªå‚æ•°è®¾ç½®æ˜¯å¦å½“è¾“å‡ºå­—æ®µé•¿åº¦è¶…è¿‡20æ—¶è¿›è¡Œæˆªå–
df.show(2,False) 
```

```
+---------+---+------+
|name     |age|gender|
+---------+---+------+
|LiLei    |15 |male  |
|HanMeiMei|16 |female|
+---------+---+------+
only showing top 2 rows
```

```python
#count
df.count()
```

```
3
```

```python
#collect
df.collect()
```

```
[Row(name='LiLei', age=15, gender='male'),
 Row(name='HanMeiMei', age=16, gender='female'),
 Row(name='DaChui', age=17, gender='male')]
```

```python
#first
df.first()
```

```
Row(name='LiLei', age=15, gender='male')
```

```python
#take
df.take(2)
```

```
[Row(name='LiLei', age=15, gender='male'),
 Row(name='HanMeiMei', age=16, gender='female')]
```

```python
#head
df.head(2)
```

```
[Row(name='LiLei', age=15, gender='male'),
 Row(name='HanMeiMei', age=16, gender='female')]

```

```python

```

**2ï¼Œç±»RDDæ“ä½œ** 


DataFrameæ”¯æŒRDDä¸­ä¸€äº›è¯¸å¦‚distinct,cache,sample,foreach,intersect,exceptç­‰æ“ä½œã€‚

å¯ä»¥æŠŠDataFrameå½“åšæ•°æ®ç±»å‹ä¸ºRowçš„RDDæ¥è¿›è¡Œæ“ä½œï¼Œå¿…è¦æ—¶å¯ä»¥å°†å…¶è½¬æ¢æˆRDDæ¥æ“ä½œã€‚

```python
df = spark.createDataFrame([("Hello World",),("Hello China",),("Hello Spark",)]).toDF("value")
df.show()
```

```
+-----------+
|      value|
+-----------+
|Hello World|
|Hello China|
|Hello Spark|
+-----------+
```

```python
#mapæ“ä½œï¼Œéœ€è¦å…ˆè½¬æ¢æˆrdd
rdd = df.rdd.map(lambda x:Row(x[0].upper()))
dfmap = rdd.toDF(["value"]).show()
```

```
+-----------+
|      value|
+-----------+
|HELLO WORLD|
|HELLO CHINA|
|HELLO SPARK|
+-----------+
```

```python
#flatMapï¼Œéœ€è¦å…ˆè½¬æ¢æˆrdd
df_flat = df.rdd.flatMap(lambda x:x[0].split(" ")).map(lambda x:Row(x)).toDF(["value"])
df_flat.show()
```

```
+-----+
|value|
+-----+
|Hello|
|World|
|Hello|
|China|
|Hello|
|Spark|
+-----+
```

```python
#filterè¿‡æ»¤
df_filter = df.rdd.filter(lambda s:s[0].endswith("Spark")).toDF(["value"])

df_filter.show()
```

```
+-----------+
|      value|
+-----------+
|Hello Spark|
+-----------+
```

```python
# filterå’Œbroadcastæ··åˆä½¿ç”¨
broads = sc.broadcast(["Hello","World"])

df_filter_broad = df_flat.filter(~col("value").isin(broads.value))

df_filter_broad.show() 
```

```
+-----+
|value|
+-----+
|China|
|Spark|
+-----+
```

```python
#distinct
df_distinct = df_flat.distinct()
df_distinct.show() 

```

```
+-----+
|value|
+-----+
|World|
|China|
|Hello|
|Spark|
+-----+
```

```python
#cacheç¼“å­˜
df.cache()
df.unpersist()
```

```python
#sampleæŠ½æ ·
dfsample = df.sample(False,0.6,0)

dfsample.show()  
```

```
+-----------+
|      value|
+-----------+
|Hello China|
|Hello Spark|
+-----------+
```

```python
df2 = spark.createDataFrame([["Hello World"],["Hello Scala"],["Hello Spark"]]).toDF("value")
df2.show()
```

```
+-----------+
|      value|
+-----------+
|Hello World|
|Hello Scala|
|Hello Spark|
+-----------+
```

```python
#intersectäº¤é›†
dfintersect = df.intersect(df2)

dfintersect.show()
```

```
+-----------+
|      value|
+-----------+
|Hello Spark|
|Hello World|
+-----------+
```

```python
#exceptAllè¡¥é›†

dfexcept = df.exceptAll(df2)
dfexcept.show()

```

```
+-----------+
|      value|
+-----------+
|Hello China|
+-----------+
```

```python

```

**3ï¼Œç±»Excelæ“ä½œ**


å¯ä»¥å¯¹DataFrameè¿›è¡Œå¢åŠ åˆ—ï¼Œåˆ é™¤åˆ—ï¼Œé‡å‘½ååˆ—ï¼Œæ’åºç­‰æ“ä½œï¼Œå»é™¤é‡å¤è¡Œï¼Œå»é™¤ç©ºè¡Œï¼Œå°±è·Ÿæ“ä½œExcelè¡¨æ ¼ä¸€æ ·ã€‚

```python
df = spark.createDataFrame([
("LiLei",15,"male"),
("HanMeiMei",16,"female"),
("DaChui",17,"male"),
("RuHua",16,None)
]).toDF("name","age","gender")

df.show()
df.printSchema()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
+---------+---+------+

root
 |-- name: string (nullable = true)
 |-- age: long (nullable = true)
 |-- gender: string (nullable = true)
```

```python
#å¢åŠ åˆ—
dfnew = df.withColumn("birthyear",-df["age"]+2020)

dfnew.show() 
```

```
+---------+---+------+---------+
|     name|age|gender|birthyear|
+---------+---+------+---------+
|    LiLei| 15|  male|     2005|
|HanMeiMei| 16|female|     2004|
|   DaChui| 17|  male|     2003|
|    RuHua| 16|  null|     2004|
+---------+---+------+---------+
```

```python
#ç½®æ¢åˆ—çš„é¡ºåº
dfupdate = dfnew.select("name","age","birthyear","gender")
dfupdate.show()
```

```python
#åˆ é™¤åˆ—
dfdrop = df.drop("gender")
dfdrop.show() 
```

```
+---------+---+
|     name|age|
+---------+---+
|    LiLei| 15|
|HanMeiMei| 16|
|   DaChui| 17|
|    RuHua| 16|
+---------+---+
```

```python
#é‡å‘½ååˆ—
dfrename = df.withColumnRenamed("gender","sex")
dfrename.show() 
```

```
+---------+---+------+
|     name|age|   sex|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
+---------+---+------+

```

```python
#æ’åºsortï¼Œå¯ä»¥æŒ‡å®šå‡åºé™åº
dfsorted = df.sort(df["age"].desc())
dfsorted.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|   DaChui| 17|  male|
|    RuHua| 16|  null|
|HanMeiMei| 16|female|
|    LiLei| 15|  male|
+---------+---+------+
```

```python
#æ’åºorderby,é»˜è®¤ä¸ºå‡åº,å¯ä»¥æ ¹æ®å¤šä¸ªå­—æ®µ
dfordered = df.orderBy(df["age"].desc(),df["gender"].desc())

dfordered.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|   DaChui| 17|  male|
|HanMeiMei| 16|female|
|    RuHua| 16|  null|
|    LiLei| 15|  male|
+---------+---+------+
```

```python
#å»é™¤nanå€¼è¡Œ
dfnotnan = df.na.drop()

dfnotnan.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
+---------+---+------+
```

```python
#å¡«å……nanå€¼
df_fill = df.na.fill("female")
df_fill.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|female|
+---------+---+------+
```

```python
#æ›¿æ¢æŸäº›å€¼
df_replace = df.na.replace({"":"female","RuHua":"SiYu"})
df_replace.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|     SiYu| 16|  null|
+---------+---+------+
```


```python
#å»é‡ï¼Œé»˜è®¤æ ¹æ®å…¨éƒ¨å­—æ®µ
df2 = df.unionAll(df)
df2.show()
dfunique = df2.dropDuplicates()
dfunique.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
+---------+---+------+

+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    RuHua| 16|  null|
|   DaChui| 17|  male|
|HanMeiMei| 16|female|
|    LiLei| 15|  male|
+---------+---+------+
```

```python
#å»é‡,æ ¹æ®éƒ¨åˆ†å­—æ®µ
dfunique_part = df.dropDuplicates(["age"])
dfunique_part.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|   DaChui| 17|  male|
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
+---------+---+------+

```

```python
#ç®€å•èšåˆæ“ä½œ
dfagg = df.agg({"name":"count","age":"max"})

dfagg.show()
```

```
+-----------+--------+
|count(name)|max(age)|
+-----------+--------+
|          4|      17|
+-----------+--------+

```

```python
#æ±‡æ€»ä¿¡æ¯
df_desc = df.describe()
df_desc.show()
```

```
+-------+------+-----------------+------+
|summary|  name|              age|gender|
+-------+------+-----------------+------+
|  count|     4|                4|     3|
|   mean|  null|             16.0|  null|
| stddev|  null|0.816496580927726|  null|
|    min|DaChui|               15|female|
|    max| RuHua|               17|  male|
+-------+------+-----------------+------+
```

```python
#é¢‘ç‡è¶…è¿‡0.5çš„å¹´é¾„å’Œæ€§åˆ«
df_freq = df.stat.freqItems(("age","gender"),0.5)

df_freq.show()
```

```
+-------------+----------------+
|age_freqItems|gender_freqItems|
+-------------+----------------+
|         [16]|          [male]|
+-------------+----------------+
```

```python

```

**4ï¼Œç±»SQLè¡¨æ“ä½œ**


ç±»SQLè¡¨æ“ä½œä¸»è¦åŒ…æ‹¬è¡¨æŸ¥è¯¢(select,selectExpr,where),è¡¨è¿æ¥(join,union,unionAll),è¡¨åˆ†ç»„(groupby,agg,pivot)ç­‰æ“ä½œã€‚

```python
df = spark.createDataFrame([
("LiLei",15,"male"),
("HanMeiMei",16,"female"),
("DaChui",17,"male"),
("RuHua",16,None)]).toDF("name","age","gender")

df.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
+---------+---+------+
```

```python
#è¡¨æŸ¥è¯¢select
dftest = df.select("name").limit(2)
dftest.show()
```

```
+---------+
|     name|
+---------+
|    LiLei|
|HanMeiMei|
+---------+
```

```python
dftest = df.select("name",df["age"] + 1)
dftest.show()
```

```
+---------+---------+
|     name|(age + 1)|
+---------+---------+
|    LiLei|       16|
|HanMeiMei|       17|
|   DaChui|       18|
|    RuHua|       17|
+---------+---------+
```

```python
#è¡¨æŸ¥è¯¢select
dftest = df.select("name",-df["age"]+2020).toDF("name","birth_year")
dftest.show()
```

```
+---------+----------+
|     name|birth_year|
+---------+----------+
|    LiLei|      2005|
|HanMeiMei|      2004|
|   DaChui|      2003|
|    RuHua|      2004|
+---------+----------+
```

```python
#è¡¨æŸ¥è¯¢selectExpr,å¯ä»¥ä½¿ç”¨UDFå‡½æ•°ï¼ŒæŒ‡å®šåˆ«åç­‰
import datetime
spark.udf.register("getBirthYear",lambda age:datetime.datetime.now().year-age)
dftest = df.selectExpr("name", "getBirthYear(age) as birth_year" , "UPPER(gender) as gender" )
dftest.show()
```

```
+---------+----------+------+
|     name|birth_year|gender|
+---------+----------+------+
|    LiLei|      2005|  MALE|
|HanMeiMei|      2004|FEMALE|
|   DaChui|      2003|  MALE|
|    RuHua|      2004|  null|
+---------+----------+------+
```

```python
#è¡¨æŸ¥è¯¢where, æŒ‡å®šSQLä¸­çš„whereå­—å¥è¡¨è¾¾å¼
dftest = df.where("gender='male' and age>15")
dftest.show()
```

```
+------+---+------+
|  name|age|gender|
+------+---+------+
|DaChui| 17|  male|
+------+---+------+
```

```python
#è¡¨æŸ¥è¯¢filter
dftest = df.filter(df["age"]>16)
dftest.show()
```

```
+------+---+------+
|  name|age|gender|
+------+---+------+
|DaChui| 17|  male|
+------+---+------+
```

```python
#è¡¨æŸ¥è¯¢filter
dftest = df.filter("gender ='male'")
dftest.show()
```

```
+------+---+------+
|  name|age|gender|
+------+---+------+
| LiLei| 15|  male|
|DaChui| 17|  male|
+------+---+------+
```

```python
#è¡¨è¿æ¥join
dfscore = spark.createDataFrame([("LiLei","male",88),("HanMeiMei","female",90),("DaChui","male",50)]) \
          .toDF("name","gender","score") 

dfscore.show()
```

```
+---------+------+-----+
|     name|gender|score|
+---------+------+-----+
|    LiLei|  male|   88|
|HanMeiMei|female|   90|
|   DaChui|  male|   50|
+---------+------+-----+
```

```python
#è¡¨è¿æ¥join,æ ¹æ®å•ä¸ªå­—æ®µ
dfjoin = df.join(dfscore.select("name","score"),"name")
dfjoin.show()
```

```
+---------+---+------+-----+
|     name|age|gender|score|
+---------+---+------+-----+
|    LiLei| 15|  male|   88|
|HanMeiMei| 16|female|   90|
|   DaChui| 17|  male|   50|
+---------+---+------+-----+
```

```python
#è¡¨è¿æ¥join,æ ¹æ®å¤šä¸ªå­—æ®µ
dfjoin = df.join(dfscore,["name","gender"])
dfjoin.show()
```

```
+---------+------+---+-----+
|     name|gender|age|score|
+---------+------+---+-----+
|HanMeiMei|female| 16|   90|
|   DaChui|  male| 17|   50|
|    LiLei|  male| 15|   88|
+---------+------+---+-----+
```

```python
#è¡¨è¿æ¥join,æ ¹æ®å¤šä¸ªå­—æ®µ
#å¯ä»¥æŒ‡å®šè¿æ¥æ–¹å¼ä¸º"inner","left","right","outer","semi","full","leftanti","anti"ç­‰å¤šç§æ–¹å¼
dfjoin = df.join(dfscore,["name","gender"],"right")
dfjoin.show()
```

```
+---------+------+---+-----+
|     name|gender|age|score|
+---------+------+---+-----+
|HanMeiMei|female| 16|   90|
|   DaChui|  male| 17|   50|
|    LiLei|  male| 15|   88|
+---------+------+---+-----+

```

```python
dfjoin = df.join(dfscore,["name","gender"],"outer")
dfjoin.show()
```

```
+---------+------+---+-----+
|     name|gender|age|score|
+---------+------+---+-----+
|HanMeiMei|female| 16|   90|
|   DaChui|  male| 17|   50|
|    LiLei|  male| 15|   88|
|    RuHua|  null| 16| null|
+---------+------+---+-----+
```

```python
#è¡¨è¿æ¥ï¼Œçµæ´»æŒ‡å®šè¿æ¥å…³ç³»
dfmark = dfscore.withColumnRenamed("gender","sex")
dfmark.show()
```

```
+---------+------+-----+
|     name|   sex|score|
+---------+------+-----+
|    LiLei|  male|   88|
|HanMeiMei|female|   90|
|   DaChui|  male|   50|
+---------+------+-----+

```

```python
dfjoin = df.join(dfmark,(df["name"] == dfmark["name"]) & (df["gender"]==dfmark["sex"]),
        "inner")
dfjoin.show()
```

```
+---------+---+------+---------+------+-----+
|     name|age|gender|     name|   sex|score|
+---------+---+------+---------+------+-----+
|HanMeiMei| 16|female|HanMeiMei|female|   90|
|   DaChui| 17|  male|   DaChui|  male|   50|
|    LiLei| 15|  male|    LiLei|  male|   88|
+---------+---+------+---------+------+-----+

```

```python
#è¡¨åˆå¹¶union
dfstudent = spark.createDataFrame([("Jim",18,"male"),("Lily",16,"female")]).toDF("name","age","gender")
dfstudent.show()
```

```
+----+---+------+
|name|age|gender|
+----+---+------+
| Jim| 18|  male|
|Lily| 16|female|
+----+---+------+
```

```python
dfunion = df.union(dfstudent)
dfunion.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 15|  male|
|HanMeiMei| 16|female|
|   DaChui| 17|  male|
|    RuHua| 16|  null|
|      Jim| 18|  male|
|     Lily| 16|female|
+---------+---+------+
```

```python
#è¡¨åˆ†ç»„ groupBy
from pyspark.sql import functions as F 
dfgroup = df.groupBy("gender").max("age")
dfgroup.show()
```

```
+------+--------+
|gender|max(age)|
+------+--------+
|  null|      16|
|female|      16|
|  male|      17|
+------+--------+
```

```python
#è¡¨åˆ†ç»„åèšåˆï¼ŒgroupBy,agg
dfagg = df.groupBy("gender").agg(F.mean("age").alias("mean_age"),
   F.collect_list("name").alias("names"))
dfagg.show()
```

```
+------+--------+---------------+
|gender|mean_age|          names|
+------+--------+---------------+
|  null|    16.0|        [RuHua]|
|female|    16.0|    [HanMeiMei]|
|  male|    16.0|[LiLei, DaChui]|
+------+--------+---------------+

```

```python
#è¡¨åˆ†ç»„èšåˆï¼ŒgroupBy,agg
dfagg = df.groupBy("gender").agg(F.expr("avg(age)"),F.expr("collect_list(name)"))
dfagg.show()

```

```
+------+--------+------------------+
|gender|avg(age)|collect_list(name)|
+------+--------+------------------+
|  null|    16.0|           [RuHua]|
|female|    16.0|       [HanMeiMei]|
|  male|    16.0|   [LiLei, DaChui]|
+------+--------+------------------+

```

```python
#è¡¨åˆ†ç»„èšåˆï¼ŒgroupBy,agg
df.groupBy("gender","age").agg(F.collect_list(col("name"))).show()
```

```
+------+---+------------------+
|gender|age|collect_list(name)|
+------+---+------------------+
|  male| 15|           [LiLei]|
|  male| 17|          [DaChui]|
|female| 16|       [HanMeiMei]|
|  null| 16|           [RuHua]|
+------+---+------------------+

```

```python
#è¡¨åˆ†ç»„åé€è§†ï¼ŒgroupBy,pivot
dfstudent = spark.createDataFrame([("LiLei",18,"male",1),("HanMeiMei",16,"female",1),
                    ("Jim",17,"male",2),("DaChui",20,"male",2)]).toDF("name","age","gender","class")
dfstudent.show()
dfstudent.groupBy("class").pivot("gender").max("age").show()
```

```
+---------+---+------+-----+
|     name|age|gender|class|
+---------+---+------+-----+
|    LiLei| 18|  male|    1|
|HanMeiMei| 16|female|    1|
|      Jim| 17|  male|    2|
|   DaChui| 20|  male|    2|
+---------+---+------+-----+

+-----+------+----+
|class|female|male|
+-----+------+----+
|    1|    16|  18|
|    2|  null|  20|
+-----+------+----+
```

```python
#çª—å£å‡½æ•°

df = spark.createDataFrame([("LiLei",78,"class1"),("HanMeiMei",87,"class1"),
                           ("DaChui",65,"class2"),("RuHua",55,"class2")]) \
    .toDF("name","score","class")

df.show()
dforder = df.selectExpr("name","score","class",
         "row_number() over (partition by class order by score desc) as order")

dforder.show()
```

```
+---------+-----+------+
|     name|score| class|
+---------+-----+------+
|    LiLei|   78|class1|
|HanMeiMei|   87|class1|
|   DaChui|   65|class2|
|    RuHua|   55|class2|
+---------+-----+------+

+---------+-----+------+-----+
|     name|score| class|order|
+---------+-----+------+-----+
|   DaChui|   65|class2|    1|
|    RuHua|   55|class2|    2|
|HanMeiMei|   87|class1|    1|
|    LiLei|   78|class1|    2|
+---------+-----+------+-----+
```

```python

```

### å…­ï¼ŒDataFrameçš„SQLäº¤äº’


å°†DataFrameæ³¨å†Œä¸ºä¸´æ—¶è¡¨è§†å›¾æˆ–è€…å…¨å±€è¡¨è§†å›¾åï¼Œå¯ä»¥ä½¿ç”¨sqlè¯­å¥å¯¹DataFrameè¿›è¡Œäº¤äº’ã€‚

ä¸ä»…å¦‚æ­¤ï¼Œè¿˜å¯ä»¥é€šè¿‡SparkSQLå¯¹Hiveè¡¨ç›´æ¥è¿›è¡Œå¢åˆ æ”¹æŸ¥ç­‰æ“ä½œã€‚



**1ï¼Œæ³¨å†Œè§†å›¾åè¿›è¡ŒSQLäº¤äº’** 

```python
#æ³¨å†Œä¸ºä¸´æ—¶è¡¨è§†å›¾, å…¶ç”Ÿå‘½å‘¨æœŸå’ŒSparkSessionç›¸å…³è”
df = spark.createDataFrame([("LiLei",18,"male"),("HanMeiMei",17,"female"),("Jim",16,"male")],
                              ("name","age","gender"))

df.show()
df.createOrReplaceTempView("student")
dfmale = spark.sql("select * from student where gender='male'")
dfmale.show()
```

```
+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 18|  male|
|HanMeiMei| 17|female|
|      Jim| 16|  male|
+---------+---+------+

+-----+---+------+
| name|age|gender|
+-----+---+------+
|LiLei| 18|  male|
|  Jim| 16|  male|
+-----+---+------+
```

```python
#æ³¨å†Œä¸ºå…¨å±€ä¸´æ—¶è¡¨è§†å›¾,å…¶ç”Ÿå‘½å‘¨æœŸå’Œæ•´ä¸ªSparkåº”ç”¨ç¨‹åºå…³è”

df.createOrReplaceGlobalTempView("student")
query = """
 select t.gender
 , collect_list(t.name) as names 
 from global_temp.student t 
 group by t.gender
""".strip("\n")

spark.sql(query).show()
#å¯ä»¥åœ¨æ–°çš„Sessionä¸­è®¿é—®
spark.newSession().sql("select * from global_temp.student").show()

```

```
+------+------------+
|gender|       names|
+------+------------+
|female| [HanMeiMei]|
|  male|[LiLei, Jim]|
+------+------------+

+---------+---+------+
|     name|age|gender|
+---------+---+------+
|    LiLei| 18|  male|
|HanMeiMei| 17|female|
|      Jim| 16|  male|
+---------+---+------+
```


**2ï¼Œå¯¹Hiveè¡¨è¿›è¡Œå¢åˆ æ”¹æŸ¥æ“ä½œ**

```python

```

```python
#åˆ é™¤hiveè¡¨

query = "DROP TABLE IF EXISTS students" 
spark.sql(query) 

```

```python
#å»ºç«‹hiveåˆ†åŒºè¡¨
#(æ³¨ï¼šä¸å¯ä»¥ä½¿ç”¨ä¸­æ–‡å­—æ®µä½œä¸ºåˆ†åŒºå­—æ®µ)

query = """CREATE TABLE IF NOT EXISTS `students`
(`name` STRING COMMENT 'å§“å',
`age` INT COMMENT 'å¹´é¾„'
)
PARTITIONED BY ( `class` STRING  COMMENT 'ç­çº§', `gender` STRING  COMMENT 'æ€§åˆ«')
""".replace("\n"," ")
spark.sql(query) 
```

```python
##åŠ¨æ€å†™å…¥æ•°æ®åˆ°hiveåˆ†åŒºè¡¨
spark.conf.set("hive.exec.dynamic.partition.mode", "nonstrict") #æ³¨æ„æ­¤å¤„æœ‰ä¸€ä¸ªè®¾ç½®æ“ä½œ
dfstudents = spark.createDataFrame([("LiLei",18,"class1","male"),
                                    ("HanMeimei",17,"class2","female"),
                                    ("DaChui",19,"class2","male"),
                                    ("Lily",17,"class1","female")]).toDF("name","age","class","gender")
dfstudents.show()

#åŠ¨æ€å†™å…¥åˆ†åŒº
dfstudents.write.mode("overwrite").format("hive")\
.partitionBy("class","gender").saveAsTable("students")
```

```python
#å†™å…¥åˆ°é™æ€åˆ†åŒº
dfstudents = spark.createDataFrame([("Jim",18,"class3","male"),
                                    ("Tom",19,"class3","male")]).toDF("name","age","class","gender")
dfstudents.createOrReplaceTempView("dfclass3")

#INSERT INTO å°¾éƒ¨è¿½åŠ , INSERT OVERWRITE TABLE è¦†ç›–åˆ†åŒº
query = """
INSERT OVERWRITE TABLE `students`
PARTITION(class='class3',gender='male') 
SELECT name,age from dfclass3
""".replace("\n"," ")
spark.sql(query)
```

```python
#å†™å…¥åˆ°æ··åˆåˆ†åŒº
dfstudents = spark.createDataFrame([("David",18,"class4","male"),
                                    ("Amy",17,"class4","female"),
                                    ("Jerry",19,"class4","male"),
                                    ("Ann",17,"class4","female")]).toDF("name","age","class","gender")
dfstudents.createOrReplaceTempView("dfclass4")

query = """
INSERT OVERWRITE TABLE `students`
PARTITION(class='class4',gender) 
SELECT name,age,gender from dfclass4
""".replace("\n"," ")
spark.sql(query)
```

```python
#è¯»å–å…¨éƒ¨æ•°æ®

dfdata = spark.sql("select * from students")
dfdata.show()
```

```
+---------+---+------+------+
|     name|age| class|gender|
+---------+---+------+------+
|      Ann| 17|class4|female|
|      Amy| 17|class4|female|
|HanMeimei| 17|class2|female|
|   DaChui| 19|class2|  male|
|    LiLei| 18|class1|  male|
|     Lily| 17|class1|female|
|    Jerry| 19|class4|  male|
|    David| 18|class4|  male|
|      Jim| 18|class3|  male|
|      Tom| 19|class3|  male|
+---------+---+------+------+
```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)

```python

```
