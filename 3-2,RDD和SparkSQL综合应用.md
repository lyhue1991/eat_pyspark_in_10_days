# 3-2,RDDå’ŒSparkSQLç»¼åˆåº”ç”¨



åœ¨pysparkå¤§æ•°æ®é¡¹ç›®å®è·µä¸­ï¼Œæˆ‘ä»¬å¾€å¾€è¦ç»¼åˆåº”ç”¨SparkSQLå’ŒRDDæ¥å®Œæˆä»»åŠ¡ã€‚

é€šå¸¸ï¼Œæˆ‘ä»¬ä¼šä½¿ç”¨SparkSQLçš„DataFrameæ¥è´Ÿè´£é¡¹ç›®ä¸­æ•°æ®è¯»å†™ç›¸å…³çš„ä»»åŠ¡ã€‚

å¯¹äºä¸€äº›èƒ½å¤Ÿè¡¨è¾¾ä¸ºè¡¨åˆå¹¶ï¼Œè¡¨æ‹¼æ¥ï¼Œè¡¨åˆ†ç»„ç­‰å¸¸è§„SQLæ“ä½œçš„ä»»åŠ¡ï¼Œæˆ‘ä»¬ä¹Ÿè‡ªç„¶å€¾å‘äºä½¿ç”¨DataFrameæ¥è¡¨è¾¾æˆ‘ä»¬çš„é€»è¾‘ã€‚

ä½†åœ¨ä¸€äº›çœŸå®é¡¹ç›®åœºæ™¯ä¸­ï¼Œå¯èƒ½ä¼šéœ€è¦å®ç°ä¸€äº›éå¸¸å¤æ‚å’Œç²¾ç»†çš„é€»è¾‘ï¼Œæˆ‘ä»¬ä¸çŸ¥é“å¦‚ä½•ä½¿ç”¨DataFrameæ¥ç›´æ¥å®ç°è¿™äº›é€»è¾‘ã€‚

æˆ–è€…ä½¿ç”¨DataFrameæ¥å®ç°å®ƒä»¬è¿‡äºå¤æ‚ï¼Œä¸æ˜¯ç®€å•åœ°å†™å‡ ä¸ªè‡ªå®šä¹‰å‡½æ•°å°±å¯ä»¥ã€‚

æˆ‘ä»¬å¾€å¾€ä¼šå°†DataFrameè½¬åŒ–ä¸ºRDDï¼Œåœ¨RDDä¸­åº”ç”¨Pythonä¸­çš„åˆ—è¡¨å’Œå­—å…¸ç­‰æ•°æ®ç»“æ„çš„æ“ä½œæ¥å®ç°è¿™ä¸ªé€»è¾‘ï¼Œç„¶åå†å°†RDDè½¬å›æˆDataFrameã€‚

ä¸‹é¢ä»¥ä¸€ä¸ªDBSCANèšç±»ç®—æ³•çš„åˆ†å¸ƒå¼å®ç°ä¸ºä¾‹ï¼Œæ¥è¯´æ˜ç»¼åˆåº”ç”¨SparkSQLå’ŒRDDçš„æ–¹æ³•ã€‚

è¿™ä¸ªæ¡ˆä¾‹çš„éš¾åº¦æ˜¯å·¥ä¸šçº§çš„ï¼Œè¯»è€…ä¸ä¸€å®šèƒ½å¤Ÿå®Œå…¨ç†è§£ï¼Œä¸ç”¨è¿‡åˆ†æ‹…å¿ƒã€‚

æˆ‘ç›¸ä¿¡ï¼Œå³ä½¿é˜…è¯»è¿™ä¸ªæ¡ˆä¾‹åä»…ç†è§£å¾ˆå°‘çš„ä¸€éƒ¨åˆ†ï¼Œä¹Ÿä¼šè®©è¯»è€…å¯¹Sparkåœ¨çœŸå®é¡¹ç›®åœºæ™¯ä¸­çš„åº”ç”¨æŠ€å·§å»ºç«‹èµ·æ›´å¥½çš„æ„Ÿè§‰ã€‚



```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark.sql import SparkSession
from pyspark.storagelevel import StorageLevel 

#SparkSQLçš„è®¸å¤šåŠŸèƒ½å°è£…åœ¨SparkSessionçš„æ–¹æ³•æ¥å£ä¸­

spark = SparkSession.builder \
        .appName("dbscan") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext
```

```python

```

### ä¸€ï¼ŒDBSCANç®—æ³•ç®€ä»‹

<!-- #region -->
DBSCANæ˜¯ä¸€ç§éå¸¸è‘—åçš„åŸºäºå¯†åº¦çš„èšç±»ç®—æ³•ã€‚å…¶è‹±æ–‡å…¨ç§°æ˜¯ Density-Based Spatial Clustering of Applications with Noiseï¼Œæ„å³ï¼šä¸€ç§åŸºäºå¯†åº¦ï¼Œå¯¹å™ªå£°é²æ£’çš„ç©ºé—´èšç±»ç®—æ³•ã€‚ç›´è§‚æ•ˆæœä¸Šçœ‹ï¼ŒDBSCANç®—æ³•å¯ä»¥æ‰¾åˆ°æ ·æœ¬ç‚¹çš„å…¨éƒ¨å¯†é›†åŒºåŸŸï¼Œå¹¶æŠŠè¿™äº›å¯†é›†åŒºåŸŸå½“åšä¸€ä¸ªä¸€ä¸ªçš„èšç±»ç°‡ã€‚

DBSCANç®—æ³•å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

* åŸºäºå¯†åº¦ï¼Œå¯¹è¿œç¦»å¯†åº¦æ ¸å¿ƒçš„å™ªå£°ç‚¹é²æ£’
* æ— éœ€çŸ¥é“èšç±»ç°‡çš„æ•°é‡
* å¯ä»¥å‘ç°ä»»æ„å½¢çŠ¶çš„èšç±»ç°‡


DBSCANçš„ç®—æ³•æ­¥éª¤åˆ†æˆä¸¤æ­¥ã€‚

1ï¼Œå¯»æ‰¾æ ¸å¿ƒç‚¹å½¢æˆä¸´æ—¶èšç±»ç°‡ã€‚

2ï¼Œåˆå¹¶ä¸´æ—¶èšç±»ç°‡å¾—åˆ°èšç±»ç°‡ã€‚


å®Œæ•´æ•™ç¨‹å¯ä»¥å‚è€ƒçŸ¥ä¹æ–‡ç« ï¼š

ã€Š20åˆ†é’Ÿå­¦ä¼šDBSCANèšç±»ç®—æ³•ã€‹ï¼šhttps://zhuanlan.zhihu.com/p/336501183


<!-- #endregion -->

```python

```

### äºŒï¼Œsklearnè°ƒåŒ…èŒƒä¾‹


1ï¼Œç”Ÿæˆæ ·æœ¬ç‚¹

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
import numpy as np
import pandas as pd
from sklearn import datasets


X,_ = datasets.make_moons(500,noise = 0.1,random_state=1)
pdf = pd.DataFrame(X,columns = ['feature1','feature2'])

pdf.plot.scatter('feature1','feature2', s = 100,alpha = 0.6, title = 'dataset by make_moon');

```

![](./data/moon_dataset_img.png)

```python

```

2ï¼Œè°ƒç”¨dbscanæ–¹æ³•å®Œæˆèšç±»

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

from sklearn.cluster import dbscan

# epsä¸ºé‚»åŸŸåŠå¾„ï¼Œmin_samplesä¸ºæœ€å°‘ç‚¹æ•°ç›®
core_samples,cluster_ids = dbscan(X, eps = 0.2, min_samples=20) 
# cluster_idsä¸­-1è¡¨ç¤ºå¯¹åº”çš„ç‚¹ä¸ºå™ªå£°ç‚¹

pdf = pd.DataFrame(np.c_[X,cluster_ids],columns = ['feature1','feature2','cluster_id'])
pdf['cluster_id'] = pdf['cluster_id'].astype('i2')

pdf.plot.scatter('feature1','feature2', s = 100,
    c = list(pdf['cluster_id']),cmap = 'rainbow',colorbar = False,
    alpha = 0.6,title = 'sklearn DBSCAN cluster result');
```

![](./data/sklearn_dbscan_img.png)

```python

```

### ä¸‰ï¼Œåˆ†å¸ƒå¼å®ç°æ€è·¯

<!-- #region -->
DBSCANç®—æ³•çš„åˆ†å¸ƒå¼å®ç°éœ€è¦è§£å†³ä»¥ä¸‹ä¸€äº›ä¸»è¦çš„é—®é¢˜ã€‚

1ï¼Œå¦‚ä½•è®¡ç®—æ ·æœ¬ç‚¹ä¸­ä¸¤ä¸¤ä¹‹é—´çš„è·ç¦»ï¼Ÿ

åœ¨å•æœºç¯å¢ƒä¸‹ï¼Œè®¡ç®—æ ·æœ¬ç‚¹ä¸¤ä¸¤ä¹‹é—´çš„è·ç¦»æ¯”è¾ƒç®€å•ï¼Œæ˜¯ä¸€ä¸ªåŒé‡éå†çš„è¿‡ç¨‹ã€‚ ä¸ºäº†å‡å°‘è®¡ç®—é‡ï¼Œå¯ä»¥ç”¨ç©ºé—´ç´¢å¼•å¦‚KDtreeè¿›è¡ŒåŠ é€Ÿã€‚

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒï¼Œæ ·æœ¬ç‚¹åˆ†å¸ƒåœ¨ä¸åŒçš„åˆ†åŒºï¼Œéš¾ä»¥åœ¨ä¸åŒçš„åˆ†åŒºä¹‹é—´ç›´æ¥è¿›è¡ŒåŒé‡éå†ã€‚ ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘çš„æ–¹æ¡ˆæ˜¯å°†æ ·æœ¬ç‚¹ä¸åŒçš„åˆ†åŒºåˆ†æˆå¤šä¸ªæ‰¹æ¬¡æ‹‰åˆ°Driverç«¯ï¼Œ ç„¶åä¾æ¬¡å¹¿æ’­åˆ°å„ä¸ªexcutoråˆ†åˆ«è®¡ç®—è·ç¦»ï¼Œå°†æœ€ç»ˆç»“æœunionï¼Œä»è€Œé—´æ¥å®ç°åŒé‡éå†ã€‚


2ï¼Œå¦‚ä½•æ„é€ ä¸´æ—¶èšç±»ç°‡ï¼Ÿ

è¿™ä¸ªé—®é¢˜ä¸éš¾ï¼Œå•æœºç¯å¢ƒå’Œåˆ†å¸ƒå¼ç¯å¢ƒçš„å®ç°å·®ä¸å¤šã€‚

éƒ½æ˜¯é€šè¿‡groupçš„æ–¹å¼ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬ç‚¹å‘¨è¾¹é‚»åŸŸåŠå¾„Rå†…çš„æ ·æœ¬ç‚¹æ•°é‡ï¼Œ

å¹¶è®°å½•å®ƒä»¬çš„id,å¦‚æœè¿™äº›æ ·æœ¬ç‚¹æ•°é‡è¶…è¿‡minpointsåˆ™æ„é€ ä¸´æ—¶èšç±»ç°‡ï¼Œå¹¶ç»´æŠ¤æ ¸å¿ƒç‚¹åˆ—è¡¨ã€‚


3ï¼Œå¦‚ä½•åˆå¹¶ç›¸è¿çš„ä¸´æ—¶èšç±»ç°‡å¾—åˆ°èšç±»ç°‡ï¼Ÿ


è¿™ä¸ªæ˜¯åˆ†å¸ƒå¼å®ç°ä¸­æœ€æœ€æ ¸å¿ƒçš„æ­¥éª¤ã€‚

åœ¨å•æœºç¯å¢ƒä¸‹ï¼Œæ ‡å‡†åšæ³•æ˜¯å¯¹æ¯ä¸€ä¸ªä¸´æ—¶èšç±»ç°‡ï¼Œ

åˆ¤æ–­å…¶ä¸­çš„æ ·æœ¬ç‚¹æ˜¯å¦åœ¨æ ¸å¿ƒç‚¹åˆ—è¡¨ï¼Œå¦‚æœæ˜¯ï¼Œåˆ™å°†è¯¥æ ·æœ¬ç‚¹æ‰€åœ¨çš„ä¸´æ—¶èšç±»ç°‡ä¸å½“å‰ä¸´æ—¶èšç±»ç°‡åˆå¹¶ã€‚å¹¶åœ¨æ ¸å¿ƒç‚¹åˆ—è¡¨ä¸­åˆ é™¤è¯¥æ ·æœ¬ç‚¹ã€‚

é‡å¤æ­¤è¿‡ç¨‹ï¼Œç›´åˆ°å½“å‰ä¸´æ—¶èšç±»ç°‡ä¸­æ‰€æœ‰çš„ç‚¹éƒ½ä¸åœ¨æ ¸å¿ƒç‚¹åˆ—è¡¨ã€‚

åœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸‹ï¼Œä¸´æ—¶èšç±»ç°‡åˆ†å¸ƒåœ¨ä¸åŒçš„åˆ†åŒºï¼Œæ— æ³•ç›´æ¥æ‰«æå…¨å±€æ ¸å¿ƒç‚¹åˆ—è¡¨è¿›è¡Œä¸´æ—¶èšç±»ç°‡çš„åˆå¹¶ã€‚

æˆ‘çš„æ–¹æ¡ˆæ˜¯å…ˆåœ¨æ¯ä¸€ä¸ªåˆ†åŒºå†…éƒ¨å¯¹å„ä¸ªä¸´æ—¶èšç±»ç°‡è¿›è¡Œåˆå¹¶ï¼Œç„¶åç¼©å°åˆ†åŒºæ•°é‡é‡æ–°åˆ†åŒºï¼Œå†åœ¨å„ä¸ªåˆ†åŒºå†…éƒ¨å¯¹æ¯ä¸ªä¸´æ—¶èšç±»ç°‡è¿›è¡Œåˆå¹¶ã€‚

ä¸æ–­é‡å¤è¿™ä¸ªè¿‡ç¨‹ï¼Œæœ€ç»ˆå°†æ‰€æœ‰çš„ä¸´æ—¶èšç±»ç°‡éƒ½åˆ’åˆ†åˆ°ä¸€ä¸ªåˆ†åŒºï¼Œå®Œæˆå¯¹å…¨éƒ¨ä¸´æ—¶èšç±»ç°‡çš„åˆå¹¶ã€‚

ä¸ºäº†é™ä½æœ€åä¸€ä¸ªåˆ†åŒºçš„å­˜å‚¨å‹åŠ›ï¼Œæˆ‘é‡‡ç”¨äº†ä¸åŒäºæ ‡å‡†çš„ä¸´æ—¶èšç±»ç°‡çš„åˆå¹¶ç®—æ³•ã€‚

å¯¹æ¯ä¸ªä¸´æ—¶èšç±»ç°‡åªå…³æ³¨å…¶ä¸­çš„æ ¸å¿ƒç‚¹id,è€Œä¸å…³æ³¨éæ ¸å¿ƒç‚¹id,ä»¥å‡å°‘å­˜å‚¨å‹åŠ›ã€‚åˆå¹¶æ—¶å°†æœ‰å…±åŒæ ¸å¿ƒç‚¹idçš„ä¸´æ—¶èšç±»ç°‡åˆå¹¶ã€‚


<!-- #endregion -->

```python

```

### å››ï¼Œåˆ†å¸ƒå¼å®ç°æ ¸å¿ƒé€»è¾‘


ä»¥ä¸‹ä¸ºDBSCANçš„åˆ†å¸ƒå¼å®ç°çš„æ ¸å¿ƒé€»è¾‘ã€‚å³ä»ä¸´æ—¶èšç±»ç°‡åˆå¹¶æˆèšç±»ç°‡çš„æ–¹æ¡ˆï¼Œè¯¥é€»è¾‘è¾ƒä¸ºç²¾ç»†ï¼Œé‡‡ç”¨RDDæ¥å®ç°ã€‚


1ï¼Œå¯»æ‰¾æ ¸å¿ƒç‚¹å½¢æˆä¸´æ—¶èšç±»ç°‡ã€‚

å‡å®šå·²ç»å¾—åˆ°äº†ä¸´æ—¶èšç±»ç°‡ï¼Œä¿¡æ¯å­˜å‚¨ä¸ºrdd_core

```python
#rdd_coreçš„æ¯ä¸€è¡Œä»£è¡¨ä¸€ä¸ªä¸´æ—¶èšç±»ç°‡ï¼š(min_core_id, core_id_set)
#core_id_setä¸ºä¸´æ—¶èšç±»ç°‡æ‰€æœ‰æ ¸å¿ƒç‚¹çš„ç¼–å·ï¼Œmin_core_idä¸ºè¿™äº›ç¼–å·ä¸­å–å€¼æœ€å°çš„ç¼–å·
rdd_core = sc.parallelize([(1,{1,2}),(2,{2,3,4}),(6,{6,8,9}),
        (4,{4,5}),(9,{9,10,11}),(15,{15,17}),(10,{10,11,18})],20)
data_core = rdd_core.collect()
data_core
```

```
[(1, {1, 2}),
 (2, {2, 3, 4}),
 (6, {6, 8, 9}),
 (4, {4, 5}),
 (9, {9, 10, 11}),
 (15, {15, 17}),
 (10, {10, 11, 18})]
```

```python
#å®šä¹‰åˆå¹¶å‡½æ•°ï¼šå°†æœ‰å…±åŒæ ¸å¿ƒç‚¹çš„ä¸´æ—¶èšç±»ç°‡åˆå¹¶
def mergeSets(list_set):
    result = []
    while  len(list_set)>0 :
        cur_set = list_set.pop(0)
        intersect_idxs = [i for i in list(range(len(list_set)-1,-1,-1)) if cur_set&list_set[i]]
        while  intersect_idxs :
            for idx in intersect_idxs:
                cur_set = cur_set|list_set[idx]

            for idx in intersect_idxs:
                list_set.pop(idx)
                
            intersect_idxs = [i for i in list(range(len(list_set)-1,-1,-1)) if cur_set&list_set[i]]
        
        result = result+[cur_set]
    return result

# æµ‹è¯•mergeSetsæ•ˆæœ
test_list_set = [{1,2,3},{3,4,5},{10,12,13},{4,5,8},{13,15},{7,8},{20,22}]
print(mergeSets(test_list_set))

```

```
[{1, 2, 3, 4, 5, 7, 8}, {10, 12, 13, 15}, {20, 22}]
```

```python

```

```python
#å¯¹rdd_coreåˆ†åŒºååœ¨æ¯ä¸ªåˆ†åŒºåˆå¹¶ï¼Œä¸æ–­å°†åˆ†åŒºæ•°é‡å‡å°‘ï¼Œæœ€ç»ˆåˆå¹¶åˆ°ä¸€ä¸ªåˆ†åŒº
#å¦‚æœæ•°æ®è§„æ¨¡ååˆ†å¤§ï¼Œéš¾ä»¥åˆå¹¶åˆ°ä¸€ä¸ªåˆ†åŒºï¼Œä¹Ÿå¯ä»¥æœ€ç»ˆåˆå¹¶åˆ°å¤šä¸ªåˆ†åŒºï¼Œå¾—åˆ°è¿‘ä¼¼ç»“æœã€‚
#rdd: (min_core_id,core_id_set)


def mergeRDD(rdd,partition_cnt):
    def fn(iterator):
        list_set = [x[1] for x in iterator]
        list_set_merged = mergeSets(list_set)
        merged_core = [(min(x),x) for x in list_set_merged] 
        return(iter(merged_core))
    rdd_merged = rdd.partitionBy(partition_cnt).mapPartitions(fn)
    return rdd_merged

rdd_core = sc.parallelize([(1,{1,2}),(2,{2,3,4}),(6,{6,8,9}),
        (4,{4,5}),(9,{9,10,11}),(15,{15,17}),(10,{10,11,18})],20)

#åˆ†åŒºè¿­ä»£è®¡ç®—ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿­ä»£æ¬¡æ•°å’Œåˆ†åŒºæ•°é‡
rdd_core = mergeRDD(rdd_core,8)
rdd_core = mergeRDD(rdd_core,4)
rdd_core = mergeRDD(rdd_core,1)
rdd_core.collect() 
```

```
[(1, {1, 2, 3, 4, 5}), (6, {6, 8, 9, 10, 11, 18}), (15, {15, 17})]
```

```python

```

### äº”ï¼Œåˆ†å¸ƒå¼å®ç°å®Œæ•´ä»£ç 


**1ï¼Œç”Ÿæˆæ ·æœ¬ç‚¹** 

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import numpy as np
import pandas as pd
from sklearn import datasets

#è®¾ç½®DBSCANå‚æ•°
eps = 0.2
min_samples=20

X,_ = datasets.make_moons(500,noise = 0.1,random_state=1)
pdf = pd.DataFrame(X,columns = ['feature1','feature2'])
pdf.plot.scatter('feature1','feature2', s = 100,alpha = 0.6, title = 'dataset by make_moon')

pdf.to_csv("./data/moon_dataset.csv",sep = "\t",index = False)

#è½¬æ¢æˆsparkä¸­çš„DataFrame
#dfdata = spark.createDataFrame(pdf)
dfdata = spark.read.option("header","true")\
  .option("inferSchema","true") \
  .option("delimiter", "\t") \
  .csv("data/moon_dataset.csv")

#å°†ç‚¹çš„åæ ‡ç”Ÿæˆä¸€ä¸ªarray,å¹¶æ·»åŠ å”¯ä¸€idåˆ—

dfinput = spark.createDataFrame(dfdata.selectExpr("array(feature1,feature2) as point") \
    .rdd.map(lambda row:row["point"]).zipWithIndex()).toDF("point","id") \
    .selectExpr("id","point").persist(StorageLevel.MEMORY_AND_DISK)

dfinput.show() 
```

```
+---+--------------------+
| id|               point|
+---+--------------------+
|  0|[0.31655567612451...|
|  1|[0.74088269972429...|
|  2|[0.87172637133182...|
|  3|[0.55552787799773...|
|  4|[2.03872887867669...|
|  5|[1.99136342379999...|
|  6|[0.22384428620202...|
|  7|[0.97295674842244...|
|  8|[-0.9213036629723...|
|  9|[0.46670632489966...|
| 10|[0.49217803972132...|
| 11|[-0.4223529513452...|
| 12|[0.31358610070888...|
| 13|[0.64848081923216...|
| 14|[0.31549460745917...|
| 15|[-0.9118786633207...|
| 16|[1.70164131101163...|
| 17|[0.10851453318658...|
| 18|[-0.3098724480520...|
| 19|[-0.2040816479108...|
+---+--------------------+
only showing top 20 rows

```


![](./data/moon_dataset_img.png)

```python

```

**2ï¼Œåˆ†æ‰¹æ¬¡å¹¿æ’­KDTreeå¾—åˆ°é‚»è¿‘å…³ç³»** 

```python
import numpy as np 
from pyspark.sql import types as T
from pyspark.sql import functions as F 
from pyspark.sql import Row,DataFrame
from sklearn.neighbors import KDTree


rdd_input = dfinput.rdd.repartition(20).persist(StorageLevel.MEMORY_AND_DISK)

#åˆ›å»ºç©ºdataframe
schema = T.StructType([
        T.StructField("m_id", T.LongType(), True),
        T.StructField("s_id", T.LongType(), True),
        T.StructField("m_point", T.ArrayType(T.DoubleType(),False), True),
        T.StructField("s_point", T.ArrayType(T.DoubleType(),False), True)])
 
dfpair_raw = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)


#åˆ†æ‰¹æ¬¡è¿›è¡Œå¹¿æ’­
partition_cnt = 10
dfmaster = dfinput.repartition(partition_cnt)

for i in range(0,partition_cnt):
    rdd_master_i = dfmaster.rdd.mapPartitionsWithIndex(
        lambda idx, iterator: iterator if (idx == i ) else iter([]) )
    master_i = rdd_master_i.collect()
    idxs_i = [x["id"] for x in master_i]
    points_i = [x["point"] for x in master_i]
    tree_i = KDTree(np.array(points_i), leaf_size=40, metric='minkowski') #æ„å»ºkdtree
    broad_i = sc.broadcast((idxs_i,points_i,tree_i))
    
    def fn(iterator):
        list_res = [] #m_id,s_id,m_point,s_point
        idxs_i,points_i,tree_i = broad_i.value
        for row in iterator:
            s_id = row["id"]
            s_point = row["point"]
            index = tree_i.query_radius(np.array([s_point]), r=2*eps)[0] #æ ¹æ®kdtreeæŸ¥è¯¢ä¸€å®šèŒƒå›´å†…çš„ç‚¹
            for j in index:
                list_res.append([idxs_i[j],s_id,points_i[j],s_point])
        return iter(list_res)
    
    dfpair_raw_i = spark.createDataFrame(rdd_input.mapPartitions(fn)).toDF("m_id","s_id","m_point","s_point")
    dfpair_raw = dfpair_raw.union(dfpair_raw_i)
    
```

```python

```

**3ï¼Œæ ¹æ®DBSCANé‚»åŸŸåŠå¾„å¾—åˆ°æœ‰æ•ˆé‚»è¿‘å…³ç³» dfpair** 

```python

# æ ¹æ®DBSCANé‚»åŸŸåŠå¾„å¾—åˆ°æœ‰æ•ˆé‚»è¿‘å…³ç³» dfpair

spark.udf.register("distance", lambda p,q:((p[0]-q[0])**2+(p[1]-q[1])**2)**0.5)
dfpair = dfpair_raw.where("distance(s_point,m_point) <"+str(eps)) \
    .persist(StorageLevel.MEMORY_AND_DISK)

dfpair.show()
```

```
+----+----+--------------------+--------------------+
|m_id|s_id|             m_point|             s_point|
+----+----+--------------------+--------------------+
| 453| 190|[-0.9305763617157...|[-0.9342271003887...|
| 310| 190|[-0.8647371774956...|[-0.9342271003887...|
| 468| 191|[-1.1236573115927...|[-1.1107926480996...|
| 379| 193|[0.29384781266215...|[0.30668711048764...|
| 315| 193|[0.27137000939033...|[0.30668711048764...|
|  45| 193|[0.41589898081748...|[0.30668711048764...|
| 206| 194|[-0.0479181540769...|[-0.1280156580069...|
| 124| 194|[-0.0717324612290...|[-0.1280156580069...|
| 194| 194|[-0.1280156580069...|[-0.1280156580069...|
| 166| 194|[0.01894280256753...|[-0.1280156580069...|
| 348| 195|[0.77799441414636...|[0.91064797784686...|
| 104| 195|[0.97950311224508...|[0.91064797784686...|
| 243| 196|[-0.7132555992338...|[-0.7605955193004...|
| 482| 196|[-0.6342018198835...|[-0.7605955193004...|
| 174| 196|[-0.7070084501262...|[-0.7605955193004...|
| 196| 196|[-0.7605955193004...|[-0.7605955193004...|
| 367| 197|[0.00547311527928...|[0.02875754813338...|
| 437| 197|[0.04652353436126...|[0.02875754813338...|
| 237| 197|[0.08078546751286...|[0.02875754813338...|
| 226| 197|[0.07689272752608...|[0.02875754813338...|
+----+----+--------------------+--------------------+
only showing top 20 rows
```

```python

```

```python

```

**4ï¼Œåˆ›å»ºä¸´æ—¶èšç±»ç°‡ dfcore**

```python
dfcore = dfpair.groupBy("s_id").agg(
  F.first("s_point").alias("s_point"),
  F.count("m_id").alias("neighbour_cnt"),
  F.collect_list("m_id").alias("neighbour_ids")
).where("neighbour_cnt>="+str(min_samples)) \
 .persist(StorageLevel.MEMORY_AND_DISK)

dfcore.show(3)
```

```
+----+--------------------+-------------+--------------------+
|s_id|             s_point|neighbour_cnt|       neighbour_ids|
+----+--------------------+-------------+--------------------+
|  26|[0.95199382446206...|           25|[150, 463, 300, 5...|
| 418|[0.04187413307127...|           22|[367, 454, 226, 4...|
|  65|[0.46872165251145...|           30|[45, 402, 44, 456...|
+----+--------------------+-------------+--------------------+
only showing top 3 rows
```

```python

```

**5ï¼Œå¾—åˆ°ä¸´æ—¶èšç±»ç°‡çš„æ ¸å¿ƒç‚¹ä¿¡æ¯**

```python
dfpair_join = dfcore.selectExpr("s_id").join(dfpair,["s_id"],"inner")
df_fids = dfcore.selectExpr("s_id as m_id")
dfpair_core = df_fids.join(dfpair_join,["m_id"],"inner")
rdd_core = dfpair_core.groupBy("s_id").agg(
  F.min("m_id").alias("min_core_id"),
  F.collect_set("m_id").alias("core_id_set")
).rdd.map(lambda row: (row["min_core_id"], set(row["core_id_set"])))

rdd_core.persist(StorageLevel.MEMORY_AND_DISK)

print("before_dbscan, rdd_core.count() = ",rdd_core.count())

```

```python

```

**6ï¼Œå¯¹rdd_coreåˆ†åŒºåˆ†æ­¥åˆå¹¶  rdd_core(min_core_id, core_id_set) **

```python
#å®šä¹‰åˆå¹¶å‡½æ•°ï¼šå°†æœ‰å…±åŒæ ¸å¿ƒç‚¹çš„ä¸´æ—¶èšç±»ç°‡åˆå¹¶
def mergeSets(list_set):
    result = []
    while  len(list_set)>0 :
        cur_set = list_set.pop(0)
        intersect_idxs = [i for i in list(range(len(list_set)-1,-1,-1)) if cur_set&list_set[i]]
        while  intersect_idxs :
            for idx in intersect_idxs:
                cur_set = cur_set|list_set[idx]

            for idx in intersect_idxs:
                list_set.pop(idx)
                
            intersect_idxs = [i for i in list(range(len(list_set)-1,-1,-1)) if cur_set&list_set[i]]
        
        result = result+[cur_set]
    return result

#å¯¹rdd_coreåˆ†åŒºååœ¨æ¯ä¸ªåˆ†åŒºåˆå¹¶ï¼Œä¸æ–­å°†åˆ†åŒºæ•°é‡å‡å°‘ï¼Œæœ€ç»ˆåˆå¹¶åˆ°ä¸€ä¸ªåˆ†åŒº
#å¦‚æœæ•°æ®è§„æ¨¡ååˆ†å¤§ï¼Œéš¾ä»¥åˆå¹¶åˆ°ä¸€ä¸ªåˆ†åŒºï¼Œä¹Ÿå¯ä»¥æœ€ç»ˆåˆå¹¶åˆ°å¤šä¸ªåˆ†åŒºï¼Œå¾—åˆ°è¿‘ä¼¼ç»“æœã€‚
#rdd: (min_core_id,core_id_set)
def mergeRDD(rdd,partition_cnt):
    def fn(iterator):
        list_set = [x[1] for x in iterator]
        list_set_merged = mergeSets(list_set)
        merged_core = [(min(x),x) for x in list_set_merged] 
        return(iter(merged_core))
    rdd_merged = rdd.partitionBy(partition_cnt).mapPartitions(fn)
    return rdd_merged


#æ­¤å¤„éœ€è¦è§†å®é™…æƒ…å†µè°ƒæ•´åˆ†åŒºæ•°é‡å’Œè¿­ä»£æ¬¡æ•°
for pcnt in (16,8,4,1):
    rdd_core = mergeRDD(rdd_core,pcnt)
    
rdd_core.persist(StorageLevel.MEMORY_AND_DISK)

print("after dbscan: rdd_core.count()=",rdd_core.count())

```

```
after dbscan: rdd_core.count()= 2
```


**7, è·å–æ¯ä¸€ä¸ªcoreçš„ç°‡ä¿¡æ¯**

```python
dfcluster_ids = spark.createDataFrame(
    rdd_core.flatMap(lambda t: [(t[0], s_id) for s_id in t[1]])).toDF("cluster_id","s_id")

dfclusters =  dfcore.join(dfcluster_ids, "s_id", "left")

dfclusters.show() 
```

```
+----+--------------------+-------------+--------------------+----------+
|s_id|             s_point|neighbour_cnt|       neighbour_ids|cluster_id|
+----+--------------------+-------------+--------------------+----------+
|  26|[0.95199382446206...|           25|[150, 463, 300, 5...|         2|
|  65|[0.46872165251145...|           30|[45, 402, 44, 456...|         0|
| 418|[0.04187413307127...|           22|[367, 454, 226, 4...|         0|
| 293|[0.74589456598500...|           30|[231, 293, 153, 3...|         2|
| 243|[-0.7132555992338...|           21|[243, 482, 174, 1...|         2|
| 278|[-0.8841688633151...|           27|[453, 310, 196, 9...|         2|
| 367|[0.00547311527928...|           24|[367, 437, 454, 2...|         0|
|  19|[-0.2040816479108...|           25|[206, 124, 194, 2...|         2|
|  54|[1.86506527195881...|           22|[331, 116, 92, 54...|         0|
| 296|[1.43490708002292...|           22|[212, 199, 473, 3...|         0|
|   0|[0.31655567612451...|           22|[315, 46, 456, 42...|         0|
| 348|[0.77799441414636...|           25|[348, 402, 374, 4...|         0|
| 415|[-0.4510104506178...|           28|[363, 407, 273, 2...|         2|
| 112|[1.38118745635267...|           28|[212, 199, 473, 3...|         0|
| 113|[1.95088315015933...|           26|[306, 255, 447, 2...|         0|
| 167|[0.39542492867803...|           22|[286, 179, 109, 1...|         2|
| 385|[-0.2769033877846...|           25|[363, 407, 122, 2...|         2|
| 237|[0.08078546751286...|           29|[367, 437, 46, 23...|         0|
| 347|[-0.7336250327143...|           21|[482, 174, 196, 9...|         2|
| 330|[0.71478678633618...|           27|[231, 293, 153, 3...|         2|
+----+--------------------+-------------+--------------------+----------+
only showing top 20 rows

```


**8ï¼Œæ±‚æ¯ä¸€ä¸ªç°‡çš„ä»£è¡¨æ ¸å¿ƒå’Œç°‡å…ƒç´ æ•°é‡**

```python
rdd_cluster = dfclusters.rdd.map(
    lambda row: (row["cluster_id"],(row["s_point"],row["neighbour_cnt"],set(row["neighbour_ids"])))
)

def reduce_fn(a,b):
    id_set = a[2]|b[2]
    result = (a[0],a[1],id_set) if a[1]>=b[1] else (b[0],b[1],id_set)
    return result

rdd_result = rdd_cluster.reduceByKey(reduce_fn)

def map_fn(t):
    cluster_id = t[0]
    representation_point = t[1][0]
    neighbour_points_cnt = t[1][1]
    id_set = list(t[1][2])
    cluster_points_cnt = len(id_set)
    return (cluster_id,representation_point,neighbour_points_cnt,cluster_points_cnt,id_set)

dfresult = spark.createDataFrame(rdd_result.map(map_fn)
    ).toDF("cluster_id","representation_point","neighbour_points_cnt","cluster_points_cnt","cluster_points_ids")

dfresult.persist(StorageLevel.MEMORY_AND_DISK)

dfresult.show(3)

```

```
+----------+--------------------+--------------------+------------------+--------------------+
|cluster_id|representation_point|neighbour_points_cnt|cluster_points_cnt|  cluster_points_ids|
+----------+--------------------+--------------------+------------------+--------------------+
|         0|[1.95163238902570...|                  32|               242|[0, 1, 4, 5, 6, 1...|
|         2|[0.95067226301300...|                  34|               241|[2, 3, 7, 9, 11, ...|
+----------+--------------------+--------------------+------------------+--------------------+

```


æ³¨æ„åˆ°æˆ‘ä»¬çš„ç»“æœä¸­

èšç±»ç°‡æ•°é‡ä¸º2ä¸ªã€‚

å™ªå£°ç‚¹æ•°é‡ä¸º500-242-241 = 17ä¸ª

å’Œè°ƒç”¨sklearnä¸­çš„ç»“æœå®Œå…¨ä¸€è‡´ã€‚

```python

```

**9ï¼Œæ±‚æ¯ä¸€ä¸ªç‚¹çš„ç°‡idï¼Œå™ªå£°ç‚¹ç°‡idèµ‹å€¼ä¸º-1**

```python
rdd_clusterid = dfresult.select("cluster_id","cluster_points_ids").rdd.flatMap(
    lambda t: [(x,t["cluster_id"]) for x in t["cluster_points_ids"]])

df_clusterid = spark.createDataFrame(rdd_clusterid).toDF("id","cluster_id")
dfoutput_raw = dfinput.join(df_clusterid,"id","left")
dfoutput = dfoutput_raw.na.fill(-1)

dfoutput = dfoutput.selectExpr("id","cluster_id","point[0] as feature1","point[1] as feature2")
dfoutput.persist(StorageLevel.MEMORY_AND_DISK)
dfoutput.show()

```

```
+---+----------+--------------------+--------------------+
| id|cluster_id|            feature1|            feature2|
+---+----------+--------------------+--------------------+
| 26|         2|  0.9519938244620684|  0.2552474492493959|
| 29|         2|  -0.863622604833635|   0.756640145262391|
|474|         2| -0.4885096982719171|  1.0491748634589007|
| 65|         0|  0.4687216525114598| -0.3609345154042032|
|191|         2|  -1.110792648099675| 0.18780773522847397|
|418|         0| 0.04187413307127004| 0.25141384401180206|
|222|         0|  1.5899557693512685|-0.42942807171107017|
|270|         0|   2.178538623657351| 0.44807664826862253|
|293|         2|  0.7458945659850041|  0.5914004203001728|
|243|         2| -0.7132555992338488|  0.8089869542594612|
|278|         2| -0.8841688633151701|  0.5147890731484406|
|367|         0|0.005473115279280807| 0.12361319219864111|
|442|         2|  0.8889028924942911|0.028497180983055058|
| 19|         2| -0.2040816479108034|  0.9856890760075208|
| 54|         0|  1.8650652719588168|-0.13541631999968182|
|296|         0|   1.434907080022921| -0.4713434821495917|
|  0|         0|  0.3165556761245117|-0.04942181785843226|
|277|         0| 0.08946739589070024|-0.22831869307482952|
|287|         0|  1.7814103104861185|-0.33109829058582907|
|348|         0|   0.777994414146364| -0.4522352978300379|
+---+----------+--------------------+--------------------+
only showing top 20 rows
```


**10ï¼Œä¿å­˜å’Œå¯è§†åŒ–ç»“æœ**

```python
dfoutput.write.format("csv") \
  .option("header","true")\
  .option("inferSchema","true") \
  .option("delimiter", "\t") \
  .save("data/dbscan_output.csv")
```

```python
pdfoutput = dfoutput.toPandas()

pdfoutput.plot.scatter('feature1','feature2', s = 100,
    c = list(pdfoutput['cluster_id']),cmap = 'rainbow',colorbar = False,
    alpha = 0.6,title = 'pyspark DBSCAN cluster result');
```

![](./data/pyspark_dbscan_img.png)


**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
