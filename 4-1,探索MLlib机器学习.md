# 4-1,æ¢ç´¢MLlibæœºå™¨å­¦ä¹ 



MLlibæ˜¯Sparkçš„æœºå™¨å­¦ä¹ åº“ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸»è¦åŠŸèƒ½ã€‚

å®ç”¨å·¥å…·ï¼šçº¿æ€§ä»£æ•°ï¼Œç»Ÿè®¡ï¼Œæ•°æ®å¤„ç†ç­‰å·¥å…·
ç‰¹å¾å·¥ç¨‹ï¼šç‰¹å¾æå–ï¼Œç‰¹å¾è½¬æ¢ï¼Œç‰¹å¾é€‰æ‹©
å¸¸ç”¨ç®—æ³•ï¼šåˆ†ç±»ï¼Œå›å½’ï¼Œèšç±»ï¼ŒååŒè¿‡æ»¤ï¼Œé™ç»´
æ¨¡å‹ä¼˜åŒ–ï¼šæ¨¡å‹è¯„ä¼°ï¼Œå‚æ•°ä¼˜åŒ–ã€‚

MLlibåº“åŒ…æ‹¬ä¸¤ä¸ªä¸åŒçš„éƒ¨åˆ†ï¼š

pyspark.mllib åŒ…å«åŸºäºrddçš„æœºå™¨å­¦ä¹ ç®—æ³•APIï¼Œç›®å‰ä¸å†æ›´æ–°ï¼Œä»¥åå°†è¢«ä¸¢å¼ƒï¼Œä¸å»ºè®®ä½¿ç”¨ã€‚

pyspark.ml åŒ…å«åŸºäºDataFrameçš„æœºå™¨å­¦ä¹ ç®—æ³•APIï¼Œå¯ä»¥ç”¨æ¥æ„å»ºæœºå™¨å­¦ä¹ å·¥ä½œæµPipelineï¼Œæ¨èä½¿ç”¨ã€‚


```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark.sql import SparkSession
from pyspark.storagelevel import StorageLevel 

#SparkSQLçš„è®¸å¤šåŠŸèƒ½å°è£…åœ¨SparkSessionçš„æ–¹æ³•æ¥å£ä¸­

spark = SparkSession.builder \
        .appName("dbscan") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext
```

```python

```

### ä¸€ï¼ŒMLlibåŸºæœ¬æ¦‚å¿µ

DataFrame: MLlibä¸­æ•°æ®çš„å­˜å‚¨å½¢å¼ï¼Œå…¶åˆ—å¯ä»¥å­˜å‚¨ç‰¹å¾å‘é‡ï¼Œæ ‡ç­¾ï¼Œä»¥åŠåŸå§‹çš„æ–‡æœ¬ï¼Œå›¾åƒã€‚

Transformerï¼šè½¬æ¢å™¨ã€‚å…·æœ‰transformæ–¹æ³•ã€‚é€šè¿‡é™„åŠ ä¸€ä¸ªæˆ–å¤šä¸ªåˆ—å°†ä¸€ä¸ªDataFrameè½¬æ¢æˆå¦å¤–ä¸€ä¸ªDataFrameã€‚

Estimatorï¼šä¼°è®¡å™¨ã€‚å…·æœ‰fitæ–¹æ³•ã€‚å®ƒæ¥å—ä¸€ä¸ªDataFrameæ•°æ®ä½œä¸ºè¾“å…¥åç»è¿‡è®­ç»ƒï¼Œäº§ç”Ÿä¸€ä¸ªè½¬æ¢å™¨Transformerã€‚

Pipelineï¼šæµæ°´çº¿ã€‚å…·æœ‰setStagesæ–¹æ³•ã€‚é¡ºåºå°†å¤šä¸ªTransformerå’Œ1ä¸ªEstimatorä¸²è”èµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªæµæ°´çº¿æ¨¡å‹ã€‚

```python

```

### äºŒï¼Œ Pipelineæµæ°´çº¿èŒƒä¾‹

ä»»åŠ¡æè¿°ï¼šç”¨é€»è¾‘å›å½’æ¨¡å‹é¢„æµ‹å¥å­ä¸­æ˜¯å¦åŒ…æ‹¬â€sparkâ€œè¿™ä¸ªå•è¯ã€‚

```python
from pyspark.ml.feature import Tokenizer,HashingTF
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator
from pyspark.ml import Pipeline,PipelineModel
from pyspark.ml.linalg import Vector
from pyspark.sql import Row
```

**1ï¼Œå‡†å¤‡æ•°æ®**

```python
dftrain = spark.createDataFrame([(0,"a b c d e spark",1.0),
                (1,"a c f",0.0),
                (2,"spark hello world",1.0),
                (3,"hadoop mapreduce",0.0),
                (4,"I love spark", 1.0),
                (5,"big data",0.0)],["id","text","label"])
dftrain.show()
```

```
+---+-----------------+-----+
| id|             text|label|
+---+-----------------+-----+
|  0|  a b c d e spark|  1.0|
|  1|            a c f|  0.0|
|  2|spark hello world|  1.0|
|  3| hadoop mapreduce|  0.0|
|  4|     I love spark|  1.0|
|  5|         big data|  0.0|
+---+-----------------+-----+
```


**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
tokenizer = Tokenizer().setInputCol("text").setOutputCol("words")
print(type(tokenizer))

hashingTF = HashingTF().setNumFeatures(100) \
   .setInputCol(tokenizer.getOutputCol()) \
   .setOutputCol("features")
print(type(hashingTF))

lr = LogisticRegression().setLabelCol("label")
#print(lr.explainParams)
lr.setFeaturesCol("features").setMaxIter(10).setRegParam(0.2)
print(type(lr))

pipe = Pipeline().setStages([tokenizer,hashingTF,lr])
print(type(pipe))     


```

```
<class 'pyspark.ml.feature.Tokenizer'>
<class 'pyspark.ml.feature.HashingTF'>
<class 'pyspark.ml.classification.LogisticRegression'>
<class 'pyspark.ml.pipeline.Pipeline'>
```


**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
model = pipe.fit(dftrain)
print(type(model))

```

```
<class 'pyspark.ml.pipeline.PipelineModel'>
```

```python

```

**4ï¼Œä½¿ç”¨æ¨¡å‹**

```python
dftest = spark.createDataFrame([(7,"spark job",1.0),(9,"hello world",0.0),
                 (10,"a b c d e",0.0),(11,"you can you up",0.0),
                (12,"spark is easy to use.",1.0)]).toDF("id","text","label")
dftest.show()

dfresult = model.transform(dftest)

dfresult.selectExpr("text","features","probability","prediction").show()


```

```
+---+--------------------+-----+
| id|                text|label|
+---+--------------------+-----+
|  7|           spark job|  1.0|
|  9|         hello world|  0.0|
| 10|           a b c d e|  0.0|
| 11|      you can you up|  0.0|
| 12|spark is easy to ...|  1.0|
+---+--------------------+-----+

+--------------------+--------------------+--------------------+----------+
|                text|            features|         probability|prediction|
+--------------------+--------------------+--------------------+----------+
|           spark job|(100,[57,86],[1.0...|[0.30134853865356...|       1.0|
|         hello world|(100,[60,89],[1.0...|[0.20714372651040...|       1.0|
|           a b c d e|(100,[50,65,67,68...|[0.24502686265469...|       1.0|
|      you can you up|(100,[33,38,51],[...|[0.87589306761045...|       0.0|
|spark is easy to ...|(100,[9,21,60,86,...|[0.07662944406376...|       1.0|
+--------------------+--------------------+--------------------+----------+
```


**5ï¼Œè¯„ä¼°æ¨¡å‹**

```python
dfresult.printSchema()
```

```
root
 |-- id: long (nullable = true)
 |-- text: string (nullable = true)
 |-- label: double (nullable = true)
 |-- words: array (nullable = true)
 |    |-- element: string (containsNull = true)
 |-- features: vector (nullable = true)
 |-- rawPrediction: vector (nullable = true)
 |-- probability: vector (nullable = true)
 |-- prediction: double (nullable = false)

```

```python
evaluator = MulticlassClassificationEvaluator().setMetricName("f1") \
    .setPredictionCol("prediction").setLabelCol("label")

#print(evaluator.explainParams())
accuracy  = evaluator.evaluate(dfresult)
print("\n accuracy = {}".format(accuracy))
```

```
accuracy = 0.5666666666666667
```

```python

```

**6ï¼Œä¿å­˜æ¨¡å‹**

```python
#å¯ä»¥å°†è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜ä¸­
model.write().overwrite().save("./data/mymodel.model")

#ä¹Ÿå¯ä»¥å°†æ²¡æœ‰è®­ç»ƒçš„æ¨¡å‹ä¿å­˜åˆ°ç£ç›˜ä¸­
#pipeline.write.overwrite().save("./data/unfit-lr-model")

```

```python
#é‡æ–°è½½å…¥æ¨¡å‹
model_loaded = PipelineModel.load("./data/mymodel.model")
model_loaded.transform(dftest).select("text","label","prediction").show()
```

```
+--------------------+-----+----------+
|                text|label|prediction|
+--------------------+-----+----------+
|           spark job|  1.0|       1.0|
|         hello world|  0.0|       1.0|
|           a b c d e|  0.0|       1.0|
|      you can you up|  0.0|       0.0|
|spark is easy to ...|  1.0|       1.0|
+--------------------+-----+----------+
```

```python

```

### ä¸‰ï¼Œç‰¹å¾å·¥ç¨‹


sparkçš„ç‰¹å¾å¤„ç†åŠŸèƒ½ä¸»è¦åœ¨ pyspark.ml.feature æ¨¡å—ä¸­ï¼ŒåŒ…æ‹¬ä»¥ä¸‹ä¸€äº›åŠŸèƒ½ã€‚

* ç‰¹å¾æå–ï¼šTf-idf, Word2Vec, CountVectorizer, FeatureHasher

* ç‰¹å¾è½¬æ¢ï¼šOneHotEncoderEstimator, Normalizer, Imputer(ç¼ºå¤±å€¼å¡«å……), StandardScaler, MinMaxScaler, Tokenizer(æ„å»ºè¯å…¸), 
  StopWordsRemover, SQLTransformer, Bucketizer, Interaction(äº¤å‰é¡¹), Binarizer(äºŒå€¼åŒ–), n-gram,â€¦â€¦

* ç‰¹å¾é€‰æ‹©ï¼šVectorSlicer(å‘é‡åˆ‡ç‰‡), RFormula, ChiSqSelector(å¡æ–¹æ£€éªŒ)

* LSHè½¬æ¢ï¼šå±€éƒ¨æ•æ„Ÿå“ˆå¸Œå¹¿æ³›ç”¨äºæµ·é‡æ•°æ®ä¸­æ±‚æœ€é‚»è¿‘ï¼Œèšç±»ç­‰ç®—æ³•ã€‚



**1ï¼ŒCountVectorizer**


CountVectorizerå¯ä»¥æå–æ–‡æœ¬ä¸­çš„è¯é¢‘ç‰¹å¾ã€‚

```python
from pyspark.ml.feature import CountVectorizer, CountVectorizerModel

df = spark.createDataFrame([
  (0, ["a", "b", "c"]),
  (1, ["a", "b", "b", "c", "a"])],["id","words"])

cvModel = CountVectorizer() \
  .setInputCol("words") \
  .setOutputCol("features") \
  .setVocabSize(3) \
  .setMinDF(2) \
  .fit(df)

cvModel.transform(df).show()

```

**2ï¼ŒWord2Vec**


Word2Vecå¯ä»¥ä½¿ç”¨æµ…å±‚ç¥ç»ç½‘ç»œæå–æ–‡æœ¬ä¸­è¯çš„ç›¸ä¼¼è¯­ä¹‰ä¿¡æ¯ã€‚

```python
from pyspark.ml.feature import Word2Vec

df_document = spark.createDataFrame([
    ("Hi I heard about Spark".split(" "), ),
    ("I wish Java could use case classes".split(" "), ),
    ("Logistic regression models are neat".split(" "), )
], ["text"])

word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol="text", outputCol="result")
model = word2Vec.fit(df_document)

df_vector = model.transform(df_document)
for row in df_vector.collect():
    text, vector = row
    print("text: [%s] => \nvector: %s\n" % (", ".join(text), str(vector)))

```

```
text: [Hi, I, heard, about, Spark] => 
vector: [-0.03952452838420868,-0.019742850959300996,-0.04259629175066948]

text: [I, wish, Java, could, use, case, classes] => 
vector: [-0.017589610069990158,0.03303118874984128,-0.03793099456067596]

text: [Logistic, regression, models, are, neat] => 
vector: [-0.03930013366043568,0.08479443639516832,-0.025407366454601288]
```

```python

```

**3ï¼Œ OnHotEncoder**


OneHotEncoderå¯ä»¥å°†ç±»åˆ«ç‰¹å¾è½¬æ¢æˆOneHotç¼–ç ã€‚

```python
from pyspark.ml.feature import OneHotEncoder

df = spark.createDataFrame([
    (0.0, 1.0),
    (1.0, 0.0),
    (2.0, 1.0),
    (0.0, 2.0),
    (0.0, 1.0),
    (2.0, 0.0)
], ["categoryIndex1", "categoryIndex2"])

encoder = OneHotEncoder(inputCols=["categoryIndex1", "categoryIndex2"],
                                 outputCols=["categoryVec1", "categoryVec2"])
model = encoder.fit(df)
encoded = model.transform(df)
encoded.show()

```

```
+--------------+--------------+-------------+-------------+
|categoryIndex1|categoryIndex2| categoryVec1| categoryVec2|
+--------------+--------------+-------------+-------------+
|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|
|           1.0|           0.0|(2,[1],[1.0])|(2,[0],[1.0])|
|           2.0|           1.0|    (2,[],[])|(2,[1],[1.0])|
|           0.0|           2.0|(2,[0],[1.0])|    (2,[],[])|
|           0.0|           1.0|(2,[0],[1.0])|(2,[1],[1.0])|
|           2.0|           0.0|    (2,[],[])|(2,[0],[1.0])|
+--------------+--------------+-------------+-------------+
```


**4, MinMaxæ ‡å‡†åŒ–**

```python
from pyspark.ml.feature import MinMaxScaler
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame([
    (0, Vectors.dense([1.0, 0.1, -1.0]),),
    (1, Vectors.dense([2.0, 1.1, 1.0]),),
    (2, Vectors.dense([3.0, 10.1, 3.0]),)
], ["id", "features"])

scaler = MinMaxScaler(inputCol="features", outputCol="scaledFeatures")

scalerModel = scaler.fit(df)

df_scaled = scalerModel.transform(df)
print("Features scaled to range: [%f, %f]" % (scaler.getMin(), scaler.getMax()))
df_scaled.select("features", "scaledFeatures").show()

```

```
Features scaled to range: [0.000000, 1.000000]
+--------------+--------------+
|      features|scaledFeatures|
+--------------+--------------+
|[1.0,0.1,-1.0]|     (3,[],[])|
| [2.0,1.1,1.0]| [0.5,0.1,0.5]|
|[3.0,10.1,3.0]| [1.0,1.0,1.0]|
+--------------+--------------+
```

```python

```

**5ï¼ŒMaxAbsScaleræ ‡å‡†åŒ–**

```python
from pyspark.ml.feature import MaxAbsScaler
from pyspark.ml.linalg import Vectors

df = spark.createDataFrame([
    (0, Vectors.dense([1.0, 0.1, -8.0]),),
    (1, Vectors.dense([2.0, 1.0, -4.0]),),
    (2, Vectors.dense([4.0, 10.0, 8.0]),)
], ["id", "features"])

scaler = MaxAbsScaler(inputCol="features", outputCol="scaledFeatures")

scalerModel = scaler.fit(df)

df_rescaled = scalerModel.transform(df)

df_rescaled.select("features", "scaledFeatures").show()

```

```
+--------------+--------------------+
|      features|      scaledFeatures|
+--------------+--------------------+
|[1.0,0.1,-8.0]|[0.25,0.010000000...|
|[2.0,1.0,-4.0]|      [0.5,0.1,-0.5]|
|[4.0,10.0,8.0]|       [1.0,1.0,1.0]|
+--------------+--------------------+
```

```python

```

**6ï¼ŒSQLTransformer**


å¯ä»¥ä½¿ç”¨SQLè¯­æ³•å°†DataFrameè¿›è¡Œè½¬æ¢ï¼Œç­‰æ•ˆäºæ³¨å†Œè¡¨çš„ä½œç”¨ã€‚

ä½†å®ƒå¯ä»¥ç”¨äºPipelineä¸­ä½œä¸ºTransformer.

```python
from pyspark.ml.feature import SQLTransformer

df = spark.createDataFrame([
    (0, 1.0, 3.0),
    (2, 2.0, 5.0)
], ["id", "v1", "v2"])
sqlTrans = SQLTransformer(
    statement="SELECT *, (v1 + v2) AS v3, (v1 * v2) AS v4 FROM __THIS__")

sqlTrans.transform(df).show()
```

```
+---+---+---+---+----+
| id| v1| v2| v3|  v4|
+---+---+---+---+----+
|  0|1.0|3.0|4.0| 3.0|
|  2|2.0|5.0|7.0|10.0|
+---+---+---+---+----+
```

```python

```

**7, Imputer**


Imputerè½¬æ¢å™¨å¯ä»¥å¡«å……ç¼ºå¤±å€¼ï¼Œç¼ºå¤±å€¼å¯ä»¥ç”¨ float("nan")æ¥è¡¨ç¤ºã€‚

```python
from pyspark.ml.feature import Imputer

df = spark.createDataFrame([
    (1.0, float("nan")),
    (2.0, float("nan")),
    (float("nan"), 3.0),
    (4.0, 4.0),
    (5.0, 5.0)
], ["a", "b"])

imputer = Imputer(inputCols=["a", "b"], outputCols=["out_a", "out_b"])
model = imputer.fit(df)

model.transform(df).show()

```

```
+---+---+-----+-----+
|  a|  b|out_a|out_b|
+---+---+-----+-----+
|1.0|NaN|  1.0|  4.0|
|2.0|NaN|  2.0|  4.0|
|NaN|3.0|  3.0|  3.0|
|4.0|4.0|  4.0|  4.0|
|5.0|5.0|  5.0|  5.0|
+---+---+-----+-----+
```

```python

```

### å››ï¼Œåˆ†ç±»æ¨¡å‹


Mllibæ”¯æŒå¸¸è§çš„æœºå™¨å­¦ä¹ åˆ†ç±»æ¨¡å‹ï¼šé€»è¾‘å›å½’ï¼ŒSoftMaxå›å½’ï¼Œå†³ç­–æ ‘ï¼Œéšæœºæ£®æ—ï¼Œæ¢¯åº¦æå‡æ ‘ï¼Œçº¿æ€§æ”¯æŒå‘é‡æœºï¼Œæœ´ç´ è´å¶æ–¯ï¼ŒOne-Vs-Restï¼Œä»¥åŠå¤šå±‚æ„ŸçŸ¥æœºæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹çš„æ¥å£ä½¿ç”¨æ–¹æ³•åŸºæœ¬å¤§åŒå°å¼‚ï¼Œä¸‹é¢ä»…ä»…åˆ—ä¸¾å¸¸ç”¨çš„å†³ç­–æ ‘ï¼Œéšæœºæ£®æ—å’Œæ¢¯åº¦æå‡æ ‘çš„ä½¿ç”¨ä½œä¸ºç¤ºèŒƒã€‚æ›´å¤šèŒƒä¾‹å‚è§å®˜æ–¹æ–‡æ¡£ã€‚


**1ï¼Œå†³ç­–æ ‘**

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import DecisionTreeClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm").load("data/sample_libsvm_data.txt")
(dftrain, dftest) = dfdata.randomSplit([0.7, 0.3])

# å¯¹labelè¿›è¡Œåºå·æ ‡æ³¨ï¼Œå°†å­—ç¬¦ä¸²æ¢æˆæ•´æ•°åºå·
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(dfdata)

# å¤„ç†åˆ†ç±»ç‰¹å¾ï¼Œç±»åˆ«å¦‚æœè¶…è¿‡4å°†è§†ä¸ºè¿ç»­å€¼
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(dfdata)

# æ„å»ºä¸€ä¸ªå†³ç­–æ ‘æ¨¡å‹
dt = DecisionTreeClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures")

# æ„å»ºæµæ°´çº¿
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])

# è®­ç»ƒæµæ°´çº¿
model = pipeline.fit(dftrain)

dfpredictions = model.transform(dftest)

dfpredictions.select("prediction", "indexedLabel", "features").show(5)

# è¯„ä¼°æ¨¡å‹è¯¯å·®
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(dfpredictions)
print("Test Error = %g " % (1.0 - accuracy))
treeModel = model.stages[2]
print(treeModel)

```

```
+----------+------------+--------------------+
|prediction|indexedLabel|            features|
+----------+------------+--------------------+
|       1.0|         1.0|(692,[98,99,100,1...|
|       1.0|         1.0|(692,[124,125,126...|
|       1.0|         1.0|(692,[124,125,126...|
|       1.0|         1.0|(692,[125,126,127...|
|       1.0|         1.0|(692,[126,127,128...|
+----------+------------+--------------------+
only showing top 5 rows

Test Error = 0.037037 
DecisionTreeClassificationModel: uid=DecisionTreeClassifier_5711dbfcd91e, depth=2, numNodes=5, numClasses=2, numFeatures=692

```


**2ï¼Œéšæœºæ£®æ—**

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm").load("data/sample_libsvm_data.txt")
(dftrain, dftest) = dfdata.randomSplit([0.7, 0.3])

# å¯¹labelè¿›è¡Œåºå·æ ‡æ³¨ï¼Œå°†å­—ç¬¦ä¸²æ¢æˆæ•´æ•°åºå·
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(dfdata)

# å¤„ç†ç±»åˆ«ç‰¹å¾
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(dfdata)


# ä½¿ç”¨éšæœºæ£®æ—æ¨¡å‹
rf = RandomForestClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", numTrees=10)

# å°†labelé‡æ–°è½¬æ¢æˆå­—ç¬¦ä¸²
labelConverter = IndexToString(inputCol="prediction", outputCol="predictedLabel",
                               labels=labelIndexer.labels)

# æ„å»ºæµæ°´çº¿
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])

# è®­ç»ƒæµæ°´çº¿
model = pipeline.fit(dftrain)

# è¿›è¡Œé¢„æµ‹
dfpredictions = model.transform(dftest)

dfpredictions.select("predictedLabel", "label", "features").show(5)

# è¯„ä¼°æ¨¡å‹
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(dfpredictions)
print("Test Error = %g" % (1.0 - accuracy))

rfModel = model.stages[2]
print(rfModel)  

```

```
+--------------+-----+--------------------+
|predictedLabel|label|            features|
+--------------+-----+--------------------+
|           0.0|  0.0|(692,[122,123,124...|
|           0.0|  0.0|(692,[124,125,126...|
|           0.0|  0.0|(692,[124,125,126...|
|           0.0|  0.0|(692,[124,125,126...|
|           0.0|  0.0|(692,[124,125,126...|
+--------------+-----+--------------------+
only showing top 5 rows

Test Error = 0
RandomForestClassificationModel: uid=RandomForestClassifier_9d8f7dfec86b, numTrees=10, numClasses=2, numFeatures=692
```

```python

```

**3ï¼Œæ¢¯åº¦æå‡æ ‘**

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import GBTClassifier
from pyspark.ml.feature import StringIndexer, VectorIndexer
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm").load("data/sample_libsvm_data.txt")
(dftrain, dftest) = dfdata.randomSplit([0.7, 0.3])

# å¯¹labelè¿›è¡Œåºå·æ ‡æ³¨ï¼Œå°†å­—ç¬¦ä¸²æ¢æˆæ•´æ•°åºå·
labelIndexer = StringIndexer(inputCol="label", outputCol="indexedLabel").fit(dfdata)

# å¤„ç†ç±»åˆ«ç‰¹å¾
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(dfdata)

# ä½¿ç”¨æ¢¯åº¦æå‡æ ‘æ¨¡å‹
gbt = GBTClassifier(labelCol="indexedLabel", featuresCol="indexedFeatures", maxIter=20)

# æ„å»ºæµæ°´çº¿
pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])

# è®­ç»ƒæµæ°´çº¿
model = pipeline.fit(dftrain)

# è¿›è¡Œé¢„æµ‹
dfpredictions = model.transform(dftest)
dfpredictions.select("prediction", "indexedLabel", "features").show(5)

# è¯„ä¼°æ¨¡å‹
evaluator = MulticlassClassificationEvaluator(
    labelCol="indexedLabel", predictionCol="prediction", metricName="accuracy")
accuracy = evaluator.evaluate(dfpredictions)
print("Test Error = %g" % (1.0 - accuracy))

gbtModel = model.stages[2]
print(gbtModel)  

```

```
+----------+------------+--------------------+
|prediction|indexedLabel|            features|
+----------+------------+--------------------+
|       1.0|         1.0|(692,[95,96,97,12...|
|       1.0|         1.0|(692,[98,99,100,1...|
|       1.0|         1.0|(692,[122,123,148...|
|       1.0|         1.0|(692,[124,125,126...|
|       1.0|         1.0|(692,[124,125,126...|
+----------+------------+--------------------+
only showing top 5 rows

Test Error = 0.0689655
GBTClassificationModel: uid = GBTClassifier_e3d7713552b3, numTrees=20, numClasses=2, numFeatures=692
```

```python

```

### äº”ï¼Œå›å½’æ¨¡å‹


Mllibæ”¯æŒå¸¸è§çš„å›å½’æ¨¡å‹ï¼Œå¦‚çº¿æ€§å›å½’ï¼Œå¹¿ä¹‰çº¿æ€§å›å½’ï¼Œå†³ç­–æ ‘å›å½’ï¼Œéšæœºæ£®æ—å›å½’ï¼Œæ¢¯åº¦æå‡æ ‘å›å½’ï¼Œç”Ÿå­˜å›å½’ï¼Œä¿åºå›å½’ã€‚

ä¸‹é¢ä»…ä»¥çº¿æ€§å›å½’å’Œå†³ç­–æ ‘å›å½’ä¸ºä¾‹ã€‚

```python

```

**1ï¼Œçº¿æ€§å›å½’**

```python
from pyspark.ml.regression import LinearRegression

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm")\
   .load("data/sample_linear_regression_data.txt")

# å®šä¹‰æ¨¡å‹
lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)

# è®­ç»ƒæ¨¡å‹
lrModel = lr.fit(dfdata)

# æ¨¡å‹å‚æ•°
print("Coefficients: %s" % str(lrModel.coefficients))
print("Intercept: %s" % str(lrModel.intercept))

# è¯„ä¼°æ¨¡å‹
trainingSummary = lrModel.summary
print("numIterations: %d" % trainingSummary.totalIterations)
print("objectiveHistory: %s" % str(trainingSummary.objectiveHistory))
trainingSummary.residuals.show()
print("RMSE: %f" % trainingSummary.rootMeanSquaredError)
print("r2: %f" % trainingSummary.r2)


```

```
Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]
Intercept: 0.1598936844239736
numIterations: 7
objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]
+--------------------+
|           residuals|
+--------------------+
|  -9.889232683103197|
|  0.5533794340053554|
|  -5.204019455758823|
| -20.566686715507508|
|    -9.4497405180564|
|  -6.909112502719486|
|  -10.00431602969873|
|   2.062397807050484|
|  3.1117508432954772|
| -15.893608229419382|
|  -5.036284254673026|
|   6.483215876994333|
|  12.429497299109002|
|  -20.32003219007654|
| -2.0049838218725005|
| -17.867901734183793|
|   7.646455887420495|
| -2.2653482182417406|
|-0.10308920436195645|
|  -1.380034070385301|
+--------------------+
only showing top 20 rows

RMSE: 10.189077
r2: 0.022861

```


```python

```

**2ï¼Œå†³ç­–æ ‘å›å½’**

```python
from pyspark.ml import Pipeline
from pyspark.ml.regression import DecisionTreeRegressor
from pyspark.ml.feature import VectorIndexer
from pyspark.ml.evaluation import RegressionEvaluator

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm").load("data/sample_libsvm_data.txt")
(dftrain, dftest) = dfdata.randomSplit([0.7, 0.3])

# å¤„ç†ç±»åˆ«ç‰¹å¾
featureIndexer =\
    VectorIndexer(inputCol="features", outputCol="indexedFeatures", maxCategories=4).fit(dfdata)

# ä½¿ç”¨å†³ç­–æ ‘æ¨¡å‹
dt = DecisionTreeRegressor(featuresCol="indexedFeatures")

# æ„å»ºæµæ°´çº¿
pipeline = Pipeline(stages=[featureIndexer, dt])

# è®­ç»ƒæµæ°´çº¿
model = pipeline.fit(dftrain)

# è¿›è¡Œé¢„æµ‹
dfpredictions = model.transform(dftest)
dfpredictions.select("prediction", "label", "features").show(5)

# è¯„ä¼°æ¨¡å‹
evaluator = RegressionEvaluator(
    labelCol="label", predictionCol="prediction", metricName="rmse")
rmse = evaluator.evaluate(dfpredictions)
print("Root Mean Squared Error (RMSE) on test data = %g" % rmse)

treeModel = model.stages[1]
print(treeModel)

```

```
+----------+-----+--------------------+
|prediction|label|            features|
+----------+-----+--------------------+
|       0.0|  0.0|(692,[123,124,125...|
|       0.0|  0.0|(692,[124,125,126...|
|       0.0|  0.0|(692,[126,127,128...|
|       0.0|  0.0|(692,[126,127,128...|
|       0.0|  0.0|(692,[126,127,128...|
+----------+-----+--------------------+
only showing top 5 rows

Root Mean Squared Error (RMSE) on test data = 0
DecisionTreeRegressionModel: uid=DecisionTreeRegressor_06213a3aaeb0, depth=2, numNodes=5, numFeatures=692
```

```python


```

### å…­ï¼Œèšç±»æ¨¡å‹


Mllibæ”¯æŒçš„èšç±»æ¨¡å‹è¾ƒå°‘ï¼Œä¸»è¦æœ‰Kå‡å€¼èšç±»ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹GMMï¼Œä»¥åŠäºŒåˆ†çš„Kå‡å€¼ï¼Œéšå«ç‹„åˆ©å…‹é›·åˆ†å¸ƒLDAæ¨¡å‹ç­‰ã€‚


**1ï¼ŒKå‡å€¼èšç±»**

```python
from pyspark.ml.clustering import KMeans
from pyspark.ml.evaluation import ClusteringEvaluator

# è½½å…¥æ•°æ®
dfdata = spark.read.format("libsvm").load("data/sample_kmeans_data.txt")

# è®­ç»ƒKmeansæ¨¡å‹
kmeans = KMeans().setK(2).setSeed(1)
model = kmeans.fit(dfdata)

# è¿›è¡Œé¢„æµ‹
dfpredictions = model.transform(dfdata)

# è¯„ä¼°æ¨¡å‹
evaluator = ClusteringEvaluator()
silhouette = evaluator.evaluate(dfpredictions)
print("Silhouette with squared euclidean distance = " + str(silhouette))

# æ‰“å°ä¸­å¿ƒç‚¹
centers = model.clusterCenters()
print("Cluster Centers: ")
for center in centers:
    print(center)
```

```
Silhouette with squared euclidean distance = 0.9997530305375207
Cluster Centers: 
[9.1 9.1 9.1]
[0.1 0.1 0.1]
```

```python

```

```python

```

**2ï¼Œé«˜æ–¯æ··åˆæ¨¡å‹**

```python
from pyspark.ml.clustering import GaussianMixture

dfdata = spark.read.format("libsvm").load("data/sample_kmeans_data.txt")

gmm = GaussianMixture().setK(2).setSeed(538009335)
model = gmm.fit(dfdata)

print("Gaussians shown as a DataFrame: ")
model.gaussiansDF.show(truncate=True)


```

```
aussians shown as a DataFrame: 
+--------------------+--------------------+
|                mean|                 cov|
+--------------------+--------------------+
|[0.10000000000001...|0.006666666666806...|
|[9.09999999999998...|0.006666666666812...|
+--------------------+--------------------+
```

```python

```

**3, äºŒåˆ†Kå‡å€¼ Bisecting k-means**


Bisecting k-meansæ˜¯ä¸€ç§è‡ªä¸Šè€Œä¸‹çš„å±‚æ¬¡èšç±»ç®—æ³•ã€‚æ‰€æœ‰çš„æ ·æœ¬ç‚¹å¼€å§‹æ—¶å±äºä¸€ä¸ªcluster,ç„¶åä¸æ–­é€šè¿‡Kå‡å€¼äºŒåˆ†è£‚å¾—åˆ°å¤šä¸ªclusterã€‚

```python
from pyspark.ml.clustering import BisectingKMeans


dfdata = spark.read.format("libsvm").load("data/sample_kmeans_data.txt")

bkm = BisectingKMeans().setK(2).setSeed(1)
model = bkm.fit(dfdata)

cost = model.computeCost(dfdata)
print("Within Set Sum of Squared Errors = " + str(cost))


print("Cluster Centers: ")
centers = model.clusterCenters()
for center in centers:
    print(center)
    
```

```
Within Set Sum of Squared Errors = 0.11999999999994547
Cluster Centers: 
[0.1 0.1 0.1]
[9.1 9.1 9.1]
```

```python

```

### ä¸ƒï¼Œé™ç»´æ¨¡å‹


Mllibä¸­æ”¯æŒçš„é™ç»´æ¨¡å‹åªæœ‰ä¸»æˆåˆ†åˆ†æPCAç®—æ³•ã€‚è¿™ä¸ªæ¨¡å‹åœ¨spark.ml.featureä¸­ï¼Œé€šå¸¸ä½œä¸ºç‰¹å¾é¢„å¤„ç†çš„ä¸€ç§æŠ€å·§ä½¿ç”¨ã€‚

```python
from pyspark.ml.feature import PCA
from pyspark.ml.linalg import Vectors

data = [(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),
        (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),
        (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)]
dfdata = spark.createDataFrame(data, ["features"])

pca = PCA(k=3, inputCol="features", outputCol="pcaFeatures")
model = pca.fit(dfdata)

dfresult = model.transform(dfdata).select("pcaFeatures")
dfresult.show(truncate=False)



```

```
+-----------------------------------------------------------+
|pcaFeatures                                                |
+-----------------------------------------------------------+
|[1.6485728230883807,-4.013282700516296,-5.524543751369388] |
|[-4.645104331781534,-1.1167972663619026,-5.524543751369387]|
|[-6.428880535676489,-5.337951427775355,-5.524543751369389] |
+-----------------------------------------------------------+
```

```python

```

```python

```

### å…«ï¼Œæ¨¡å‹ä¼˜åŒ–


æ¨¡å‹ä¼˜åŒ–ä¸€èˆ¬ä¹Ÿç§°ä½œæ¨¡å‹é€‰æ‹©(Model selection)æˆ–è€…è¶…å‚è°ƒä¼˜(hyperparameter tuning)ã€‚

Mllibæ”¯æŒç½‘æ ¼æœç´¢æ–¹æ³•è¿›è¡Œè¶…å‚è°ƒä¼˜ï¼Œç›¸å…³å‡½æ•°åœ¨spark.ml.tunningæ¨¡å—ä¸­ã€‚

æœ‰ä¸¤ç§ä½¿ç”¨ç½‘æ ¼æœç´¢æ–¹æ³•çš„æ¨¡å¼ï¼Œä¸€ç§æ˜¯é€šè¿‡äº¤å‰éªŒè¯(cross-validation)æ–¹å¼è¿›è¡Œä½¿ç”¨ï¼Œå¦å¤–ä¸€ç§æ˜¯é€šè¿‡ç•™å‡ºæ³•(hold-out)æ–¹æ³•è¿›è¡Œä½¿ç”¨ã€‚

äº¤å‰éªŒè¯æ¨¡å¼ä½¿ç”¨çš„æ˜¯K-foldäº¤å‰éªŒè¯ï¼Œå°†æ•°æ®éšæœºç­‰åˆ†åˆ’åˆ†æˆKä»½ï¼Œæ¯æ¬¡å°†ä¸€ä»½ä½œä¸ºéªŒè¯é›†ï¼Œå…¶ä½™ä½œä¸ºè®­ç»ƒé›†ï¼Œæ ¹æ®Kæ¬¡éªŒè¯é›†çš„å¹³å‡ç»“æœæ¥å†³å®šè¶…å‚é€‰å–ï¼Œè®¡ç®—æˆæœ¬è¾ƒé«˜ï¼Œä½†æ˜¯ç»“æœæ›´åŠ å¯é ã€‚

è€Œç•™å‡ºæ³•åªç”¨å°†æ•°æ®éšæœºåˆ’åˆ†æˆè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œä»…æ ¹æ®éªŒè¯é›†çš„å•æ¬¡ç»“æœå†³å®šè¶…å‚é€‰å–ï¼Œç»“æœæ²¡æœ‰äº¤å‰éªŒè¯å¯é ï¼Œä½†è®¡ç®—æˆæœ¬è¾ƒä½ã€‚

å¦‚æœæ•°æ®è§„æ¨¡è¾ƒå¤§ï¼Œä¸€èˆ¬é€‰æ‹©ç•™å‡ºæ³•ï¼Œå¦‚æœæ•°æ®è§„æ¨¡è¾ƒå°ï¼Œåˆ™åº”è¯¥é€‰æ‹©äº¤å‰éªŒè¯æ¨¡å¼ã€‚



**1ï¼Œäº¤å‰éªŒè¯æ¨¡å¼**

```python
from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import HashingTF, Tokenizer
from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# å‡†å¤‡æ•°æ®
dfdata = spark.createDataFrame([
    (0, "a b c d e spark", 1.0),
    (1, "b d", 0.0),
    (2, "spark f g h", 1.0),
    (3, "hadoop mapreduce", 0.0),
    (4, "b spark who", 1.0),
    (5, "g d a y", 0.0),
    (6, "spark fly", 1.0),
    (7, "was mapreduce", 0.0),
    (8, "e spark program", 1.0),
    (9, "a e c l", 0.0),
    (10, "spark compile", 1.0),
    (11, "hadoop software", 0.0)
], ["id", "text", "label"])

# æ„å»ºæµæ°´çº¿ï¼ŒåŒ…å«ï¼š tokenizer, hashingTF, lr.
tokenizer = Tokenizer(inputCol="text", outputCol="words")
hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol="features")
lr = LogisticRegression(maxIter=10)
pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])

# ç°åœ¨æˆ‘ä»¬å°†æ•´ä¸ªæµæ°´çº¿è§†ä½œä¸€ä¸ªEstimatorè¿›è¡Œç»Ÿä¸€çš„è¶…å‚æ•°è°ƒä¼˜
# æ„å»ºç½‘æ ¼ï¼š hashingTF.numFeatures æœ‰ 3 ä¸ªå¯é€‰å€¼  and lr.regParam æœ‰2ä¸ªå¯é€‰å€¼
# æˆ‘ä»¬çš„ç½‘æ ¼ç©ºé—´æ€»å…±æœ‰2*3=6ä¸ªç‚¹éœ€è¦æœç´¢
paramGrid = ParamGridBuilder() \
    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .build()

# åˆ›å»º5æŠ˜äº¤å‰éªŒè¯è¶…å‚è°ƒä¼˜å™¨
crossval = CrossValidator(estimator=pipeline,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(),
                          numFolds=5) 

# fitåä¼šè¾“å‡ºæœ€ä¼˜çš„æ¨¡å‹
cvModel = crossval.fit(dfdata)

# å‡†å¤‡é¢„æµ‹æ•°æ®
test = spark.createDataFrame([
    (4, "spark i j k"),
    (5, "l m n"),
    (6, "mapreduce spark"),
    (7, "apache hadoop")
], ["id", "text"])

# ä½¿ç”¨æœ€ä¼˜æ¨¡å‹è¿›è¡Œé¢„æµ‹
prediction = cvModel.transform(test)
selected = prediction.select("id", "text", "probability", "prediction")
for row in selected.collect():
    print(row)
```

```
Row(id=4, text='spark i j k', probability=DenseVector([0.2661, 0.7339]), prediction=1.0)
Row(id=5, text='l m n', probability=DenseVector([0.9209, 0.0791]), prediction=0.0)
Row(id=6, text='mapreduce spark', probability=DenseVector([0.4429, 0.5571]), prediction=1.0)
Row(id=7, text='apache hadoop', probability=DenseVector([0.8584, 0.1416]), prediction=0.0)
```

```python

```

**2ï¼Œç•™å‡ºæ³•æ¨¡å¼**

```python
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit

# å‡†å¤‡æ•°æ®
dfdata = spark.read.format("libsvm")\
    .load("data/sample_linear_regression_data.txt")
dftrain, dftest = dfdata.randomSplit([0.9, 0.1], seed=12345)

lr = LinearRegression(maxIter=10)

# æ„å»ºç½‘æ ¼ä½œä¸ºè¶…å‚æ•°æœç´¢ç©ºé—´
paramGrid = ParamGridBuilder()\
    .addGrid(lr.regParam, [0.1, 0.01]) \
    .addGrid(lr.fitIntercept, [False, True])\
    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\
    .build()

# åˆ›å»ºç•™å‡ºæ³•è¶…å‚è°ƒä¼˜å™¨
tvs = TrainValidationSplit(estimator=lr,
                           estimatorParamMaps=paramGrid,
                           evaluator=RegressionEvaluator(),
                           # 80% çš„æ•°æ®ä½œä¸ºè®­ç»ƒé›†ï¼Œ20çš„æ•°æ®ä½œä¸ºéªŒè¯é›†
                           trainRatio=0.8)

# è®­ç»ƒåä¼šè¾“å‡ºæœ€ä¼˜è¶…å‚çš„æ¨¡å‹
model = tvs.fit(dftrain)

# ä½¿ç”¨æ¨¡å‹è¿›è¡Œé¢„æµ‹
model.transform(dftest)\
    .select("features", "label", "prediction")\
    .show()

```

```
+--------------------+--------------------+--------------------+
|            features|               label|          prediction|
+--------------------+--------------------+--------------------+
|(10,[0,1,2,3,4,5,...| -17.026492264209548| -1.6265106840933026|
|(10,[0,1,2,3,4,5,...|  -16.71909683360509|-0.01129960392982...|
|(10,[0,1,2,3,4,5,...| -15.375857723312297|  0.9008270143746643|
|(10,[0,1,2,3,4,5,...| -13.772441561702871|   3.435609049373433|
|(10,[0,1,2,3,4,5,...| -13.039928064104615|  0.3670260850771136|
|(10,[0,1,2,3,4,5,...|   -9.42898793151394|   -3.26399994121536|
|(10,[0,1,2,3,4,5,...|    -9.2679651250406| -0.1762581278405398|
|(10,[0,1,2,3,4,5,...|  -9.173693798406978| -0.2824541263038875|
|(10,[0,1,2,3,4,5,...| -7.1500991588127265|   3.087239142258043|
|(10,[0,1,2,3,4,5,...|  -6.930603551528371| 0.12389571117374062|
|(10,[0,1,2,3,4,5,...|  -6.456944198081549| -0.7275144195427645|
|(10,[0,1,2,3,4,5,...| -3.2843694575334834| -0.9048235164747517|
|(10,[0,1,2,3,4,5,...|   -1.99891354174786|  0.9588887587748192|
|(10,[0,1,2,3,4,5,...| -0.4683784136986876|  0.6261083785799368|
|(10,[0,1,2,3,4,5,...|-0.44652227528840105| 0.19068393875752507|
|(10,[0,1,2,3,4,5,...| 0.10157453780074743| -0.9062122256799047|
|(10,[0,1,2,3,4,5,...|  0.2105613019270259|   1.225604620956131|
|(10,[0,1,2,3,4,5,...|  2.1214592666251364|  0.2854396644518767|
|(10,[0,1,2,3,4,5,...|  2.8497179990245116|  1.3569268250561075|
|(10,[0,1,2,3,4,5,...|   3.980473021620311|  2.5359695420417965|
+--------------------+--------------------+--------------------+
only showing top 20 rows
```

```python

```

### ä¹ï¼Œå®ç”¨å·¥å…·


pyspark.ml.linalgæ¨¡å—æä¾›äº†çº¿æ€§ä»£æ•°å‘é‡å’ŒçŸ©é˜µå¯¹è±¡ã€‚

pyspark.ml.statæ¨¡å—æä¾›äº†æ•°ç†ç»Ÿè®¡è¯¸å¦‚å¡æ–¹æ£€éªŒï¼Œç›¸å…³æ€§åˆ†æç­‰åŠŸèƒ½ã€‚


**1ï¼Œå‘é‡å’ŒçŸ©é˜µ**


pyspark.ml.linalg æ”¯æŒ DenseVectorï¼ŒSparseVectorï¼ŒDenseMatrixï¼ŒSparseMatrixç±»ã€‚

å¹¶å¯ä»¥ä½¿ç”¨Matriceså’ŒVectorsæä¾›çš„å·¥å‚æ–¹æ³•åˆ›å»ºå‘é‡å’ŒçŸ©é˜µã€‚

```python
from pyspark.ml.linalg import DenseVector, SparseVector


#ç¨ å¯†å‘é‡
dense_vec = DenseVector([1, 0, 0, 2.0, 0])

print("dense_vec: ", dense_vec)
print("dense_vec.numNonzeros: ", dense_vec.numNonzeros())


#ç¨€ç–å‘é‡
#å‚æ•°åˆ†åˆ«æ˜¯ç»´åº¦ï¼Œéé›¶ç´¢å¼•ï¼Œéé›¶å…ƒç´ å€¼
sparse_vec = SparseVector(5, [0,3],[1.0,2.0])  
print("sparse_vec: ", sparse_vec)


```

```
dense_vec:  [1.0,0.0,0.0,2.0,0.0]
dense_vec.numNonzeros:  2
sparse_vec:  (5,[0,3],[1.0,2.0])
```

```python
dense_vec.toArray()
```

```
array([1., 0., 0., 2., 0.])
```

```python
from pyspark.ml.linalg import DenseMatrix, SparseMatrix

#ç¨ å¯†çŸ©é˜µ
#å‚æ•°åˆ†åˆ«æ˜¯ è¡Œæ•°ï¼Œåˆ—æ•°ï¼Œå…ƒç´ å€¼ï¼Œæ˜¯å¦è½¬ç½®(é»˜è®¤False)
dense_matrix = DenseMatrix(3, 2, [1, 3, 5, 2, 4, 6])


#ç¨€ç–çŸ©é˜µ
#å‚æ•°åˆ†åˆ«æ˜¯ è¡Œæ•°ï¼Œåˆ—æ•°ï¼Œåœ¨ç¬¬å‡ ä¸ªå…ƒç´ åˆ—ç´¢å¼•åŠ 1ï¼Œè¡Œç´¢å¼•ï¼Œéé›¶å…ƒç´ å€¼
sparse_matrix = SparseMatrix(3, 3, [0, 2, 3, 6],
    [0, 2, 1, 0, 1, 2], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0])

print("sparse_matrix.toArray(): \n", sparse_matrix.toArray())


```

```
sparse_matrix.toArray(): 
 [[1. 0. 4.]
 [0. 3. 5.]
 [2. 0. 6.]]
```

```python
from pyspark.ml.linalg import Vectors,Matrices

#å·¥å‚æ–¹æ³•
vec = Vectors.zeros(3)
matrix = Matrices.dense(2,2,[1,2,3,5])

print(matrix)
```

```
DenseMatrix([[1., 3.],
             [2., 5.]])
```

```python
    
```

**2,æ•°ç†ç»Ÿè®¡**

```python
#ç›¸å…³æ€§åˆ†æ
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import Correlation

data = [(Vectors.sparse(4, [(0, 1.0), (3, -2.0)]),),
        (Vectors.dense([4.0, 5.0, 0.0, 3.0]),),
        (Vectors.dense([6.0, 7.0, 0.0, 8.0]),),
        (Vectors.sparse(4, [(0, 9.0), (3, 1.0)]),)]
df = spark.createDataFrame(data, ["features"])

r1 = Correlation.corr(df, "features").head()
print("Pearson correlation matrix:\n" + str(r1[0]))

r2 = Correlation.corr(df, "features", "spearman").head()
print("Spearman correlation matrix:\n" + str(r2[0]))
```

```
Pearson correlation matrix:
DenseMatrix([[1.        , 0.05564149,        nan, 0.40047142],
             [0.05564149, 1.        ,        nan, 0.91359586],
             [       nan,        nan, 1.        ,        nan],
             [0.40047142, 0.91359586,        nan, 1.        ]])
Spearman correlation matrix:
DenseMatrix([[1.        , 0.10540926,        nan, 0.4       ],
             [0.10540926, 1.        ,        nan, 0.9486833 ],
             [       nan,        nan, 1.        ,        nan],
             [0.4       , 0.9486833 ,        nan, 1.        ]])
```

```python
#å¡æ–¹æ£€éªŒ
from pyspark.ml.linalg import Vectors
from pyspark.ml.stat import ChiSquareTest

data = [(0.0, Vectors.dense(0.5, 10.0)),
        (0.0, Vectors.dense(1.5, 20.0)),
        (1.0, Vectors.dense(1.5, 30.0)),
        (0.0, Vectors.dense(3.5, 30.0)),
        (0.0, Vectors.dense(3.5, 40.0)),
        (1.0, Vectors.dense(3.5, 40.0))]
df = spark.createDataFrame(data, ["label", "features"])

r = ChiSquareTest.test(df, "features", "label").head()
print("pValues: " + str(r.pValues))
print("degreesOfFreedom: " + str(r.degreesOfFreedom))
print("statistics: " + str(r.statistics))
```
```
pValues: [0.6872892787909721,0.6822703303362126]
degreesOfFreedom: [2, 3]
statistics: [0.75,1.5]

```



**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
