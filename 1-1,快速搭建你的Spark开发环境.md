# 1-1,å¿«é€Ÿæ­å»ºä½ çš„Sparkå¼€å‘ç¯å¢ƒ



### ä¸€ï¼Œæ­å»ºæœ¬åœ°pysparkå•æœºç»ƒä¹ ç¯å¢ƒ


ä»¥ä¸‹è¿‡ç¨‹æœ¬åœ°å•æœºç‰ˆpysparkç»ƒä¹ ç¼–ç¨‹ç¯å¢ƒçš„é…ç½®æ–¹æ³•ã€‚

æ³¨æ„ï¼šä»…é…ç½®ç»ƒä¹ ç¯å¢ƒæ— éœ€å®‰è£…hadoop,æ— éœ€å®‰è£…scala.


**1ï¼Œå®‰è£…Java8**

ä¸‹è½½åœ°å€ï¼šhttps://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

æ³¨æ„é¿å…å®‰è£…å…¶å®ƒç‰ˆæœ¬çš„jdkå¦åˆ™å¯èƒ½ä¼šæœ‰ä¸å…¼å®¹sparkçš„æƒ…å†µã€‚
æ³¨æ„è®¾ç½®JAVA_HOMEï¼Œå¹¶æ·»åŠ å®ƒåˆ°é»˜è®¤è·¯å¾„PATHä¸­

WINDOWSä¸‹å®‰è£…jdk8è¯¦ç»†æ•™ç¨‹å¯ä»¥å‚è€ƒï¼š

https://www.cnblogs.com/heqiyoujing/p/9502726.html

<br/>

å®‰è£…æˆåŠŸåï¼Œåœ¨å‘½ä»¤è¡Œä¸­è¾“å…¥ java -versionï¼Œå¯ä»¥çœ‹åˆ°ç±»ä¼¼å¦‚ä¸‹çš„ç»“æœã€‚

![](./data/java_versionæç¤º.png)



**2ï¼Œä¸‹è½½è§£å‹spark**

sparkå®˜ç½‘ä¸‹è½½: http://spark.apache.org/downloads.html

ç™¾åº¦äº‘ç›˜é“¾æ¥: https://pan.baidu.com/s/1mUMavclShgvigjaKwoSF_A  å¯†ç :fixh

ä¸‹è½½åè§£å‹æ”¾å…¥åˆ°ä¸€ä¸ªå¸¸ç”¨è½¯ä»¶çš„å®‰è£…è·¯å¾„ï¼Œå¦‚ï¼š

/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2



<!-- #region -->
å¯¹äºLinuxç”¨æˆ·ï¼Œå’Œmacç”¨æˆ·ï¼Œå»ºè®®åƒå¦‚ä¸‹æ–¹å¼åœ¨~/.bashrcä¸­è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œä»¥ä¾¿å¯ä»¥å¯åŠ¨spark-submitå’Œspark-shellã€‚

windowsç”¨æˆ·å¯ä»¥å¿½ç•¥ä»¥ä¸‹è®¾ç½®ã€‚

```bash

export PYTHONPATH=$/Users/liangyun/anaconda3/bin/python
export PATH="/Users/liangyun/anaconda3/bin:${PATH}"

export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_172.jdk/Contents/Home
export SPARK_HOME="/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"

export PYSPARK_PYTHON=$PYTHONPATH
export PYSPARK_DRIVER_PYTHON=$PYTHONPATH
export PYSPARK_DRIVER_PYTHON_OPTS='notebook'

```
<!-- #endregion -->

<!-- #region -->
**3ï¼Œå®‰è£…findspark**

```python
!pip install findspark
```

å®‰è£…æˆåŠŸåå¯ä»¥åœ¨jupyterä¸­è¿è¡Œå¦‚ä¸‹ä»£ç 
<!-- #endregion -->

```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

```

```python
import pyspark 
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("test").setMaster("local[4]")
sc = SparkContext(conf=conf)

print("spark version:",pyspark.__version__)
rdd = sc.parallelize(["hello","spark"])
print(rdd.reduce(lambda x,y:x+' '+y))

```

```
spark version: 3.0.1
hello spark
```

```python

```

<!-- #region -->
**4ï¼Œæ•‘å‘½æ–¹æ¡ˆ**

å¦‚æœä»¥ä¸Šè¿‡ç¨‹ç”±äºjavaç¯å¢ƒé…ç½®ç­‰å› ç´ æ²¡èƒ½æˆåŠŸå®‰è£…pysparkã€‚

å¯ä»¥åœ¨å’Œé²¸ç¤¾åŒºçš„äº‘ç«¯notebookç¯å¢ƒä¸­ç›´æ¥å­¦ä¹ pysparkã€‚

å’Œé²¸ç¤¾åŒºçš„äº‘ç«¯notebookç¯å¢ƒä¸­å·²ç»å®‰è£…å¥½äº†pysparkã€‚


https://www.kesci.com/home/column/5fe6aa955e24ed00302304e0

<!-- #endregion -->

```python

```

<!-- #region -->
### äºŒï¼Œè¿è¡Œpysparkçš„å„ç§æ–¹å¼

<br/>


pysparkä¸»è¦é€šè¿‡ä»¥ä¸‹ä¸€äº›æ–¹å¼è¿è¡Œã€‚

1ï¼Œé€šè¿‡pysparkè¿›å…¥pysparkå•æœºäº¤äº’å¼ç¯å¢ƒã€‚

è¿™ç§æ–¹å¼ä¸€èˆ¬ç”¨æ¥æµ‹è¯•ä»£ç ã€‚

ä¹Ÿå¯ä»¥æŒ‡å®šjupyteræˆ–è€…ipythonä¸ºäº¤äº’ç¯å¢ƒã€‚

<br/>


**2ï¼Œé€šè¿‡spark-submitæäº¤Sparkä»»åŠ¡åˆ°é›†ç¾¤è¿è¡Œã€‚**

è¿™ç§æ–¹å¼å¯ä»¥æäº¤Pythonè„šæœ¬æˆ–è€…JaråŒ…åˆ°é›†ç¾¤ä¸Šè®©æˆç™¾ä¸Šåƒä¸ªæœºå™¨è¿è¡Œä»»åŠ¡ã€‚

è¿™ä¹Ÿæ˜¯å·¥ä¸šç•Œç”Ÿäº§ä¸­é€šå¸¸ä½¿ç”¨sparkçš„æ–¹å¼ã€‚

<br/>

3ï¼Œé€šè¿‡zepplin notebookäº¤äº’å¼æ‰§è¡Œã€‚

zepplinæ˜¯jupyter notebookçš„apacheå¯¹åº”äº§å“ã€‚


<br/>

**4, Pythonå®‰è£…findsparkå’Œpysparkåº“ã€‚**

å¯ä»¥åœ¨jupyterå’Œå…¶å®ƒPythonç¯å¢ƒä¸­åƒè°ƒç”¨æ™®é€šåº“ä¸€æ ·åœ°è°ƒç”¨pysparkåº“ã€‚

è¿™ä¹Ÿæ˜¯æœ¬ä¹¦é…ç½®pysparkç»ƒä¹ ç¯å¢ƒçš„æ–¹å¼ã€‚



<!-- #endregion -->

```python

```

<!-- #region -->
### ä¸‰ï¼Œé€šè¿‡spark-submitæäº¤ä»»åŠ¡åˆ°é›†ç¾¤è¿è¡Œå¸¸è§é—®é¢˜

<br/>


ä»¥ä¸‹ä¸ºåœ¨é›†ç¾¤ä¸Šè¿è¡Œpysparkæ—¶ç›¸å…³çš„ä¸€äº›é—®é¢˜ï¼Œ

1ï¼Œpysparkæ˜¯å¦èƒ½å¤Ÿè°ƒç”¨Scalaæˆ–è€…Javaå¼€å‘çš„jaråŒ…ï¼Ÿ

ç­”ï¼šåªæœ‰Driverä¸­èƒ½å¤Ÿè°ƒç”¨jaråŒ…ï¼Œé€šè¿‡Py4Jè¿›è¡Œè°ƒç”¨ï¼Œåœ¨excutorsä¸­æ— æ³•è°ƒç”¨ã€‚

2ï¼Œpysparkå¦‚ä½•åœ¨excutorsä¸­å®‰è£…è¯¸å¦‚pandas,numpyç­‰åŒ…ï¼Ÿ

ç­”ï¼šå¯ä»¥é€šè¿‡condaå»ºç«‹Pythonç¯å¢ƒï¼Œç„¶åå°†å…¶å‹ç¼©æˆzipæ–‡ä»¶ä¸Šä¼ åˆ°hdfsä¸­ï¼Œå¹¶åœ¨æäº¤ä»»åŠ¡æ—¶æŒ‡å®šç¯å¢ƒã€‚
å½“ç„¶ï¼Œæœ€ç®€å•ç›´æ¥çš„æ–¹æ¡ˆæ˜¯æŠŠä½ æƒ³è¦çš„anacondaç¯å¢ƒæ‰“åŒ…æˆzipä¸Šä¼ åˆ°é›†ç¾¤hdfsç¯å¢ƒä¸­ã€‚
æ³¨æ„ï¼Œä½ æ‰“åŒ…çš„æœºå™¨åº”å½“å’Œé›†ç¾¤çš„æœºå™¨å…·æœ‰ç›¸åŒçš„linuxæ“ä½œç³»ç»Ÿã€‚


3ï¼Œpysparkå¦‚ä½•æ·»åŠ è‡ªå·±ç¼–å†™çš„å…¶å®ƒPythonè„šæœ¬åˆ°excutorsä¸­çš„PYTHONPATHä¸­ï¼Ÿ

ç­”ï¼šå¯ä»¥ç”¨py-fileså‚æ•°è®¾ç½®ï¼Œå¯ä»¥æ·»åŠ .py,.egg æˆ–è€…å‹ç¼©æˆ.zipçš„Pythonè„šæœ¬ï¼Œåœ¨excutorsä¸­å¯ä»¥importå®ƒä»¬ã€‚

4ï¼Œpysparkå¦‚ä½•æ·»åŠ ä¸€äº›é…ç½®æ–‡ä»¶åˆ°å„ä¸ªexcutorsä¸­çš„å·¥ä½œè·¯å¾„ä¸­ï¼Ÿ

ç­”ï¼šå¯ä»¥ç”¨fileså‚æ•°è®¾ç½®ï¼Œä¸åŒæ–‡ä»¶åä¹‹é—´ä»¥é€—å·åˆ†éš”ï¼Œåœ¨excutorsä¸­ç”¨SparkFiles.get(fileName)è·å–ã€‚
<!-- #endregion -->

<!-- #region -->

```bash
#æäº¤pythonå†™çš„ä»»åŠ¡
spark-submit --master yarn \
--deploy-mode cluster \
--executor-memory 12G \
--driver-memory 12G \
--num-executors 100 \
--executor-cores 2 \
--conf spark.yarn.maxAppAttempts=2 \
--conf spark.default.parallelism=1600 \
--conf spark.sql.shuffle.partitions=1600 \
--conf spark.memory.offHeap.enabled=true \
--conf spark.memory.offHeap.size=2g\
--conf spark.task.maxFailures=10 \
--conf spark.stage.maxConsecutiveAttempts=10 \
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./anaconda3.zip/anaconda3/bin/python #æŒ‡å®šexcutorsçš„Pythonç¯å¢ƒ
--conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON = ./anaconda3.zip/anaconda3/bin/python  #clusteræ¨¡å¼æ—¶å€™è®¾ç½®
--archives viewfs:///user/hadoop-xxx/yyy/anaconda3.zip #ä¸Šä¼ åˆ°hdfsçš„Pythonç¯å¢ƒ
--files  data.csv,profile.txt
--py-files  pkg.py,tqdm.py
pyspark_demo.py 
```
<!-- #endregion -->

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/Pythonä¸ç®—æ³•ä¹‹ç¾.png)
