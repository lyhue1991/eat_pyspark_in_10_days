# 2-4,7é“SparkSQLç¼–ç¨‹ç»ƒä¹ é¢˜

ä¸ºå¼ºåŒ–SparkSQLç¼–ç¨‹åŸºæœ¬åŠŸï¼Œç°æä¾›ä¸€äº›å°ç»ƒä¹ é¢˜ã€‚

è¯»è€…å¯ä»¥ä½¿ç”¨SparkSQLç¼–ç¨‹å®Œæˆè¿™äº›å°ç»ƒä¹ é¢˜ï¼Œå¹¶è¾“å‡ºç»“æœã€‚

è¿™äº›ç»ƒä¹ é¢˜åŸºæœ¬å¯ä»¥åœ¨15è¡Œä»£ç ä»¥å†…å®Œæˆï¼Œå¦‚æœé‡åˆ°å›°éš¾ï¼Œå»ºè®®å›çœ‹ä¸Šä¸€èŠ‚SparkSQLçš„ä»‹ç»ã€‚

å®Œæˆè¿™äº›ç»ƒä¹ é¢˜åï¼Œå¯ä»¥æŸ¥çœ‹æœ¬èŠ‚åé¢çš„å‚è€ƒç­”æ¡ˆï¼Œå’Œè‡ªå·±çš„å®ç°æ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”ã€‚

æˆ‘æ•¢æ‰“èµŒï¼Œè¿™äº›ç»ƒä¹ é¢˜ä¸€å®šä¼šè®©å¤§å®¶æœ‰ä¸€ç§ä¼¼æ›¾ç›¸è¯†ä¹‹æ„Ÿã€‚


```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark.sql import SparkSession

#SparkSQLçš„è®¸å¤šåŠŸèƒ½å°è£…åœ¨SparkSessionçš„æ–¹æ³•æ¥å£ä¸­

spark = SparkSession.builder \
        .appName("test") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext

```

### ä¸€ï¼Œç»ƒä¹ é¢˜åˆ—è¡¨


**1ï¼Œæ±‚å¹³å‡æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataçš„å¹³å‡å€¼
data = [1,5,7,10,23,20,6,5,10,7,10]

```

**2ï¼Œæ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ•°
data =  [1,5,7,10,23,20,6,5,10,7,10]

```

```python

```

**3ï¼Œæ±‚TopN**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score, æ‰¾å‡ºscoreæ’åå‰3çš„å­¦ç”Ÿ, scoreç›¸åŒå¯ä»¥ä»»å–
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
n = 3
```

```python

```

```python

```

**4ï¼Œæ’åºå¹¶è¿”å›åºå·**


```python
#ä»»åŠ¡ï¼šæ’åºå¹¶è¿”å›åºå·, å¤§å°ç›¸åŒçš„åºå·å¯ä»¥ä¸åŒ
data = [1,7,8,5,3,18,34,9,0,12,8]

```

```python

```

**5ï¼ŒäºŒæ¬¡æ’åº**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score
#é¦–å…ˆæ ¹æ®å­¦ç”Ÿçš„scoreä»å¤§åˆ°å°æ’åºï¼Œå¦‚æœscoreç›¸åŒï¼Œæ ¹æ®ageä»å¤§åˆ°å°
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]


```

```python

```

**6ï¼Œè¿æ¥æ“ä½œ**

```python
#ä»»åŠ¡ï¼šå·²çŸ¥ç­çº§ä¿¡æ¯è¡¨å’Œæˆç»©è¡¨ï¼Œæ‰¾å‡ºç­çº§å¹³å‡åˆ†åœ¨75åˆ†ä»¥ä¸Šçš„ç­çº§
#ç­çº§ä¿¡æ¯è¡¨åŒ…æ‹¬class,name,æˆç»©è¡¨åŒ…æ‹¬name,score

classes = [("class1","LiLei"), ("class1","HanMeiMei"),("class2","DaChui"),("class2","RuHua")]
scores = [("LiLei",76),("HanMeiMei",80),("DaChui",70),("RuHua",60)]

```

```python

```

```python

```

```python

```

**7ï¼Œåˆ†ç»„æ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬classå’Œageã€‚æ±‚æ¯ä¸ªç­çº§å­¦ç”Ÿå¹´é¾„çš„ä¼—æ•°ã€‚
students = [("class1",15),("class1",15),("class2",16),("class2",16),("class1",17),("class2",19)]


```

```python

```

```python

```

### äºŒï¼Œç»ƒä¹ é¢˜å‚è€ƒç­”æ¡ˆ

**1ï¼Œæ±‚å¹³å‡æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataçš„å¹³å‡å€¼
data = [1,5,7,10,23,20,6,5,10,7,10]

dfdata = spark.createDataFrame([(x,) for x in data]).toDF("value")
dfagg = dfdata.agg({"value":"avg"})
dfagg.show()

```

```
+-----------------+
|       avg(value)|
+-----------------+
|9.454545454545455|
+-----------------+
```


**2ï¼Œæ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ•°ï¼Œè‹¥æœ‰å¤šä¸ªï¼Œæ±‚è¿™äº›æ•°çš„å¹³å‡å€¼
from pyspark.sql import functions as F 
data =  [1,5,7,10,23,20,7,5,10,7,10]

dfdata = spark.createDataFrame([(x,1) for x in data]).toDF("key","value")
dfcount = dfdata.groupby("key").agg(F.count("value").alias("count")).cache()
max_count = dfcount.agg(F.max("count").alias("max_count")).take(1)[0]["max_count"]
dfmode = dfcount.where("count={}".format(max_count))
mode = dfmode.agg(F.expr("mean(key) as mode")).take(1)[0]["mode"]
print("mode:",mode)

dfcount.unpersist()

```

```
mode: 8.5
```

```python

```

**3ï¼Œæ±‚TopN**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score, æ‰¾å‡ºscoreæ’åå‰3çš„å­¦ç”Ÿ, scoreç›¸åŒå¯ä»¥ä»»å–
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
n = 3

dfstudents = spark.createDataFrame(students).toDF("name","age","score")
dftopn = dfstudents.orderBy("score", ascending=False).limit(n)

dftopn.show()
```

```
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 18|   87|
|HanMeiMei| 16|   77|
|      Jim| 18|   77|
+---------+---+-----+
```

```python

```

**4ï¼Œæ’åºå¹¶è¿”å›åºå·**


```python
#ä»»åŠ¡ï¼šæŒ‰ä»å°åˆ°å¤§æ’åºå¹¶è¿”å›åºå·, å¤§å°ç›¸åŒçš„åºå·å¯ä»¥ä¸åŒ

data = [1,7,8,5,3,18,34,9,0,12,8]

from copy import deepcopy
from pyspark.sql import types as T
from pyspark.sql import Row,DataFrame

def addLongIndex(df, field_name):
    schema = deepcopy(df.schema)
    schema = schema.add(T.StructField(field_name, T.LongType()))
    rdd_with_index = df.rdd.zipWithIndex()

    def merge_row(t):
        row,index= t
        dic = row.asDict() 
        dic.update({field_name:index})
        row_merged = Row(**dic)
        return row_merged

    rdd_row = rdd_with_index.map(lambda t:merge_row(t))
    return spark.createDataFrame(rdd_row,schema)

dfdata = spark.createDataFrame([(x,) for x in data]).toDF("value")
dfsorted = dfdata.sort(dfdata["value"])

dfsorted_index = addLongIndex(dfsorted,"index")

dfsorted_index.show() 

```

```
+-----+-----+
|value|index|
+-----+-----+
|    0|    0|
|    1|    1|
|    3|    2|
|    5|    3|
|    7|    4|
|    8|    5|
|    8|    6|
|    9|    7|
|   12|    8|
|   18|    9|
|   34|   10|
+-----+-----+
```

```python

```

**5ï¼ŒäºŒæ¬¡æ’åº**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score
#é¦–å…ˆæ ¹æ®å­¦ç”Ÿçš„scoreä»å¤§åˆ°å°æ’åºï¼Œå¦‚æœscoreç›¸åŒï¼Œæ ¹æ®ageä»å¤§åˆ°å°
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
dfstudents = spark.createDataFrame(students).toDF("name","age","score")
dfsorted = dfstudents.orderBy(dfstudents["score"].desc(),dfstudents["age"].desc())
dfsorted.show()

```

```
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 18|   87|
|      Jim| 18|   77|
|HanMeiMei| 16|   77|
|   DaChui| 16|   66|
|    RuHua| 18|   50|
+---------+---+-----+
```

```python

```

```python

```

**6ï¼Œè¿æ¥æ“ä½œ**

```python
#ä»»åŠ¡ï¼šå·²çŸ¥ç­çº§ä¿¡æ¯è¡¨å’Œæˆç»©è¡¨ï¼Œæ‰¾å‡ºç­çº§å¹³å‡åˆ†åœ¨75åˆ†ä»¥ä¸Šçš„ç­çº§
#ç­çº§ä¿¡æ¯è¡¨åŒ…æ‹¬class,name,æˆç»©è¡¨åŒ…æ‹¬name,score

from pyspark.sql import functions as F 
classes = [("class1","LiLei"), ("class1","HanMeiMei"),("class2","DaChui"),("class2","RuHua")]
scores = [("LiLei",76),("HanMeiMei",80),("DaChui",70),("RuHua",60)]

dfclass = spark.createDataFrame(classes).toDF("class","name")
dfscore = spark.createDataFrame(scores).toDF("name","score")

dfstudents = dfclass.join(dfscore,on ="name" ,how = "left")

dfagg = dfstudents.groupBy("class").agg(F.avg("score").alias("avg_score")).where("avg_score>75.0")
     
dfagg.show()
```

```
+------+---------+
| class|avg_score|
+------+---------+
|class1|     78.0|
+------+---------+
```

```python

```

**7ï¼Œåˆ†ç»„æ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬classå’Œageã€‚æ±‚æ¯ä¸ªç­çº§å­¦ç”Ÿå¹´é¾„çš„ä¼—æ•°ã€‚

students = [("class1",15),("class1",15),("class2",16),("class2",16),("class1",17),("class2",19)]

```

```python

from pyspark.sql import functions as F 

def mode(arr):
    dict_cnt = {}
    for x in arr:
        dict_cnt[x] = dict_cnt.get(x,0)+1
    max_cnt = max(dict_cnt.values())
    most_values = [k for k,v in dict_cnt.items() if v==max_cnt]
    s = 0.0
    for x in most_values:
        s = s + x
    return s/len(most_values)
spark.udf.register("udf_mode",mode)
dfstudents = spark.createDataFrame(students).toDF("class","score")
dfscores = dfstudents.groupBy("class").agg(F.collect_list("score").alias("scores"))
dfmode = dfscores.selectExpr("class","udf_mode(scores) as mode_score")
dfmode.show()

```

```
+------+----------+
| class|mode_score|
+------+----------+
|class2|      16.0|
|class1|      15.0|
+------+----------+

```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
