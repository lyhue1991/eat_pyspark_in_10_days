# 2-2, 7é“RDDç¼–ç¨‹ç»ƒä¹ é¢˜



ä¸ºå¼ºåŒ–RDDç¼–ç¨‹APIçš„ä½¿ç”¨ç»éªŒï¼Œç°æä¾›ä¸€äº›å°ç»ƒä¹ é¢˜ã€‚

è¯»è€…å¯ä»¥ä½¿ç”¨RDDçš„ç¼–ç¨‹APIå®Œæˆè¿™äº›å°ç»ƒä¹ é¢˜ï¼Œå¹¶è¾“å‡ºç»“æœã€‚

è¿™äº›ç»ƒä¹ é¢˜åŸºæœ¬å¯ä»¥åœ¨15è¡Œä»£ç ä»¥å†…å®Œæˆï¼Œå¦‚æœé‡åˆ°å›°éš¾ï¼Œå»ºè®®å›çœ‹ä¸Šä¸€èŠ‚RDDçš„APIä»‹ç»ã€‚

å®Œæˆè¿™äº›ç»ƒä¹ é¢˜åï¼Œå¯ä»¥æŸ¥çœ‹æœ¬èŠ‚åé¢çš„å‚è€ƒç­”æ¡ˆï¼Œå’Œè‡ªå·±çš„å®ç°æ–¹æ¡ˆè¿›è¡Œå¯¹æ¯”ã€‚


```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("rdd_tutorial").setMaster("local[4]")
sc = SparkContext(conf=conf)

print(pyspark.__version__)

```

### ä¸€ï¼Œç»ƒä¹ é¢˜åˆ—è¡¨


**1ï¼Œæ±‚å¹³å‡æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataçš„å¹³å‡å€¼
data = [1,5,7,10,23,20,6,5,10,7,10]

```

**2ï¼Œæ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ•°
data =  [1,5,7,10,23,20,6,5,10,7,10]

```

```python

```

**3ï¼Œæ±‚TopN**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score, æ‰¾å‡ºscoreæ’åå‰3çš„å­¦ç”Ÿ, scoreç›¸åŒå¯ä»¥ä»»å–
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
n = 3
```

```python

```

```python

```

**4ï¼Œæ’åºå¹¶è¿”å›åºå·**


```python
#ä»»åŠ¡ï¼šæ’åºå¹¶è¿”å›åºå·, å¤§å°ç›¸åŒçš„åºå·å¯ä»¥ä¸åŒ
data = [1,7,8,5,3,18,34,9,0,12,8]

```

```python

```

**5ï¼ŒäºŒæ¬¡æ’åº**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score
#é¦–å…ˆæ ¹æ®å­¦ç”Ÿçš„scoreä»å¤§åˆ°å°æ’åºï¼Œå¦‚æœscoreç›¸åŒï¼Œæ ¹æ®ageä»å¤§åˆ°å°
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]


```

```python

```

**6ï¼Œè¿æ¥æ“ä½œ**

```python
#ä»»åŠ¡ï¼šå·²çŸ¥ç­çº§ä¿¡æ¯è¡¨å’Œæˆç»©è¡¨ï¼Œæ‰¾å‡ºç­çº§å¹³å‡åˆ†åœ¨75åˆ†ä»¥ä¸Šçš„ç­çº§
#ç­çº§ä¿¡æ¯è¡¨åŒ…æ‹¬class,name,æˆç»©è¡¨åŒ…æ‹¬name,score

classes = [("class1","LiLei"), ("class1","HanMeiMei"),("class2","DaChui"),("class2","RuHua")]
scores = [("LiLei",76),("HanMeiMei",80),("DaChui",70),("RuHua",60)]

```

```python

```

```python

```

```python

```

**7ï¼Œåˆ†ç»„æ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬classå’Œageã€‚æ±‚æ¯ä¸ªç­çº§å­¦ç”Ÿå¹´é¾„çš„ä¼—æ•°ã€‚
students = [("class1",15),("class1",15),("class2",16),("class2",16),("class1",17),("class2",19)]


```

```python

```

```python

```

### äºŒï¼Œç»ƒä¹ é¢˜å‚è€ƒç­”æ¡ˆ

**1ï¼Œæ±‚å¹³å‡æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataçš„å¹³å‡å€¼
data = [1,5,7,10,23,20,6,5,10,7,10]

rdd_data = sc.parallelize(data)
s = rdd_data.reduce(lambda x,y:x+y+0.0)
n = rdd_data.count()
avg = s/n
print("average:",avg)

```

**2ï¼Œæ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæ±‚dataä¸­å‡ºç°æ¬¡æ•°æœ€å¤šçš„æ•°ï¼Œè‹¥æœ‰å¤šä¸ªï¼Œæ±‚è¿™äº›æ•°çš„å¹³å‡å€¼
data =  [1,5,7,10,23,20,7,5,10,7,10]

rdd_data = sc.parallelize(data)
rdd_count = rdd_data.map(lambda x:(x,1)).reduceByKey(lambda x,y:x+y)
max_count = rdd_count.map(lambda x:x[1]).reduce(lambda x,y: x if x>=y else y)
rdd_mode = rdd_count.filter(lambda x:x[1]==max_count).map(lambda x:x[0])
mode = rdd_mode.reduce(lambda x,y:x+y+0.0)/rdd_mode.count()
print("mode:",mode)
```

```python

```

**3ï¼Œæ±‚TopN**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score, æ‰¾å‡ºscoreæ’åå‰3çš„å­¦ç”Ÿ, scoreç›¸åŒå¯ä»¥ä»»å–
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
n = 3

rdd_students = sc.parallelize(students)
rdd_sorted = rdd_students.sortBy(lambda x:x[2],ascending = False)

students_topn = rdd_sorted.take(n)
print(students_topn)
```

```python

```

**4ï¼Œæ’åºå¹¶è¿”å›åºå·**


```python
#ä»»åŠ¡ï¼šæŒ‰ä»å°åˆ°å¤§æ’åºå¹¶è¿”å›åºå·, å¤§å°ç›¸åŒçš„åºå·å¯ä»¥ä¸åŒ
data = [1,7,8,5,3,18,34,9,0,12,8]

rdd_data = sc.parallelize(data)
rdd_sorted = rdd_data.map(lambda x:(x,1)).sortByKey().map(lambda x:x[0])
rdd_sorted_index = rdd_sorted.zipWithIndex()

print(rdd_sorted_index.collect())

```

```
[(0, 0), (1, 1), (3, 2), (5, 3), (7, 4), (8, 5), (8, 6), (9, 7), (12, 8), (18, 9), (34, 10)]
```

```python

```

**5ï¼ŒäºŒæ¬¡æ’åº**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬name,age,score
#é¦–å…ˆæ ¹æ®å­¦ç”Ÿçš„scoreä»å¤§åˆ°å°æ’åºï¼Œå¦‚æœscoreç›¸åŒï¼Œæ ¹æ®ageä»å¤§åˆ°å°
from student import Student
students = [("LiLei",18,87),("HanMeiMei",16,77),("DaChui",16,66),("Jim",18,77),("RuHua",18,50)]
rdd_students = sc.parallelize(students)
```

```python
%%writefile student.py
#ä¸ºäº†åœ¨RDDä¸­ä½¿ç”¨è‡ªå®šä¹‰ç±»ï¼Œéœ€è¦å°†ç±»çš„åˆ›å»ºä»£ç å…¶å†™å…¥åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå¦åˆ™ä¼šæœ‰åºåˆ—åŒ–é”™è¯¯
class Student:
    def __init__(self,name,age,score):
        self.name = name
        self.age = age
        self.score = score
    def __gt__(self,other):
        if self.score > other.score:
            return True
        elif self.score==other.score and self.age>other.age:
            return True
        else:
            return False
```

```python
rdd_sorted = rdd_students \
    .map(lambda t:Student(t[0],t[1],t[2]))\
    .sortBy(lambda x:x,ascending = False)\
    .map(lambda student:(student.name,student.age,student.score))

#å‚è€ƒæ–¹æ¡ˆï¼šæ­¤å¤„å·§å¦™åœ°å¯¹scoreå’Œageè¿›è¡Œç¼–ç æ¥è¡¨è¾¾å…¶æ’åºä¼˜å…ˆçº§å…³ç³»ï¼Œé™¤éageè¶…è¿‡100000ï¼Œä»¥ä¸‹é€»è¾‘æ— é”™è¯¯ã€‚
#rdd_sorted = rdd_students.sortBy(lambda x:100000*x[2]+x[1],ascending=False)

rdd_sorted.collect()
```

```python

```

**6ï¼Œè¿æ¥æ“ä½œ**

```python
#ä»»åŠ¡ï¼šå·²çŸ¥ç­çº§ä¿¡æ¯è¡¨å’Œæˆç»©è¡¨ï¼Œæ‰¾å‡ºç­çº§å¹³å‡åˆ†åœ¨75åˆ†ä»¥ä¸Šçš„ç­çº§
#ç­çº§ä¿¡æ¯è¡¨åŒ…æ‹¬class,name,æˆç»©è¡¨åŒ…æ‹¬name,score

classes = [("class1","LiLei"), ("class1","HanMeiMei"),("class2","DaChui"),("class2","RuHua")]
scores = [("LiLei",76),("HanMeiMei",80),("DaChui",70),("RuHua",60)]

rdd_classes = sc.parallelize(classes).map(lambda x:(x[1],x[0]))
rdd_scores = sc.parallelize(scores)
rdd_join = rdd_scores.join(rdd_classes).map(lambda t:(t[1][1],t[1][0]))

def average(iterator):
    data = list(iterator)
    s = 0.0
    for x in data:
        s = s + x
    return s/len(data)

rdd_result = rdd_join.groupByKey().map(lambda t:(t[0],average(t[1]))).filter(lambda t:t[1]>75)
print(rdd_result.collect())
```

```python

```

**7ï¼Œåˆ†ç»„æ±‚ä¼—æ•°**

```python
#ä»»åŠ¡ï¼šæœ‰ä¸€æ‰¹å­¦ç”Ÿä¿¡æ¯è¡¨æ ¼ï¼ŒåŒ…æ‹¬classå’Œageã€‚æ±‚æ¯ä¸ªç­çº§å­¦ç”Ÿå¹´é¾„çš„ä¼—æ•°ã€‚

students = [("class1",15),("class1",15),("class2",16),("class2",16),("class1",17),("class2",19)]

```

```python
def mode(arr):
    dict_cnt = {}
    for x in arr:
        dict_cnt[x] = dict_cnt.get(x,0)+1
    max_cnt = max(dict_cnt.values())
    most_values = [k for k,v in dict_cnt.items() if v==max_cnt]
    s = 0.0
    for x in most_values:
        s = s + x
    return s/len(most_values)

rdd_students = sc.parallelize(students)
rdd_classes = rdd_students.aggregateByKey([],lambda arr,x:arr+[x],lambda arr1,arr2:arr1+arr2)
rdd_mode = rdd_classes.map(lambda t:(t[0],mode(t[1])))

print(rdd_mode.collect())

```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
