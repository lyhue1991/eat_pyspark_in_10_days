# 2-1, 2å°æ—¶å…¥é—¨Sparkä¹‹RDDç¼–ç¨‹


<!-- #region -->
æœ¬èŠ‚å°†ä»‹ç»RDDæ•°æ®ç»“æ„çš„å¸¸ç”¨å‡½æ•°ã€‚
åŒ…æ‹¬å¦‚ä¸‹å†…å®¹:

* åˆ›å»ºRDD 
* å¸¸ç”¨Actionæ“ä½œ
* å¸¸ç”¨Transformationæ“ä½œ
* å¸¸ç”¨PairRDDçš„è½¬æ¢æ“ä½œ
* ç¼“å­˜æ“ä½œ
* å…±äº«å˜é‡
* åˆ†åŒºæ“ä½œ


è¿™äº›å‡½æ•°ä¸­ï¼Œæˆ‘æœ€å¸¸ç”¨çš„æ˜¯å¦‚ä¸‹15ä¸ªå‡½æ•°ï¼Œéœ€è¦è®¤çœŸæŒæ¡å…¶ç”¨æ³•ã€‚

* map
* flatMap
* mapPartitions
* filter
* count
* reduce
* take
* saveAsTextFile
* collect
* join
* union
* persist
* repartition
* reduceByKey
* aggregateByKey




<!-- #endregion -->

```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("rdd_tutorial").setMaster("local[4]")
sc = SparkContext(conf=conf)

print(pyspark.__version__)
```

```
3.0.1
```


### ä¸€ï¼Œåˆ›å»ºRDD


åˆ›å»ºRDDä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼Œä¸€ä¸ªæ˜¯textFileåŠ è½½æœ¬åœ°æˆ–è€…é›†ç¾¤æ–‡ä»¶ç³»ç»Ÿä¸­çš„æ•°æ®ï¼Œ

ç¬¬äºŒä¸ªæ˜¯ç”¨parallelizeæ–¹æ³•å°†Driverä¸­çš„æ•°æ®ç»“æ„å¹¶è¡ŒåŒ–æˆRDDã€‚

```python
#ä»æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿä¸­åŠ è½½æ•°æ®
file = "./data/hello.txt"
rdd = sc.textFile(file,3)
rdd.collect()
```

```
['hello world',
 'hello spark',
 'spark love jupyter',
 'spark love pandas',
 'spark love sql']
```

```python
#ä»é›†ç¾¤æ–‡ä»¶ç³»ç»Ÿä¸­åŠ è½½æ•°æ®
#file = "hdfs://localhost:9000/user/hadoop/data.txt"
#ä¹Ÿå¯ä»¥çœå»hdfs://localhost:9000
#rdd = sc.textFile(file,3)
```

```python
#parallelizeå°†Driverä¸­çš„æ•°æ®ç»“æ„ç”ŸæˆRDD,ç¬¬äºŒä¸ªå‚æ•°æŒ‡å®šåˆ†åŒºæ•°
rdd = sc.parallelize(range(1,11),2)
rdd.collect()
```

```
[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
```

```python

```

### äºŒï¼Œå¸¸ç”¨Actionæ“ä½œ

Actionæ“ä½œå°†è§¦å‘åŸºäºRDDä¾èµ–å…³ç³»çš„è®¡ç®—ã€‚


**collect**

```python
rdd = sc.parallelize(range(10),5) 
```

```python
#collectæ“ä½œå°†æ•°æ®æ±‡é›†åˆ°Driver,æ•°æ®è¿‡å¤§æ—¶æœ‰è¶…å†…å­˜é£é™©
all_data = rdd.collect()
all_data
```

```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```


**take**

```python
#takeæ“ä½œå°†å‰è‹¥å¹²ä¸ªæ•°æ®æ±‡é›†åˆ°Driverï¼Œç›¸æ¯”collectå®‰å…¨
rdd = sc.parallelize(range(10),5) 
part_data = rdd.take(4)
part_data
```

```
[0, 1, 2, 3]
```


**takeSample**

```python
#takeSampleå¯ä»¥éšæœºå–è‹¥å¹²ä¸ªåˆ°Driver,ç¬¬ä¸€ä¸ªå‚æ•°è®¾ç½®æ˜¯å¦æ”¾å›æŠ½æ ·
rdd = sc.parallelize(range(10),5) 
sample_data = rdd.takeSample(False,10,0)
sample_data
```

```
[7, 8, 1, 5, 3, 4, 2, 0, 9, 6]
```


**first**

```python
#firstå–ç¬¬ä¸€ä¸ªæ•°æ®
rdd = sc.parallelize(range(10),5) 
first_data = rdd.first()
print(first_data)
```

```
0
```


**count**

```python
#countæŸ¥çœ‹RDDå…ƒç´ æ•°é‡
rdd = sc.parallelize(range(10),5)
data_count = rdd.count()
print(data_count)
```

```
10
```


**reduce**

```python
#reduceåˆ©ç”¨äºŒå…ƒå‡½æ•°å¯¹æ•°æ®è¿›è¡Œè§„çº¦
rdd = sc.parallelize(range(10),5) 
rdd.reduce(lambda x,y:x+y)

```

```
45
```


**foreach**

```python
#foreachå¯¹æ¯ä¸€ä¸ªå…ƒç´ æ‰§è¡ŒæŸç§æ“ä½œï¼Œä¸ç”Ÿæˆæ–°çš„RDD
#ç´¯åŠ å™¨ç”¨æ³•è¯¦è§å…±äº«å˜é‡
rdd = sc.parallelize(range(10),5) 
accum = sc.accumulator(0)
rdd.foreach(lambda x:accum.add(x))
print(accum.value)
```

```
45
```


**countByKey**

```python
#countByKeyå¯¹Pair RDDæŒ‰keyç»Ÿè®¡æ•°é‡
pairRdd = sc.parallelize([(1,1),(1,4),(3,9),(2,16)]) 
pairRdd.countByKey()
```

```
defaultdict(int, {1: 2, 3: 1, 2: 1})
```


**saveAsTextFile**

```python
#saveAsTextFileä¿å­˜rddæˆtextæ–‡ä»¶åˆ°æœ¬åœ°
text_file = "./data/rdd.txt"
rdd = sc.parallelize(range(5))
rdd.saveAsTextFile(text_file)

```

```python
#é‡æ–°è¯»å…¥ä¼šè¢«è§£ææ–‡æœ¬
rdd_loaded = sc.textFile(file)
rdd_loaded.collect()
```

```
['2', '3', '4', '1', '0']
```

```python

```

### ä¸‰ï¼Œå¸¸ç”¨Transformationæ“ä½œ


Transformationè½¬æ¢æ“ä½œå…·æœ‰æ‡’æƒ°æ‰§è¡Œçš„ç‰¹æ€§ï¼Œå®ƒåªæŒ‡å®šæ–°çš„RDDå’Œå…¶çˆ¶RDDçš„ä¾èµ–å…³ç³»ï¼Œåªæœ‰å½“Actionæ“ä½œè§¦å‘åˆ°è¯¥ä¾èµ–çš„æ—¶å€™ï¼Œå®ƒæ‰è¢«è®¡ç®—ã€‚


**map**

```python
#mapæ“ä½œå¯¹æ¯ä¸ªå…ƒç´ è¿›è¡Œä¸€ä¸ªæ˜ å°„è½¬æ¢
rdd = sc.parallelize(range(10),3)
rdd.collect()
```

```
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]
```

```python
rdd.map(lambda x:x**2).collect()
```

```
[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
```

```python

```

**filter**

```python
#filteråº”ç”¨è¿‡æ»¤æ¡ä»¶è¿‡æ»¤æ‰ä¸€äº›æ•°æ®
rdd = sc.parallelize(range(10),3)
rdd.filter(lambda x:x>5).collect()
```

```
[6, 7, 8, 9]
```


**flatMap**

```python
#flatMapæ“ä½œæ‰§è¡Œå°†æ¯ä¸ªå…ƒç´ ç”Ÿæˆä¸€ä¸ªArrayåå‹å¹³
rdd = sc.parallelize(["hello world","hello China"])
rdd.map(lambda x:x.split(" ")).collect()
```

```
[['hello', 'world'], ['hello', 'China']]
```

```python
rdd.flatMap(lambda x:x.split(" ")).collect()
```

```
['hello', 'world', 'hello', 'China']
```

```python

```

**sample**

```python
#sampleå¯¹åŸrddåœ¨æ¯ä¸ªåˆ†åŒºæŒ‰ç…§æ¯”ä¾‹è¿›è¡ŒæŠ½æ ·,ç¬¬ä¸€ä¸ªå‚æ•°è®¾ç½®æ˜¯å¦å¯ä»¥é‡å¤æŠ½æ ·
rdd = sc.parallelize(range(10),1)
rdd.sample(False,0.5,0).collect()
```

```
[1, 4, 9]
```

```python

```

**distinct**

```python
#distinctå»é‡
rdd = sc.parallelize([1,1,2,2,3,3,4,5])
rdd.distinct().collect()
```

```
[4, 1, 5, 2, 3]
```

```python

```

**subtract**

```python
#subtractæ‰¾åˆ°å±äºå‰ä¸€ä¸ªrddè€Œä¸å±äºåä¸€ä¸ªrddçš„å…ƒç´ 
a = sc.parallelize(range(10))
b = sc.parallelize(range(5,15))
a.subtract(b).collect()
```

```
[0, 1, 2, 3, 4]
```

```python

```

**union** 

```python
#unionåˆå¹¶æ•°æ®
a = sc.parallelize(range(5))
b = sc.parallelize(range(3,8))
a.union(b).collect()
```

```
[0, 1, 2, 3, 4, 3, 4, 5, 6, 7]
```

```python

```

**intersection**

```python
#intersectionæ±‚äº¤é›†
a = sc.parallelize(range(1,6))
b = sc.parallelize(range(3,9))
a.intersection(b).collect()
```

```
[3, 4, 5]
```

```python

```

**cartesian**

```python
#cartesianç¬›å¡å°”ç§¯
boys = sc.parallelize(["LiLei","Tom"])
girls = sc.parallelize(["HanMeiMei","Lily"])
boys.cartesian(girls).collect()

```

```
[('LiLei', 'HanMeiMei'),
 ('LiLei', 'Lily'),
 ('Tom', 'HanMeiMei'),
 ('Tom', 'Lily')]
 ```

```python

```

**sortBy**

```python
#æŒ‰ç…§æŸç§æ–¹å¼è¿›è¡Œæ’åº
#æŒ‡å®šæŒ‰ç…§ç¬¬3ä¸ªå…ƒç´ å¤§å°è¿›è¡Œæ’åº
rdd = sc.parallelize([(1,2,3),(3,2,2),(4,1,1)])
rdd.sortBy(lambda x:x[2]).collect()

```

```
[(4, 1, 1), (3, 2, 2), (1, 2, 3)]
```

```python

```

**zip**

```python
#æŒ‰ç…§æ‹‰é“¾æ–¹å¼è¿æ¥ä¸¤ä¸ªRDDï¼Œæ•ˆæœç±»ä¼¼pythonçš„zipå‡½æ•°
#éœ€è¦ä¸¤ä¸ªRDDå…·æœ‰ç›¸åŒçš„åˆ†åŒºï¼Œæ¯ä¸ªåˆ†åŒºå…ƒç´ æ•°é‡ç›¸åŒ

rdd_name = sc.parallelize(["LiLei","Hanmeimei","Lily"])
rdd_age = sc.parallelize([19,18,20])

rdd_zip = rdd_name.zip(rdd_age)
print(rdd_zip.collect())
```

```
[('LiLei', 19), ('Hanmeimei', 18), ('Lily', 20)]
```

```python

```

**zipWithIndex**

```python
#å°†RDDå’Œä¸€ä¸ªä»0å¼€å§‹çš„é€’å¢åºåˆ—æŒ‰ç…§æ‹‰é“¾æ–¹å¼è¿æ¥ã€‚
rdd_name =  sc.parallelize(["LiLei","Hanmeimei","Lily","Lucy","Ann","Dachui","RuHua"])
rdd_index = rdd_name.zipWithIndex()
print(rdd_index.collect())
```

```
[('LiLei', 0), ('Hanmeimei', 1), ('Lily', 2), ('Lucy', 3), ('Ann', 4), ('Dachui', 5), ('RuHua', 6)]
```

```python

```

### å››ï¼Œå¸¸ç”¨PairRDDçš„è½¬æ¢æ“ä½œ


PairRDDæŒ‡çš„æ˜¯æ•°æ®ä¸ºé•¿åº¦ä¸º2çš„tupleç±»ä¼¼(k,v)ç»“æ„çš„æ•°æ®ç±»å‹çš„RDD,å…¶æ¯ä¸ªæ•°æ®çš„ç¬¬ä¸€ä¸ªå…ƒç´ è¢«å½“åškeyï¼Œç¬¬äºŒä¸ªå…ƒç´ è¢«å½“åšvalue. 


**reduceByKey**

```python
#reduceByKeyå¯¹ç›¸åŒçš„keyå¯¹åº”çš„valuesåº”ç”¨äºŒå…ƒå½’å¹¶æ“ä½œ
rdd = sc.parallelize([("hello",1),("world",2),
                               ("hello",3),("world",5)])
rdd.reduceByKey(lambda x,y:x+y).collect()
```

```
[('hello', 4), ('world', 7)]
```

```python

```

**groupByKey**

```python
#groupByKeyå°†ç›¸åŒçš„keyå¯¹åº”çš„valuesæ”¶é›†æˆä¸€ä¸ªIterator
rdd = sc.parallelize([("hello",1),("world",2),("hello",3),("world",5)])
rdd.groupByKey().collect()
```

```
[('hello', <pyspark.resultiterable.ResultIterable at 0x119c6ae48>),
 ('world', <pyspark.resultiterable.ResultIterable at 0x119c6a860>)]
```

```python

```

**sortByKey**

```python
#sortByKeyæŒ‰ç…§keyæ’åº,å¯ä»¥æŒ‡å®šæ˜¯å¦é™åº
rdd = sc.parallelize([("hello",1),("world",2),
                               ("China",3),("Beijing",5)])
rdd.sortByKey(False).collect()
```

```
[('world', 2), ('hello', 1), ('China', 3), ('Beijing', 5)]
```

```python

```

**join**

```python
#joinç›¸å½“äºæ ¹æ®keyè¿›è¡Œå†…è¿æ¥
age = sc.parallelize([("LiLei",18),
                        ("HanMeiMei",16),("Jim",20)])
gender = sc.parallelize([("LiLei","male"),
                        ("HanMeiMei","female"),("Lucy","female")])
age.join(gender).collect()

```

```
[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]
```

```python

```

**leftOuterJoinå’ŒrightOuterJoin**

```python
#leftOuterJoinç›¸å½“äºå…³ç³»è¡¨çš„å·¦è¿æ¥

age = sc.parallelize([("LiLei",18),
                        ("HanMeiMei",16)])
gender = sc.parallelize([("LiLei","male"),
                        ("HanMeiMei","female"),("Lucy","female")])
age.leftOuterJoin(gender).collect()

```

```
[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]
```

```python
#rightOuterJoinç›¸å½“äºå…³ç³»è¡¨çš„å³è¿æ¥
age = sc.parallelize([("LiLei",18),
                        ("HanMeiMei",16),("Jim",20)])
gender = sc.parallelize([("LiLei","male"),
                        ("HanMeiMei","female")])
age.rightOuterJoin(gender).collect()

```

```
[('LiLei', (18, 'male')), ('HanMeiMei', (16, 'female'))]
```

```python

```

```python

```

**cogroup**

```python
#cogroupç›¸å½“äºå¯¹ä¸¤ä¸ªè¾“å…¥åˆ†åˆ«goupByKeyç„¶åå†å¯¹ç»“æœè¿›è¡ŒgroupByKey

x = sc.parallelize([("a",1),("b",2),("a",3)])
y = sc.parallelize([("a",2),("b",3),("b",5)])

result = x.cogroup(y).collect()
print(result)
print(list(result[0][1][0]))
```

```
[('a', (<pyspark.resultiterable.ResultIterable object at 0x119c6acc0>, <pyspark.resultiterable.ResultIterable object at 0x119c6aba8>)), ('b', (<pyspark.resultiterable.ResultIterable object at 0x119c6a978>, <pyspark.resultiterable.ResultIterable object at 0x119c6a940>))]
[1, 3]
```

```python

```

**subtractByKey**

```python
#subtractByKeyå»é™¤xä¸­é‚£äº›keyä¹Ÿåœ¨yä¸­çš„å…ƒç´ 

x = sc.parallelize([("a",1),("b",2),("c",3)])
y = sc.parallelize([("a",2),("b",(1,2))])

x.subtractByKey(y).collect()
```

```
[('c', 3)]
```

```python

```

**foldByKey**

```python
#foldByKeyçš„æ“ä½œå’ŒreduceByKeyç±»ä¼¼ï¼Œä½†æ˜¯è¦æä¾›ä¸€ä¸ªåˆå§‹å€¼
x = sc.parallelize([("a",1),("b",2),("a",3),("b",5)],1)
x.foldByKey(1,lambda x,y:x*y).collect()

```

```
[('a', 3), ('b', 10)]
```

```python

```

```python

```

```python

```

### äº”ï¼Œç¼“å­˜æ“ä½œ

<!-- #region -->
å¦‚æœä¸€ä¸ªrddè¢«å¤šä¸ªä»»åŠ¡ç”¨ä½œä¸­é—´é‡ï¼Œé‚£ä¹ˆå¯¹å…¶è¿›è¡Œcacheç¼“å­˜åˆ°å†…å­˜ä¸­å¯¹åŠ å¿«è®¡ç®—ä¼šéå¸¸æœ‰å¸®åŠ©ã€‚

å£°æ˜å¯¹ä¸€ä¸ªrddè¿›è¡Œcacheåï¼Œè¯¥rddä¸ä¼šè¢«ç«‹å³ç¼“å­˜ï¼Œè€Œæ˜¯ç­‰åˆ°å®ƒç¬¬ä¸€æ¬¡è¢«è®¡ç®—å‡ºæ¥æ—¶æ‰è¿›è¡Œç¼“å­˜ã€‚

å¯ä»¥ä½¿ç”¨persistæ˜ç¡®æŒ‡å®šå­˜å‚¨çº§åˆ«ï¼Œå¸¸ç”¨çš„å­˜å‚¨çº§åˆ«æ˜¯MEMORY_ONLYå’ŒEMORY_AND_DISKã€‚

å¦‚æœä¸€ä¸ªRDDåé¢ä¸å†ç”¨åˆ°ï¼Œå¯ä»¥ç”¨unpersisté‡Šæ”¾ç¼“å­˜ï¼Œunpersistæ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚


ç¼“å­˜æ•°æ®ä¸ä¼šåˆ‡æ–­è¡€ç¼˜ä¾èµ–å…³ç³»ï¼Œè¿™æ˜¯å› ä¸ºç¼“å­˜æ•°æ®æŸäº›åˆ†åŒºæ‰€åœ¨çš„èŠ‚ç‚¹æœ‰å¯èƒ½ä¼šæœ‰æ•…éšœï¼Œä¾‹å¦‚å†…å­˜æº¢å‡ºæˆ–è€…èŠ‚ç‚¹æŸåã€‚

è¿™æ—¶å€™å¯ä»¥æ ¹æ®è¡€ç¼˜å…³ç³»é‡æ–°è®¡ç®—è¿™ä¸ªåˆ†åŒºçš„æ•°æ®ã€‚

<!-- #endregion -->

```python

```

```python
#cacheç¼“å­˜åˆ°å†…å­˜ä¸­ï¼Œä½¿ç”¨å­˜å‚¨çº§åˆ« MEMORY_ONLYã€‚
#MEMORY_ONLYæ„å‘³ç€å¦‚æœå†…å­˜å­˜å‚¨ä¸ä¸‹ï¼Œæ”¾å¼ƒå­˜å‚¨å…¶ä½™éƒ¨åˆ†ï¼Œéœ€è¦æ—¶é‡æ–°è®¡ç®—ã€‚
a = sc.parallelize(range(10000),5)
a.cache()
sum_a = a.reduce(lambda x,y:x+y)
cnt_a = a.count()
mean_a = sum_a/cnt_a

print(mean_a)

```

```python
#persistç¼“å­˜åˆ°å†…å­˜æˆ–ç£ç›˜ä¸­ï¼Œé»˜è®¤ä½¿ç”¨å­˜å‚¨çº§åˆ«MEMORY_AND_DISK
#MEMORY_AND_DISKæ„å‘³ç€å¦‚æœå†…å­˜å­˜å‚¨ä¸ä¸‹ï¼Œå…¶ä½™éƒ¨åˆ†å­˜å‚¨åˆ°ç£ç›˜ä¸­ã€‚
#persistå¯ä»¥æŒ‡å®šå…¶å®ƒå­˜å‚¨çº§åˆ«ï¼Œcacheç›¸å½“äºpersist(MEMORY_ONLY)
from  pyspark.storagelevel import StorageLevel
a = sc.parallelize(range(10000),5)
a.persist(StorageLevel.MEMORY_AND_DISK)
sum_a = a.reduce(lambda x,y:x+y)
cnt_a = a.count()
mean_a = sum_a/cnt_a

a.unpersist() #ç«‹å³é‡Šæ”¾ç¼“å­˜
print(mean_a)
```

```python

```

### å…­ï¼Œå…±äº«å˜é‡


å½“sparké›†ç¾¤åœ¨è®¸å¤šèŠ‚ç‚¹ä¸Šè¿è¡Œä¸€ä¸ªå‡½æ•°æ—¶ï¼Œé»˜è®¤æƒ…å†µä¸‹ä¼šæŠŠè¿™ä¸ªå‡½æ•°æ¶‰åŠåˆ°çš„å¯¹è±¡åœ¨æ¯ä¸ªèŠ‚ç‚¹ç”Ÿæˆä¸€ä¸ªå‰¯æœ¬ã€‚

ä½†æ˜¯ï¼Œæœ‰æ—¶å€™éœ€è¦åœ¨ä¸åŒèŠ‚ç‚¹æˆ–è€…èŠ‚ç‚¹å’ŒDriverä¹‹é—´å…±äº«å˜é‡ã€‚

Sparkæä¾›ä¸¤ç§ç±»å‹çš„å…±äº«å˜é‡ï¼Œå¹¿æ’­å˜é‡å’Œç´¯åŠ å™¨ã€‚

å¹¿æ’­å˜é‡æ˜¯ä¸å¯å˜å˜é‡ï¼Œå®ç°åœ¨ä¸åŒèŠ‚ç‚¹ä¸åŒä»»åŠ¡ä¹‹é—´å…±äº«æ•°æ®ã€‚

å¹¿æ’­å˜é‡åœ¨æ¯ä¸ªæœºå™¨ä¸Šç¼“å­˜ä¸€ä¸ªåªè¯»çš„å˜é‡ï¼Œè€Œä¸æ˜¯ä¸ºæ¯ä¸ªtaskç”Ÿæˆä¸€ä¸ªå‰¯æœ¬ï¼Œå¯ä»¥å‡å°‘æ•°æ®çš„ä¼ è¾“ã€‚

ç´¯åŠ å™¨ä¸»è¦æ˜¯ä¸åŒèŠ‚ç‚¹å’ŒDriverä¹‹é—´å…±äº«å˜é‡ï¼Œåªèƒ½å®ç°è®¡æ•°æˆ–è€…ç´¯åŠ åŠŸèƒ½ã€‚

ç´¯åŠ å™¨çš„å€¼åªæœ‰åœ¨Driverä¸Šæ˜¯å¯è¯»çš„ï¼Œåœ¨èŠ‚ç‚¹ä¸Šä¸å¯è§ã€‚

```python
#å¹¿æ’­å˜é‡ broadcast ä¸å¯å˜ï¼Œåœ¨æ‰€æœ‰èŠ‚ç‚¹å¯è¯»

broads = sc.broadcast(100)

rdd = sc.parallelize(range(10))
print(rdd.map(lambda x:x+broads.value).collect())

print(broads.value)
```

```
[100, 101, 102, 103, 104, 105, 106, 107, 108, 109]
100
```

```python
#ç´¯åŠ å™¨ åªèƒ½åœ¨Driverä¸Šå¯è¯»ï¼Œåœ¨å…¶å®ƒèŠ‚ç‚¹åªèƒ½è¿›è¡Œç´¯åŠ 

total = sc.accumulator(0)
rdd = sc.parallelize(range(10),3)

rdd.foreach(lambda x:total.add(x))
total.value
```

```
45
```

```python
# è®¡ç®—æ•°æ®çš„å¹³å‡å€¼
rdd = sc.parallelize([1.1,2.1,3.1,4.1])
total = sc.accumulator(0.1)
count = sc.accumulator(0)

def func(x):
    total.add(x)
    count.add(1)
    
rdd.foreach(func)

total.value/count.value
```

```
2.625
```

```python

```

### ä¸ƒï¼Œåˆ†åŒºæ“ä½œ


åˆ†åŒºæ“ä½œåŒ…æ‹¬æ”¹å˜åˆ†åŒºæ“ä½œï¼Œä»¥åŠé’ˆå¯¹åˆ†åŒºæ‰§è¡Œçš„ä¸€äº›è½¬æ¢æ“ä½œã€‚


glomï¼šå°†ä¸€ä¸ªåˆ†åŒºå†…çš„æ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªåˆ—è¡¨ä½œä¸ºä¸€è¡Œã€‚

coalesceï¼šshuffleå¯é€‰ï¼Œé»˜è®¤ä¸ºFalseæƒ…å†µä¸‹çª„ä¾èµ–ï¼Œä¸èƒ½å¢åŠ åˆ†åŒºã€‚repartitionå’ŒpartitionByè°ƒç”¨å®ƒå®ç°ã€‚

repartitionï¼šæŒ‰éšæœºæ•°è¿›è¡Œshuffleï¼Œç›¸åŒkeyä¸ä¸€å®šåœ¨åŒä¸€ä¸ªåˆ†åŒº

partitionByï¼šæŒ‰keyè¿›è¡Œshuffleï¼Œç›¸åŒkeyæ”¾å…¥åŒä¸€ä¸ªåˆ†åŒº

HashPartitionerï¼šé»˜è®¤åˆ†åŒºå™¨ï¼Œæ ¹æ®keyçš„hashå€¼è¿›è¡Œåˆ†åŒºï¼Œç›¸åŒçš„keyè¿›å…¥åŒä¸€åˆ†åŒºï¼Œæ•ˆç‡è¾ƒé«˜ï¼Œkeyä¸å¯ä¸ºArray.

RangePartitionerï¼šåªåœ¨æ’åºç›¸å…³å‡½æ•°ä¸­ä½¿ç”¨ï¼Œé™¤ç›¸åŒçš„keyè¿›å…¥åŒä¸€åˆ†åŒºï¼Œç›¸é‚»çš„keyä¹Ÿä¼šè¿›å…¥åŒä¸€åˆ†åŒºï¼Œkeyå¿…é¡»å¯æ’åºã€‚

TaskContext:  è·å–å½“å‰åˆ†åŒºidæ–¹æ³• TaskContext.get.partitionId

mapPartitionsï¼šæ¯æ¬¡å¤„ç†åˆ†åŒºå†…çš„ä¸€æ‰¹æ•°æ®ï¼Œé€‚åˆéœ€è¦åˆ†æ‰¹å¤„ç†æ•°æ®çš„æƒ…å†µï¼Œæ¯”å¦‚å°†æ•°æ®æ’å…¥æŸä¸ªè¡¨ï¼Œæ¯æ‰¹æ•°æ®åªéœ€è¦å¼€å¯ä¸€æ¬¡æ•°æ®åº“è¿æ¥ï¼Œå¤§å¤§å‡å°‘äº†è¿æ¥å¼€æ”¯

mapPartitionsWithIndexï¼šç±»ä¼¼mapPartitionsï¼Œæä¾›äº†åˆ†åŒºç´¢å¼•ï¼Œè¾“å…¥å‚æ•°ä¸ºï¼ˆiï¼ŒIteratorï¼‰

foreachPartitionï¼šç±»ä¼¼foreachï¼Œä½†æ¯æ¬¡æä¾›ä¸€ä¸ªPartitionçš„ä¸€æ‰¹æ•°æ®




**glom**

```python
#glomå°†ä¸€ä¸ªåˆ†åŒºå†…çš„æ•°æ®è½¬æ¢ä¸ºä¸€ä¸ªåˆ—è¡¨ä½œä¸ºä¸€è¡Œã€‚
a = sc.parallelize(range(10),2)
b = a.glom()
b.collect() 
```

```
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
```

```python

```

**coalesce**

```python
#coalesce é»˜è®¤shuffleä¸ºFalseï¼Œä¸èƒ½å¢åŠ åˆ†åŒºï¼Œåªèƒ½å‡å°‘åˆ†åŒº
#å¦‚æœè¦å¢åŠ åˆ†åŒºï¼Œè¦è®¾ç½®shuffle = true
#parallelizeç­‰è®¸å¤šæ“ä½œå¯ä»¥æŒ‡å®šåˆ†åŒºæ•°
a = sc.parallelize(range(10),3)  
print(a.getNumPartitions())
print(a.glom().collect())

```

```
3
[[0, 1, 2], [3, 4, 5], [6, 7, 8, 9]]
```

```python
b = a.coalesce(2) 
print(b.glom().collect())
```

```
[[0, 1, 2], [3, 4, 5, 6, 7, 8, 9]]
```

```python

```

**repartition**

```python
#repartitionæŒ‰éšæœºæ•°è¿›è¡Œshuffleï¼Œç›¸åŒkeyä¸ä¸€å®šåœ¨ä¸€ä¸ªåˆ†åŒºï¼Œå¯ä»¥å¢åŠ åˆ†åŒº
#repartitionå®é™…ä¸Šè°ƒç”¨coalesceå®ç°ï¼Œè®¾ç½®äº†shuffle = True
a = sc.parallelize(range(10),3)  
c = a.repartition(4) 
print(c.glom().collect())

```

```
[[6, 7, 8, 9], [3, 4, 5], [], [0, 1, 2]]
```

```python
#repartitionæŒ‰éšæœºæ•°è¿›è¡Œshuffleï¼Œç›¸åŒkeyä¸ä¸€å®šåœ¨ä¸€ä¸ªåˆ†åŒº
a = sc.parallelize([("a",1),("a",1),("a",2),("c",3)])  
c = a.repartition(2)
print(c.glom().collect())
```

```
[[('a', 1), ('a', 2), ('c', 3)], [('a', 1)]]
```


**partitionBy** 

```python
#partitionByæŒ‰keyè¿›è¡Œshuffleï¼Œç›¸åŒkeyä¸€å®šåœ¨ä¸€ä¸ªåˆ†åŒº
a = sc.parallelize([("a",1),("a",1),("a",2),("c",3)])  
c = a.partitionBy(2)
print(c.glom().collect())
```

```python

```

**mapPartitions**

```python
#mapPartitionså¯ä»¥å¯¹æ¯ä¸ªåˆ†åŒºåˆ†åˆ«æ‰§è¡Œæ“ä½œ
#æ¯æ¬¡å¤„ç†åˆ†åŒºå†…çš„ä¸€æ‰¹æ•°æ®ï¼Œé€‚åˆéœ€è¦æŒ‰æ‰¹å¤„ç†æ•°æ®çš„æƒ…å†µ
#ä¾‹å¦‚å°†æ•°æ®å†™å…¥æ•°æ®åº“æ—¶ï¼Œå¯ä»¥æå¤§çš„å‡å°‘è¿æ¥æ¬¡æ•°ã€‚
#mapPartitionsçš„è¾“å…¥åˆ†åŒºå†…æ•°æ®ç»„æˆçš„Iteratorï¼Œå…¶è¾“å‡ºä¹Ÿéœ€è¦æ˜¯ä¸€ä¸ªIterator
#ä»¥ä¸‹ä¾‹å­æŸ¥çœ‹æ¯ä¸ªåˆ†åŒºå†…çš„æ•°æ®,ç›¸å½“äºç”¨mapPartitionså®ç°äº†glomçš„åŠŸèƒ½ã€‚
a = sc.parallelize(range(10),2)
a.mapPartitions(lambda it:iter([list(it)])).collect()
```

```
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
```

```python

```

**mapPartitionsWithIndex**

```python
#mapPartitionsWithIndexå¯ä»¥è·å–ä¸¤ä¸ªå‚æ•°
#å³åˆ†åŒºidå’Œæ¯ä¸ªåˆ†åŒºå†…çš„æ•°æ®ç»„æˆçš„Iterator
a = sc.parallelize(range(11),2)

def func(pid,it):
    s = sum(it)
    return(iter([str(pid) + "|" + str(s)]))
    [str(pid) + "|" + str]
b = a.mapPartitionsWithIndex(func)
b.collect()
```

```python

```

```python
#åˆ©ç”¨TaskContextå¯ä»¥è·å–å½“å‰æ¯ä¸ªå…ƒç´ çš„åˆ†åŒº
from pyspark.taskcontext import TaskContext
a = sc.parallelize(range(5),3)
c = a.map(lambda x:(TaskContext.get().partitionId(),x))
c.collect()

```

```
[(0, 0), (1, 1), (1, 2), (2, 3), (2, 4)]
```


**foreachPartitions**

```python
#foreachPartitionå¯¹æ¯ä¸ªåˆ†åŒºåˆ†åˆ«æ‰§è¡Œæ“ä½œ
#èŒƒä¾‹ï¼šæ±‚æ¯ä¸ªåˆ†åŒºå†…æœ€å¤§å€¼çš„å’Œ
total = sc.accumulator(0.0)

a = sc.parallelize(range(1,101),3)

def func(it):
    total.add(max(it))
    
a.foreachPartition(func)
total.value
```

```
199.0
```

```python

```

**aggregate** 

```python
#aggregateæ˜¯ä¸€ä¸ªActionæ“ä½œ
#aggregateæ¯”è¾ƒå¤æ‚ï¼Œå…ˆå¯¹æ¯ä¸ªåˆ†åŒºæ‰§è¡Œä¸€ä¸ªå‡½æ•°ï¼Œå†å¯¹æ¯ä¸ªåˆ†åŒºç»“æœæ‰§è¡Œä¸€ä¸ªåˆå¹¶å‡½æ•°ã€‚
#ä¾‹å­ï¼šæ±‚å…ƒç´ ä¹‹å’Œä»¥åŠå…ƒç´ ä¸ªæ•°
#ä¸‰ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºåˆå§‹å€¼ï¼Œç¬¬äºŒä¸ªä¸ºåˆ†åŒºæ‰§è¡Œå‡½æ•°ï¼Œç¬¬ä¸‰ä¸ªä¸ºç»“æœåˆå¹¶æ‰§è¡Œå‡½æ•°ã€‚
rdd = sc.parallelize(range(1,21),3)
def inner_func(t,x):
    return((t[0]+x,t[1]+1))

def outer_func(p,q):
    return((p[0]+q[0],p[1]+q[1]))

rdd.aggregate((0,0),inner_func,outer_func)

```

```
(210, 20)
```

```python

```

**aggregateByKey**

```python
#aggregateByKeyçš„æ“ä½œå’Œaggregateç±»ä¼¼ï¼Œä½†æ˜¯ä¼šå¯¹æ¯ä¸ªkeyåˆ†åˆ«è¿›è¡Œæ“ä½œ
#ç¬¬ä¸€ä¸ªå‚æ•°ä¸ºåˆå§‹å€¼ï¼Œç¬¬äºŒä¸ªå‚æ•°ä¸ºåˆ†åŒºå†…å½’å¹¶å‡½æ•°ï¼Œç¬¬ä¸‰ä¸ªå‚æ•°ä¸ºåˆ†åŒºé—´å½’å¹¶å‡½æ•°

a = sc.parallelize([("a",1),("b",1),("c",2),
                             ("a",2),("b",3)],3)
b = a.aggregateByKey(0,lambda x,y:max(x,y),
                            lambda x,y:max(x,y))
b.collect()
```
```
[('b', 3), ('a', 2), ('c', 2)]
```






**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
