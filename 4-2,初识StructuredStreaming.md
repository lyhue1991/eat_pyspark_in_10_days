# 4-2,åˆè¯†StructuredStreaming



è®¾æƒ³æˆ‘ä»¬è¦è®¾è®¡ä¸€ä¸ªäº¤æ˜“æ•°æ®å±•ç¤ºç³»ç»Ÿï¼Œå®æ—¶å‘ˆç°æ¯”ç‰¹å¸æœ€è¿‘1sé’Ÿçš„æˆäº¤å‡ä»·ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡äº¤æ˜“æ•°æ®æ¥å£ä»¥éå¸¸ä½çš„å»¶è¿Ÿè·å¾—å…¨çƒå„ä¸ªæ¯”ç‰¹å¸äº¤æ˜“å¸‚åœºçš„æ¯ä¸€ç¬”æ¯”ç‰¹å¸çš„æˆäº¤ä»·ï¼Œæˆäº¤é¢ï¼Œäº¤æ˜“æ—¶é—´ã€‚

ç”±äºæ¯”ç‰¹å¸äº¤æ˜“äº‹ä»¶ä¸€ç›´åœ¨å‘ç”Ÿï¼Œæ‰€ä»¥äº¤æ˜“äº‹ä»¶è§¦å‘çš„äº¤æ˜“æ•°æ®ä¼šåƒæµæ°´ä¸€æ ·æºæºä¸æ–­åœ°é€šè¿‡äº¤æ˜“æ¥å£ä¼ ç»™æˆ‘ä»¬ã€‚

å¦‚ä½•å¯¹è¿™ç§æµå¼æ•°æ®è¿›è¡Œå®æ—¶çš„è®¡ç®—å‘¢ï¼Ÿæˆ‘ä»¬éœ€è¦ä½¿ç”¨æµè®¡ç®—å·¥å…·ï¼Œåœ¨æ•°æ®åˆ°è¾¾çš„æ—¶å€™å°±ç«‹å³å¯¹å…¶è¿›è¡Œè®¡ç®—ã€‚

å¸‚é¢ä¸Šä¸»æµçš„å¼€æºæµè®¡ç®—å·¥å…·ä¸»è¦æœ‰ Storm, Flink å’Œ Sparkã€‚

å…¶ä¸­Stormçš„å»¶è¿Ÿæœ€ä½ï¼Œä¸€èˆ¬ä¸ºå‡ æ¯«ç§’åˆ°å‡ åæ¯«ç§’ï¼Œä½†æ•°æ®ååé‡è¾ƒä½ï¼Œæ¯ç§’èƒ½å¤Ÿå¤„ç†çš„äº‹ä»¶åœ¨å‡ åä¸‡å·¦å³ï¼Œå»ºè®¾æˆæœ¬é«˜ã€‚

Flinkæ˜¯ç›®å‰å›½å†…äº’è”ç½‘å‚å•†ä¸»è¦ä½¿ç”¨çš„æµè®¡ç®—å·¥å…·ï¼Œå»¶è¿Ÿä¸€èˆ¬åœ¨å‡ ååˆ°å‡ ç™¾æ¯«ç§’ï¼Œæ•°æ®ååé‡éå¸¸é«˜ï¼Œæ¯ç§’èƒ½å¤„ç†çš„äº‹ä»¶å¯ä»¥è¾¾åˆ°å‡ ç™¾ä¸Šåƒä¸‡ï¼Œå»ºè®¾æˆæœ¬ä½ã€‚

Sparké€šè¿‡Spark Streamingæˆ–Spark Structured Streamingæ”¯æŒæµè®¡ç®—ã€‚ä½†Sparkçš„æµè®¡ç®—æ˜¯å°†æµæ•°æ®æŒ‰ç…§æ—¶é—´åˆ†å‰²æˆä¸€ä¸ªä¸€ä¸ªçš„å°æ‰¹æ¬¡(mini-batch)è¿›è¡Œå¤„ç†çš„ï¼Œå…¶å»¶è¿Ÿä¸€èˆ¬åœ¨1ç§’å·¦å³ã€‚ååé‡å’ŒFlinkç›¸å½“ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯Spark Structured Streaming ç°åœ¨ä¹Ÿæ”¯æŒäº†Continous Streaming æ¨¡å¼ï¼Œå³åœ¨æ•°æ®åˆ°è¾¾æ—¶å°±è¿›è¡Œè®¡ç®—ï¼Œä¸è¿‡ç›®å‰è¿˜å¤„äºæµ‹è¯•é˜¶æ®µï¼Œä¸æ˜¯ç‰¹åˆ«æˆç†Ÿã€‚

è™½ç„¶ä»ç›®å‰æ¥çœ‹ï¼Œåœ¨æµè®¡ç®—æ–¹é¢ï¼ŒFlinkæ¯”Sparkæ›´å…·æ€§èƒ½ä¼˜åŠ¿ï¼Œæ˜¯å½“ä¹‹æ— æ„§çš„ç‹è€…ã€‚ä½†ç”±äºSparkæ‹¥æœ‰æ¯”Flinkæ›´åŠ æ´»è·ƒçš„ç¤¾åŒºï¼Œå…¶æµè®¡ç®—åŠŸèƒ½ä¹Ÿåœ¨ä¸æ–­åœ°å®Œå–„å’Œå‘å±•ï¼Œæœªæ¥åœ¨æµè®¡ç®—é¢†åŸŸæˆ–è®¸è¶³ä»¥æŒ‘æˆ˜Flinkçš„ç‹è€…åœ°ä½ã€‚


```python
import pyspark
from pyspark.sql import SparkSession
from pyspark.sql import types as T
from pyspark.sql import functions as F 
import time,os,random

#æœ¬æ–‡ä¸»è¦ç”¨å°æ•°æ®æµ‹è¯•ï¼Œè®¾ç½®è¾ƒå°çš„åˆ†åŒºæ•°å¯ä»¥è·å¾—æ›´é«˜æ€§èƒ½
spark = SparkSession.builder \
        .appName("structured streaming") \
        .config("spark.sql.shuffle.partitions","8") \
        .config("spark.default.parallelism","8") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext

```

### ä¸€ï¼ŒStructured Streaming åŸºæœ¬æ¦‚å¿µ


**æµè®¡ç®—(Streaming)å’Œæ‰¹è®¡ç®—(Batch)**:

æ‰¹è®¡ç®—æˆ–æ‰¹å¤„ç†æ˜¯å¤„ç†ç¦»çº¿æ•°æ®ã€‚å•ä¸ªå¤„ç†æ•°æ®é‡å¤§ï¼Œå¤„ç†é€Ÿåº¦æ¯”è¾ƒæ…¢ã€‚

æµè®¡ç®—æ˜¯å¤„ç†åœ¨çº¿å®æ—¶äº§ç”Ÿçš„æ•°æ®ã€‚å•æ¬¡å¤„ç†çš„æ•°æ®é‡å°ï¼Œä½†å¤„ç†é€Ÿåº¦æ›´å¿«ã€‚

<br/>


**Spark Streaming å’Œ Spark Structured Streaming**:

Sparkåœ¨2.0ä¹‹å‰ï¼Œä¸»è¦ä½¿ç”¨çš„Spark Streamingæ¥æ”¯æŒæµè®¡ç®—ï¼Œå…¶æ•°æ®ç»“æ„æ¨¡å‹ä¸ºDStreamï¼Œå…¶å®å°±æ˜¯ä¸€ä¸ªä¸ªå°æ‰¹æ¬¡æ•°æ®æ„æˆçš„RDDé˜Ÿåˆ—ã€‚

ç›®å‰ï¼ŒSparkä¸»è¦æ¨èçš„æµè®¡ç®—æ¨¡å—æ˜¯Structured Streamingï¼Œå…¶æ•°æ®ç»“æ„æ¨¡å‹æ˜¯Unbounded DataFrameï¼Œå³æ²¡æœ‰è¾¹ç•Œçš„æ•°æ®è¡¨ã€‚

ç›¸æ¯”äº Spark Streaming å»ºç«‹åœ¨ RDDæ•°æ®ç»“æ„ä¸Šé¢ï¼ŒStructured Streaming æ˜¯å»ºç«‹åœ¨ SparkSQLåŸºç¡€ä¸Šï¼ŒDataFrameçš„ç»å¤§éƒ¨åˆ†APIä¹Ÿèƒ½å¤Ÿç”¨åœ¨æµè®¡ç®—ä¸Šï¼Œå®ç°äº†æµè®¡ç®—å’Œæ‰¹å¤„ç†çš„ä¸€ä½“åŒ–ï¼Œå¹¶ä¸”ç”±äºSparkSQLçš„ä¼˜åŒ–ï¼Œå…·æœ‰æ›´å¥½çš„æ€§èƒ½ï¼Œå®¹é”™æ€§ä¹Ÿæ›´å¥½ã€‚

<br/>


**source å’Œ sink**:

sourceå³æµæ•°æ®ä»ä½•è€Œæ¥ã€‚åœ¨Spark Structured Streaming ä¸­ï¼Œä¸»è¦å¯ä»¥ä»ä»¥ä¸‹æ–¹å¼æ¥å…¥æµæ•°æ®ã€‚

1, Kafka Sourceã€‚å½“æ¶ˆæ¯ç”Ÿäº§è€…å‘é€çš„æ¶ˆæ¯åˆ°è¾¾æŸä¸ªtopicçš„æ¶ˆæ¯é˜Ÿåˆ—æ—¶ï¼Œå°†è§¦å‘è®¡ç®—ã€‚è¿™æ˜¯structured Streaming æœ€å¸¸ç”¨çš„æµæ•°æ®æ¥æºã€‚

2, File Sourceã€‚å½“è·¯å¾„ä¸‹æœ‰æ–‡ä»¶è¢«æ›´æ–°æ—¶ï¼Œå°†è§¦å‘è®¡ç®—ã€‚è¿™ç§æ–¹å¼é€šå¸¸è¦æ±‚æ–‡ä»¶åˆ°è¾¾è·¯å¾„æ˜¯åŸå­æ€§(ç¬é—´åˆ°è¾¾ï¼Œä¸æ˜¯æ…¢æ…¢å†™å…¥)çš„ï¼Œä»¥ç¡®ä¿è¯»å–åˆ°æ•°æ®çš„å®Œæ•´æ€§ã€‚åœ¨å¤§éƒ¨åˆ†æ–‡ä»¶ç³»ç»Ÿä¸­ï¼Œå¯ä»¥é€šè¿‡moveæ“ä½œå®ç°è¿™ä¸ªç‰¹æ€§ã€‚

3, Socket Sourceã€‚éœ€è¦åˆ¶å®šhoståœ°å€å’Œportç«¯å£å·ã€‚è¿™ç§æ–¹å¼ä¸€èˆ¬åªç”¨æ¥æµ‹è¯•ä»£ç ã€‚linuxç¯å¢ƒä¸‹å¯ä»¥ç”¨ncå‘½ä»¤æ¥å¼€å¯ç½‘ç»œé€šä¿¡ç«¯å£å‘é€æ¶ˆæ¯æµ‹è¯•ã€‚

sinkå³æµæ•°æ®è¢«å¤„ç†åä»ä½•è€Œå»ã€‚åœ¨Spark Structured Streaming ä¸­ï¼Œä¸»è¦å¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼è¾“å‡ºæµæ•°æ®è®¡ç®—ç»“æœã€‚

1, Kafka Sinkã€‚å°†å¤„ç†åçš„æµæ•°æ®è¾“å‡ºåˆ°kafkaæŸä¸ªæˆ–æŸäº›topicä¸­ã€‚

2, File Sinkã€‚ å°†å¤„ç†åçš„æµæ•°æ®å†™å…¥åˆ°æ–‡ä»¶ç³»ç»Ÿä¸­ã€‚

3, ForeachBatch Sinkã€‚ å¯¹äºæ¯ä¸€ä¸ªmicro-batchçš„æµæ•°æ®å¤„ç†åçš„ç»“æœï¼Œç”¨æˆ·å¯ä»¥ç¼–å†™å‡½æ•°å®ç°è‡ªå®šä¹‰å¤„ç†é€»è¾‘ã€‚ä¾‹å¦‚å†™å…¥åˆ°å¤šä¸ªæ–‡ä»¶ä¸­ï¼Œæˆ–è€…å†™å…¥åˆ°æ–‡ä»¶å¹¶æ‰“å°ã€‚

4ï¼Œ Foreach Sinkã€‚ä¸€èˆ¬åœ¨Continuousè§¦å‘æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œç”¨æˆ·ç¼–å†™å‡½æ•°å®ç°æ¯ä¸€è¡Œçš„å¤„ç†å¤„ç†ã€‚

5ï¼ŒConsole Sinkã€‚æ‰“å°åˆ°Driverç«¯æ§åˆ¶å°ï¼Œå¦‚æœæ—¥å¿—é‡å¤§ï¼Œè°¨æ…ä½¿ç”¨ã€‚ä¸€èˆ¬ä¾›è°ƒè¯•ä½¿ç”¨ã€‚

6ï¼ŒMemory Sinkã€‚è¾“å‡ºåˆ°å†…å­˜ä¸­ï¼Œä¾›è°ƒè¯•ä½¿ç”¨ã€‚


**append mode, complete mode å’Œ update mode**:

è¿™äº›æ˜¯æµæ•°æ®è¾“å‡ºåˆ°sinkä¸­çš„æ–¹å¼ï¼Œå«åš output modeã€‚

append mode æ˜¯é»˜è®¤æ–¹å¼ï¼Œå°†æ–°æµè¿‡æ¥çš„æ•°æ®çš„è®¡ç®—ç»“æœæ·»åŠ åˆ°sinkä¸­ã€‚

complete mode ä¸€èˆ¬é€‚ç”¨äºæœ‰aggregationæŸ¥è¯¢çš„æƒ…å†µã€‚æµè®¡ç®—å¯åŠ¨å¼€å§‹åˆ°ç›®å‰ä¸ºæ­¢æ¥æ”¶åˆ°çš„å…¨éƒ¨æ•°æ®çš„è®¡ç®—ç»“æœæ·»åŠ åˆ°sinkä¸­ã€‚

update mode åªæœ‰æœ¬æ¬¡ç»“æœä¸­å’Œä¹‹å‰ç»“æœä¸ä¸€æ ·çš„è®°å½•æ‰ä¼šæ·»åŠ åˆ°sinkä¸­ã€‚

<br/>



**operation å’Œ query**:

åœ¨SparkSQLæ‰¹å¤„ç†ä¸­ï¼Œç®—å­è¢«åˆ†ä¸ºTransformationç®—å­å’ŒActionç®—å­ã€‚Spark Structured Streaming æœ‰æ‰€ä¸åŒï¼Œæ‰€æœ‰é’ˆå¯¹æµæ•°æ®çš„ç®—å­éƒ½æ˜¯æ‡’æƒ°æ‰§è¡Œçš„ï¼Œå«åšoperationã€‚

DataFrameçš„Actionç®—å­(ä¾‹å¦‚show,count,reduce)éƒ½ä¸å¯ä»¥åœ¨Spark Structured Streamingä¸­ä½¿ç”¨ï¼Œè€Œå¤§éƒ¨åˆ†Transformationç®—å­éƒ½å¯ä»¥åœ¨Structured Streamingä¸­ä½¿ç”¨(ä¾‹å¦‚select,where,groupBy,agg)ã€‚

ä½†ä¹Ÿæœ‰äº›æ“ä½œä¸å¯ä»¥(ä¾‹å¦‚sort, distinct,æŸäº›ç±»å‹çš„joinæ“ä½œï¼Œä»¥åŠè¿ç»­çš„aggæ“ä½œç­‰)ã€‚

å¦‚æœè¦è§¦å‘æ‰§è¡Œï¼Œéœ€è¦é€šè¿‡writeStreamå¯åŠ¨ä¸€ä¸ªqueryï¼ŒæŒ‡å®šsinkï¼Œoutput modeï¼Œä»¥åŠè§¦å‘å™¨triggerç±»å‹ã€‚

ä»ä¸€å®šæ„ä¹‰ä¸Šï¼Œå¯ä»¥å°†writeStreamç†è§£æˆStructured Streaming å”¯ä¸€çš„ Action ç®—å­ã€‚

Spark Structured Streamingæ”¯æŒçš„è§¦å‘å™¨triggerç±»å‹ä¸»è¦æœ‰ä»¥ä¸‹ä¸€äº›ã€‚

1ï¼Œunspecifiedã€‚ä¸æŒ‡å®štriggerç±»å‹ï¼Œä»¥micro-batchæ–¹å¼è§¦å‘ï¼Œå½“ä¸Šä¸€ä¸ªmicro-batchæ‰§è¡Œå®Œæˆåï¼Œå°†ä¸­é—´æ”¶åˆ°çš„æ•°æ®ä½œä¸ºä¸‹ä¸€ä¸ªmicro-batchçš„æ•°æ®ã€‚

2ï¼Œfixed interval micro-batchesã€‚æŒ‡å®šæ—¶é—´é—´éš”çš„micro-batchã€‚å¦‚æœä¸Šä¸€ä¸ªmicro-batchåœ¨é—´éš”æ—¶é—´å†…å®Œæˆï¼Œéœ€è¦ç­‰å¾…æŒ‡å®šé—´éš”æ—¶é—´ã€‚å¦‚æœä¸Šä¸€ä¸ªmicro-batchåœ¨é—´éš”æ—¶é—´åæ‰å®Œæˆï¼Œé‚£ä¹ˆä¼šåœ¨ä¸Šä¸€ä¸ªmicro-batchæ‰§è¡Œå®Œæˆåç«‹å³æ‰§è¡Œã€‚

3ï¼Œone-time micro-batchã€‚åªè§¦å‘ä¸€æ¬¡,ä»¥micro-batchæ–¹å¼è§¦å‘ã€‚ä¸€ç§åœ¨æµè®¡ç®—æ¨¡å¼ä¸‹æ‰§è¡Œæ‰¹å¤„ç†çš„æ–¹æ³•ã€‚

4ï¼Œcontinuous with fixed checkpoint intervalã€‚æ¯ä¸ªäº‹ä»¶è§¦å‘ä¸€æ¬¡ï¼ŒçœŸæ­£çš„æµè®¡ç®—ï¼Œè¿™ç§æ¨¡å¼ç›®å‰è¿˜å¤„äºå®éªŒé˜¶æ®µã€‚

<br/>


**event timeï¼Œ processing time å’Œ watermarking**:

event time æ˜¯æµæ•°æ®çš„å‘ç”Ÿæ—¶é—´ï¼Œä¸€èˆ¬åµŒå…¥åˆ°æµæ•°æ®ä¸­ä½œä¸ºä¸€ä¸ªå­—æ®µã€‚ 

processing time æ˜¯æŒ‡æ•°æ®è¢«å¤„ç†çš„æ—¶é—´ã€‚

Spark Structured Streaming ä¸€èˆ¬ ä½¿ç”¨ event timeä½œä¸º Windowsåˆ‡åˆ†çš„ä¾æ®ï¼Œä¾‹å¦‚æ¯ç§’é’Ÿçš„æˆäº¤å‡ä»·ï¼Œæ˜¯å–event timeä¸­æ¯ç§’é’Ÿçš„æ•°æ®è¿›è¡Œå¤„ç†ã€‚

è€ƒè™‘åˆ°æ•°æ®å­˜åœ¨å»¶è¿Ÿï¼Œå¦‚æœä¸€ä¸ªæ•°æ®åˆ°è¾¾æ—¶ï¼Œå…¶å¯¹åº”çš„æ—¶é—´æ‰¹æ¬¡å·²ç»è¢«è®¡ç®—è¿‡äº†ï¼Œé‚£ä¹ˆä¼šé‡æ–°è®¡ç®—è¿™ä¸ªæ—¶é—´æ‰¹æ¬¡çš„æ•°æ®å¹¶æ›´æ–°ä¹‹å‰çš„è®¡ç®—ç»“æœã€‚ä½†æ˜¯å¦‚æœè¿™ä¸ªæ•°æ®å»¶è¿Ÿå¤ªä¹…ï¼Œé‚£ä¹ˆå¯ä»¥è®¾ç½®watermarking(æ°´ä½çº¿)æ¥å…è®¸ä¸¢å¼ƒ processing timeå’Œevent timeç›¸å·®å¤ªä¹…çš„æ•°æ®ï¼Œå³å»¶è¿Ÿè¿‡ä¹…çš„æ•°æ®ã€‚**æ³¨æ„è¿™ç§ä¸¢å¼ƒæ˜¯æˆ–è®¸ä¼šå‘ç”Ÿçš„ï¼Œä¸æ˜¯ä¸€å®šä¼šä¸¢å¼ƒ**ã€‚

<br/>


**at-most onceï¼Œat-least once å’Œ exactly once**:

è¿™æ˜¯åˆ†å¸ƒå¼æµè®¡ç®—ç³»ç»Ÿåœ¨æŸäº›æœºå™¨å‘ç”Ÿå‘ç”Ÿæ•…éšœæ—¶ï¼Œå¯¹ç»“æœä¸€è‡´æ€§(æ— è®ºæœºå™¨æ˜¯å¦å‘ç”Ÿæ•…éšœï¼Œç»“æœéƒ½ä¸€æ ·)çš„ä¿è¯æ°´å¹³ã€‚ååº”äº†åˆ†å¸ƒå¼æµè®¡ç®—ç³»ç»Ÿçš„å®¹é”™èƒ½åŠ›ã€‚

at-most onceï¼Œæœ€å¤šä¸€æ¬¡ã€‚æ¯ä¸ªæ•°æ®æˆ–äº‹ä»¶æœ€å¤šè¢«ç¨‹åºä¸­çš„æ‰€æœ‰ç®—å­å¤„ç†ä¸€æ¬¡ã€‚è¿™æœ¬è´¨ä¸Šæ˜¯ä¸€ç§å°½åŠ›è€Œä¸ºçš„æ–¹æ³•ï¼Œåªè¦æœºå™¨å‘ç”Ÿæ•…éšœï¼Œå°±ä¼šä¸¢å¼ƒä¸€äº›æ•°æ®ã€‚è¿™æ˜¯æ¯”è¾ƒä½æ°´å¹³çš„ä¸€è‡´æ€§ä¿è¯ã€‚

at-least onceï¼Œè‡³å°‘ä¸€æ¬¡ã€‚æ¯ä¸ªæ•°æ®æˆ–äº‹ä»¶è‡³å°‘è¢«ç¨‹åºä¸­çš„æ‰€æœ‰ç®—å­å¤„ç†ä¸€æ¬¡ã€‚è¿™æ„å‘³ç€å½“æœºå™¨å‘ç”Ÿæ•…éšœæ—¶ï¼Œæ•°æ®ä¼šä»æŸä¸ªä½ç½®å¼€å§‹é‡ä¼ ã€‚ä½†æœ‰äº›æ•°æ®å¯èƒ½åœ¨å‘ç”Ÿæ•…éšœå‰è¢«æ‰€æœ‰ç®—å­å¤„ç†äº†ä¸€æ¬¡ï¼Œåœ¨å‘ç”Ÿæ•…éšœåé‡ä¼ æ—¶åˆè¢«æ‰€æœ‰ç®—å­å¤„ç†äº†ä¸€æ¬¡ï¼Œç”šè‡³é‡ä¼ æ—¶åˆæœ‰æœºå™¨å‘ç”Ÿäº†æ•…éšœï¼Œç„¶åå†æ¬¡é‡ä¼ ï¼Œç„¶ååˆè¢«æ‰€æœ‰ç®—å­å¤„ç†äº†ä¸€æ¬¡ã€‚å› æ­¤æ˜¯è‡³å°‘è¢«å¤„ç†ä¸€æ¬¡ã€‚è¿™æ˜¯ä¸€ç§ä¸­é—´æ°´å¹³çš„ä¸€è‡´æ€§ä¿è¯ã€‚

exactly onceï¼Œæ°å¥½ä¸€æ¬¡ã€‚ä»è®¡ç®—ç»“æœçœ‹ï¼Œæ¯ä¸ªæ•°æ®æˆ–äº‹ä»¶éƒ½æ°å¥½è¢«ç¨‹åºä¸­çš„æ‰€æœ‰ç®—å­å¤„ç†ä¸€æ¬¡ã€‚è¿™æ˜¯ä¸€ç§æœ€é«˜æ°´å¹³çš„ä¸€è‡´æ€§ä¿è¯ã€‚

spark structured streaming åœ¨micro-batchè§¦å‘å™¨ç±»å‹ä¸‹ï¼Œsinkæ˜¯Fileæƒ…å†µä¸‹ï¼Œå¯ä»¥ä¿è¯ä¸ºexactly onceçš„ä¸€è‡´æ€§æ°´å¹³ã€‚

ä½†æ˜¯åœ¨continuouè§¦å‘å™¨ç±»å‹ä¸‹ï¼Œåªèƒ½ä¿è¯æ˜¯at-least onceçš„ä¸€è‡´æ€§æ°´å¹³ã€‚

è¯¦æƒ…å‚è€ƒå¦‚ä¸‹æ–‡ç« ï¼šã€Šè°ˆè°ˆæµè®¡ç®—ä¸­çš„ã€Exactly Onceã€ç‰¹æ€§ã€‹

https://segmentfault.com/a/1190000019353382


```python

```

### äºŒï¼Œword count åŸºæœ¬èŒƒä¾‹


ä¸‹é¢èŒƒä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†ç”¨Pythonä»£ç åœ¨ä¸€ä¸ªç›®å½•ä¸‹ä¸æ–­ç”Ÿæˆä¸€äº›ç®€å•å¥å­ç»„æˆçš„æ–‡ä»¶ã€‚

ç„¶åç”¨pysparkè¯»å–æ–‡ä»¶æµï¼Œå¹¶è¿›è¡Œè¯é¢‘ç»Ÿè®¡ï¼Œå¹¶å°†ç»“æœæ‰“å°ã€‚



ä¸‹é¢æ˜¯ç”Ÿæˆæ–‡ä»¶æµçš„ä»£ç ã€‚å¹¶é€šè¿‡subprocess.Popenè°ƒç”¨å®ƒå¼‚æ­¥æ‰§è¡Œã€‚

```python
%%writefile make_streamming_data.py
import random 
import os 
import time 
import shutil
sentences = ["eat tensorflow2 in 30 days","eat pytorch in 20 days","eat pyspark in 10 days"]
data_path = "./data/streamming_data"

if os.path.exists(data_path):
    shutil.rmtree(data_path)
    
os.makedirs(data_path)

for i in range(20):
    line = random.choice(sentences)
    tmp_file = str(i)+".txt"
    with open(tmp_file,"w") as f:
        f.write(line)
        f.flush()
    shutil.move(tmp_file,os.path.join(data_path,tmp_file))
    time.sleep(1)
    
```

```python
# åœ¨åå°å¼‚æ­¥ç”Ÿæˆæ–‡ä»¶æµ
import subprocess
cmd = ["python", "make_streamming_data.py"]
process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

#process.wait() #ç­‰å¾…ç»“æŸ

```

```python
#é€šè¿‡ readStream åˆ›å»ºstreaming dataframe
schema = T.StructType().add("value", "string")
data_path = "./data/streamming_data"

dflines = spark \
    .readStream \
    .option("sep", ".") \
    .schema(schema) \
    .csv(data_path)

dflines.printSchema() 
print(dflines.isStreaming) 
```

```
root
 |-- value: string (nullable = true)

True
```

```python
#å®æ–½operatorè½¬æ¢
dfwords = dflines.select(F.explode(F.split(dflines.value, " ")).alias("word"))
dfwordCounts = dfwords.groupBy("word").count()

```

```python
#æ‰§è¡Œquery, æ³¨æ„æ˜¯å¼‚æ­¥æ–¹å¼æ‰§è¡Œ, ç›¸å½“äºæ˜¯å¼€å¯äº†åå°è¿›ç¨‹

def foreach_batch_function(df, epoch_id):
    print("Batch: ",epoch_id)
    df.show()

query = dfwordCounts \
    .writeStream \
    .outputMode("complete")\
    .foreachBatch(foreach_batch_function) \
    .start()

#query.awaitTermination() #é˜»å¡å½“å‰è¿›ç¨‹ç›´åˆ°queryå‘ç”Ÿå¼‚å¸¸æˆ–è€…è¢«stop

print(query.isActive)

#60såä¸»åŠ¨åœæ­¢query
time.sleep(30)
query.stop()

print(query.isActive)


```

```
True
Batch:  0
+-----------+-----+
|       word|count|
+-----------+-----+
|        eat|   10|
|       days|   10|
|         20|    4|
|tensorflow2|    3|
|         30|    3|
|         10|    3|
|    pyspark|    3|
|         in|   10|
|    pytorch|    4|
+-----------+-----+

Batch:  1
+-----------+-----+
|       word|count|
+-----------+-----+
|        eat|   13|
|       days|   13|
|         20|    4|
|tensorflow2|    5|
|         30|    5|
|         10|    4|
|    pyspark|    4|
|         in|   13|
|    pytorch|    4|
+-----------+-----+

Batch:  2
+-----------+-----+
|       word|count|
+-----------+-----+
|        eat|   15|
|       days|   15|
|         20|    5|
|tensorflow2|    5|
|         30|    5|
|         10|    5|
|    pyspark|    5|
|         in|   15|
|    pytorch|    5|
+-----------+-----+

Batch:  3
+-----------+-----+
|       word|count|
+-----------+-----+
|        eat|   18|
|       days|   18|
|         20|    6|
|tensorflow2|    5|
|         30|    5|
|         10|    7|
|    pyspark|    7|
|         in|   18|
|    pytorch|    6|
+-----------+-----+

Batch:  4
+-----------+-----+
|       word|count|
+-----------+-----+
|        eat|   20|
|       days|   20|
|         20|    7|
|tensorflow2|    5|
|         30|    5|
|         10|    8|
|    pyspark|    8|
|         in|   20|
|    pytorch|    7|
+-----------+-----+

False
```

```python

```

```python

```

### ä¸‰ï¼Œåˆ›å»ºStreaming DataFrame



å¯ä»¥ä»Kafka Sourceï¼ŒFile Source ä»¥åŠ Socket Source ä¸­åˆ›å»º Streaming DataFrameã€‚



**1ï¼Œä»Kafka Source åˆ›å»º**


éœ€è¦å®‰è£…kafkaï¼Œå¹¶åŠ è½½å…¶jaråŒ…åˆ°ä¾èµ–ä¸­ã€‚

è¯¦ç»†å‚è€ƒï¼šhttp://spark.apache.org/docs/2.2.0/structured-streaming-kafka-integration.html

ä»¥ä¸‹ä»£ç ä»…ä¾›ç¤ºèŒƒï¼Œè¿è¡Œéœ€è¦é…ç½®ç›¸å…³kafkaç¯å¢ƒã€‚

```python
df = spark \
  .read \
  .format("kafka") \
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2") \
  .option("subscribe", "topic1") \
  .load()

```

```python

```

**2ï¼Œä»File Source åˆ›å»º**


æ”¯æŒè¯»å–parquetæ–‡ä»¶ï¼Œcsvæ–‡ä»¶ï¼Œjsonæ–‡ä»¶ï¼Œtxtæ–‡ä»¶ç›®å½•ã€‚éœ€è¦æŒ‡å®šschemaã€‚



```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")
dfstudents.printSchema() 
```

```python
query = dfstudents.writeStream \
    .outputMode("append")\
    .format("parquet") \
    .option("checkpointLocation", "./data/checkpoint/") \
    .option("path", "./data/students_parquet/") \
    .start()

#query.awaitTermination()
```

```python

```

```python

```

**3,ä»Socket Sourceåˆ›å»º**

åœ¨bashä¸­è¾“å…¥nc -lk 9999 å¼€å¯socketç½‘ç»œé€šä¿¡ç«¯å£ï¼Œç„¶ååœ¨å…¶ä¸­è¾“å…¥ä¸€äº›å¥å­ï¼Œå¦‚ï¼š

```
hello world
hello China
hello Beijing
```

```python
dflines = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()
```

```python

```

```python

```

### ä¸‰ï¼Œä½¿ç”¨operatorè½¬æ¢


å¯ä»¥åœ¨Streaming DataFrameä¸Šä½¿ç”¨Static DataFrameå¤§éƒ¨åˆ†å¸¸è§„Transformationç®—å­ã€‚

è¿˜å¯ä»¥é’ˆå¯¹event timeè¿›è¡Œæ»‘åŠ¨çª—å£(window)æ“ä½œï¼Œå¯ä»¥é€šè¿‡è®¾ç½®æ°´ä½çº¿(watermarking)æ¥ä¸¢å¼ƒå»¶è¿Ÿè¿‡ä¹…çš„æ•°æ®ã€‚

ä¸ä»…å¦‚æ­¤ï¼Œå¯ä»¥å¯¹Streaming DataFrameå’Œ Static DataFrame è¿›è¡Œè¡¨è¿æ¥ joinæ“ä½œã€‚

ç”šè‡³ä¸¤ä¸ªStreaming DataFrameä¹‹å‰ä¹Ÿæ˜¯å¯ä»¥joinçš„ã€‚ 



**1ï¼ŒBasic Operators** 


ä¸€äº›å¸¸ç”¨çš„Transformationç®—å­éƒ½å¯ä»¥åœ¨Unbounded DataFrameä¸Šä½¿ç”¨ï¼Œä¾‹å¦‚select,selectExpr, where, groupBy, aggç­‰ç­‰ã€‚

ä¹Ÿå¯ä»¥åƒæ‰¹å¤„ç†ä¸­çš„é™æ€çš„DataFrameé‚£æ ·ï¼Œæ³¨å†Œä¸´æ—¶è§†å›¾ï¼Œç„¶ååœ¨è§†å›¾ä¸Šä½¿ç”¨SQLè¯­æ³•ã€‚


```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")
dfstudents.printSchema() 

dfstudents.createOrReplaceTempView("students")

dfstudents_old = spark.sql("select * from students where age >25")
print(dfstudents_old.isStreaming)
```

```python

```

```python

```

**2, Window Operations on Event Time**


åŸºäºäº‹ä»¶æ—¶é—´æ»‘åŠ¨çª—ä¸Šçš„èšåˆæ“ä½œå’Œå…¶å®ƒåˆ—çš„goupByæ“ä½œéå¸¸ç›¸ä¼¼ï¼Œè½åœ¨åŒä¸€ä¸ªæ—¶é—´çª—çš„è®°å½•å°±å¥½åƒå…·æœ‰ç›¸åŒçš„keyï¼Œå®ƒä»¬å°†è¿›è¡Œèšåˆã€‚

ä¸‹é¢æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªè™šæ‹Ÿçš„æ¯”ç‰¹å¸äº¤æ˜“ä»·æ ¼çš„ä¾‹å­æ¥å±•ç¤ºåŸºäºäº‹ä»¶æ—¶é—´æ»‘åŠ¨çª—ä¸Šçš„èšåˆæ“ä½œã€‚



```python
%%writefile make_trading_data.py

import random 
import os 
import time 
import datetime 
import json 
import shutil

data_path = "./data/trading_data"

if os.path.exists(data_path):
    shutil.rmtree(data_path)
    
os.makedirs(data_path)

for i in range(20):
    now =  datetime.datetime.now()
    now_str =  now.strftime("%Y-%m-%d %H:%M:%S.%f")
    
    #æ„é€ å»¶è¿Ÿæ•°æ®, å»¶è¿Ÿ20minå·¦å³
    right_now = now - datetime.timedelta(minutes = 20)
    right_now_str = right_now.strftime("%Y-%m-%d %H:%M:%S.%f")
    
    if i%2==0:
        dic = {"dt": now_str, "amount": 100, "price": 10000.0+random.choice(range(5))}
    else:
        dic = {"dt": right_now_str, "amount": 100 ,"price": 100.0-random.choice(range(5))}
        
    tmp_file = str(i)+".json"
    with open(tmp_file,"w") as f:
        json.dump(dic,f)
    shutil.move(tmp_file,os.path.join(data_path,tmp_file))
    time.sleep(10)
```

```python
# åœ¨åå°å¼‚æ­¥ç”Ÿæˆæ–‡ä»¶æµ
import subprocess
cmd = ["python", "make_trading_data.py"]
process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)

#process.wait() #ç­‰å¾…ç»“æŸ
```

```python
data_path = "./data/trading_data"
schema = T.StructType().add("dt","string").add("amount","integer").add("price","double")
dfprice_raw = spark.readStream.schema(schema).json("./data/trading_data")
dfprice_raw.printSchema() 
```

```python
dfprice = dfprice_raw.selectExpr("cast(dt as timestamp) as dt","amount","price", "amount*price as volume")
dfprice.printSchema() 
```

```python
# æ§åˆ¶å°æ–¹å¼è¾“å‡ºï¼Œå¯èƒ½éœ€è¦åœ¨jupyter çš„logç•Œé¢æŸ¥çœ‹è¾“å‡ºæ—¥å¿—

query = dfprice.writeStream \
    .outputMode("append")\
    .format("console") \
    .start()

time.sleep(20)
query.stop()

#query.awaitTermination()

```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------+------+-------+---------+
|                  dt|amount|  price|   volume|
+--------------------+------+-------+---------+
|2020-12-21 08:11:...|   100|10004.0|1000400.0|
+--------------------+------+-------+---------+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------+------+-----+------+
|                  dt|amount|price|volume|
+--------------------+------+-----+------+
|2020-12-21 07:51:...|   100| 99.0|9900.0|
+--------------------+------+-----+------+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------------+------+-------+---------+
|                  dt|amount|  price|   volume|
+--------------------+------+-------+---------+
|2020-12-21 08:12:...|   100|10003.0|1000300.0|
+--------------------+------+-------+---------+

```

```python
#ä¸‹é¢æˆ‘ä»¬å°†dfpriceæŒ‰ç…§æ—¶é—´åˆ†çª—ï¼Œçª—å£èŒƒå›´ä¸º10minï¼Œæ»‘åŠ¨å‘¨æœŸä¸º5minï¼Œå¹¶ç»Ÿè®¡æ»‘åŠ¨çª—å£å†…çš„å¹³å‡äº¤æ˜“ä»·æ ¼

dfprice_avg = dfprice.groupBy(F.window(dfprice.dt, "10 minutes", "5 minutes")) \
   .agg(F.sum("amount").alias("amount"), F.sum("volume").alias("volume")) \
   .selectExpr("window","window.start","window.end","volume/amount as avg_price")

dfprice_avg.printSchema() 
```

```python
query = dfprice_avg.writeStream \
    .outputMode("complete")\
    .format("console") \
    .start()

time.sleep(60)
query.stop()

#query.awaitTermination()

```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:25...|2020-12-21 08:25:00|2020-12-21 08:35:00|  10004.0|
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     97.0|
|[2020-12-21 08:05...|2020-12-21 08:05:00|2020-12-21 08:15:00|     97.0|
|[2020-12-21 08:20...|2020-12-21 08:20:00|2020-12-21 08:30:00|  10004.0|
+--------------------+-------------------+-------------------+---------+

-------------------------------------------
Batch: 1
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:25...|2020-12-21 08:25:00|2020-12-21 08:35:00|  10002.0|
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     97.0|
|[2020-12-21 08:05...|2020-12-21 08:05:00|2020-12-21 08:15:00|     97.0|
|[2020-12-21 08:20...|2020-12-21 08:20:00|2020-12-21 08:30:00|  10002.0|
+--------------------+-------------------+-------------------+---------+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:25...|2020-12-21 08:25:00|2020-12-21 08:35:00|  10002.0|
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     98.5|
|[2020-12-21 08:05...|2020-12-21 08:05:00|2020-12-21 08:15:00|     98.5|
|[2020-12-21 08:20...|2020-12-21 08:20:00|2020-12-21 08:30:00|  10002.0|
+--------------------+-------------------+-------------------+---------+
```

```python
#è¿›ä¸€æ­¥åœ°ï¼Œæˆ‘ä»¬è®¾ç½®watermarking(æ°´ä½çº¿)ä¸º20åˆ†é’Ÿ, åˆ™è¶…å‡ºæ°´ä½çº¿çš„æ•°æ®å°†å…è®¸è¢«ä¸¢å¼ƒ(ä½†ä¸ä¸€å®šè¢«ä¸¢å¼ƒ)

dfprice_avg = dfprice.withWatermark("dt", "20 minutes") \
   .groupBy(F.window(dfprice.dt, "10 minutes", "5 minutes")) \
   .agg(F.sum("amount").alias("amount"), F.sum("volume").alias("volume")) \
   .selectExpr("window","window.start","window.end","volume/amount as avg_price")

dfprice_avg.printSchema() 


```

```python
#è®¾ç½®æ°´ä½çº¿åï¼Œ outputModeå¿…é¡»æ˜¯appendæˆ–è€…update
query = dfprice_avg.writeStream \
    .outputMode("update")\
    .format("console") \
    .start()

time.sleep(60)
query.stop()

#query.awaitTermination()
```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     96.0|
|[2020-12-21 08:15...|2020-12-21 08:15:00|2020-12-21 08:25:00|  10001.0|
|[2020-12-21 08:20...|2020-12-21 08:20:00|2020-12-21 08:30:00|  10001.0|
|[2020-12-21 07:55...|2020-12-21 07:55:00|2020-12-21 08:05:00|     96.0|
+--------------------+-------------------+-------------------+---------+

-------------------------------------------
Batch: 1
-------------------------------------------
+------+-----+---+---------+
|window|start|end|avg_price|
+------+-----+---+---------+
+------+-----+---+---------+

-------------------------------------------
Batch: 2
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     97.5|
|[2020-12-21 07:55...|2020-12-21 07:55:00|2020-12-21 08:05:00|     97.5|
+--------------------+-------------------+-------------------+---------+

-------------------------------------------
Batch: 3
-------------------------------------------
+--------------------+-------------------+-------------------+------------------+
|              window|              start|                end|         avg_price|
+--------------------+-------------------+-------------------+------------------+
|[2020-12-21 08:15...|2020-12-21 08:15:00|2020-12-21 08:25:00|10000.666666666666|
|[2020-12-21 08:20...|2020-12-21 08:20:00|2020-12-21 08:30:00|10000.666666666666|
+--------------------+-------------------+-------------------+------------------+

-------------------------------------------
Batch: 4
-------------------------------------------
+------+-----+---+---------+
|window|start|end|avg_price|
+------+-----+---+---------+
+------+-----+---+---------+

-------------------------------------------
Batch: 5
-------------------------------------------
+--------------------+-------------------+-------------------+---------+
|              window|              start|                end|avg_price|
+--------------------+-------------------+-------------------+---------+
|[2020-12-21 08:00...|2020-12-21 08:00:00|2020-12-21 08:10:00|     98.0|
|[2020-12-21 07:55...|2020-12-21 07:55:00|2020-12-21 08:05:00|     98.0|
+--------------------+-------------------+-------------------+---------+
```

```python

```

**3, Join Operations**


Streaming DataFrame å¯ä»¥å’Œ Static DataFrame è¿›è¡Œ Inner æˆ–è€… Left Outer è¿æ¥æ“ä½œã€‚joinåçš„ç»“æœä¾ç„¶æ˜¯ä¸€ä¸ª Streaming DataFrameã€‚

æ­¤å¤– Streaming  DataFrame ä¹Ÿå¯ä»¥å’Œ  Streaming  DataFrame è¿›è¡Œ Inner join. 

è¿™ç§joinæœºåˆ¶æ˜¯é€šè¿‡è¿½æº¯è¢«joinçš„ Streaming DataFrame å·²ç»æ¥æ”¶åˆ°çš„æµæ•°æ®å’Œä¸»åŠ¨ joinçš„ Streaming DataFrameçš„å½“å‰æ‰¹æ¬¡è¿›è¡Œkeyçš„é…å¯¹ï¼Œä¸ºäº†é¿å…è¿½æº¯è¿‡å»å¤ªä¹…çš„æ•°æ®é€ æˆæ€§èƒ½ç“¶é¢ˆï¼Œå¯ä»¥é€šè¿‡è®¾ç½® watermark æ¥æ¸…ç©ºè¿‡å»å¤ªä¹…çš„å†å²æ•°æ®çš„Stateï¼Œæ•°æ®è¢«æ¸…ç©ºStateåå°†å…è®¸ä¸è¢«é…å¯¹æŸ¥è¯¢ã€‚



```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")
dfstudents.printSchema() 
```

ä¸‹é¢æ˜¯Streaming DataFrame å’Œ Static DataFrame è¿›è¡Œ joinçš„ç¤ºèŒƒã€‚

```python
dfclasses = spark.createDataFrame([("LiLei","class1"),("Hanmeimei","class2"),("Lily","class3")]).toDF("name","class")
dfclasses.printSchema() 
```

```python
# ç¤ºèŒƒ Streaming DataFrame  inner join Static DataFrame
dfjoin_inner = dfstudents.join(dfclasses, "name", "inner")
dfjoin_inner.printSchema()
print(dfjoin_inner.isStreaming)
```

```
root
 |-- name: string (nullable = true)
 |-- age: integer (nullable = true)
 |-- score: double (nullable = true)
 |-- class: string (nullable = true)

True
```

```python
query = dfjoin_inner.writeStream \
    .outputMode("append")\
    .format("console") \
    .start()

time.sleep(10)
query.stop()

#query.awaitTermination()
```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------+---+-----+------+
|     name|age|score| class|
+---------+---+-----+------+
|    LiLei| 12| 75.5|class1|
|Hanmeimei| 16| 90.0|class2|
|     Lily| 15| 68.0|class3|
+---------+---+-----+------+
```

```python
# ç¤ºèŒƒ Streaming DataFrame  left join Static DataFrame
dfjoin_left = dfstudents.join(dfclasses, "name", "left")
dfjoin_left.printSchema()
print(dfjoin_left.isStreaming)

```

```python
query = dfjoin_left.writeStream \
    .outputMode("append")\
    .format("console") \
    .start()

time.sleep(10)
query.stop()

#query.awaitTermination()
```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------+---+-----+------+
|     name|age|score| class|
+---------+---+-----+------+
|    LiLei| 12| 75.5|class1|
|   Justin| 19| 87.0|  null|
|     Lily| 15| 68.0|class3|
|     Andy| 17| 80.0|  null|
|Hanmeimei| 16| 90.0|class2|
|  Michael| 20| 70.5|  null|
+---------+---+-----+------+
```


ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„Streaming DataFrame inner join Streaming DataFrameç¤ºèŒƒã€‚

```python

dfhometown = dfstudents.selectExpr("name","if(rand()>0.5,'China','USA') as hometown")
dfhometown.printSchema()
print(dfhometown.isStreaming)

```

```
root
 |-- name: string (nullable = true)
 |-- hometown: string (nullable = false)

True
```

```python
dfjoin_streaming = dfstudents.join(dfhometown,"name","inner")
dfjoin_streaming.printSchema()
print(dfjoin_streaming.isStreaming)
```

```python
query = dfjoin_streaming.writeStream \
    .outputMode("append")\
    .format("console") \
    .start()

time.sleep(10)
query.stop()

#query.awaitTermination()
```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------+---+-----+--------+
|     name|age|score|hometown|
+---------+---+-----+--------+
|    LiLei| 12| 75.5|     USA|
|   Justin| 19| 87.0|   China|
|     Lily| 15| 68.0|   China|
|Hanmeimei| 16| 90.0|   China|
|     Andy| 17| 80.0|     USA|
|  Michael| 20| 70.5|   China|
+---------+---+-----+--------+
```

```python

```

```python

```

### å››ï¼Œè¾“å‡º Structured Streaming çš„ç»“æœ


Streaming DataFrame æ”¯æŒä»¥ä¸‹ç±»å‹çš„ç»“æœè¾“å‡ºï¼š

* Kafka Sinkã€‚å°†å¤„ç†åçš„æµæ•°æ®è¾“å‡ºåˆ°kafkaæŸä¸ªæˆ–æŸäº›topicä¸­ã€‚

* File Sinkã€‚ å°†å¤„ç†åçš„æµæ•°æ®å†™å…¥åˆ°æ–‡ä»¶ç³»ç»Ÿä¸­ã€‚

* ForeachBatch Sinkã€‚ å¯¹äºæ¯ä¸€ä¸ªmicro-batchçš„æµæ•°æ®å¤„ç†åçš„ç»“æœï¼Œç”¨æˆ·å¯ä»¥ç¼–å†™å‡½æ•°å®ç°è‡ªå®šä¹‰å¤„ç†é€»è¾‘ã€‚ä¾‹å¦‚å†™å…¥åˆ°å¤šä¸ªæ–‡ä»¶ä¸­ï¼Œæˆ–è€…å†™å…¥åˆ°æ–‡ä»¶å¹¶æ‰“å°ã€‚

* Foreach Sinkã€‚ä¸€èˆ¬åœ¨Continuousè§¦å‘æ¨¡å¼ä¸‹ä½¿ç”¨ï¼Œç”¨æˆ·ç¼–å†™å‡½æ•°å®ç°æ¯ä¸€è¡Œçš„å¤„ç†ã€‚

* Console Sinkã€‚æ‰“å°åˆ°Driverç«¯æ§åˆ¶å°ï¼Œå¦‚æœæ—¥å¿—é‡å¤§ï¼Œè°¨æ…ä½¿ç”¨ã€‚ä¸€èˆ¬ä¾›è°ƒè¯•ä½¿ç”¨ã€‚

* Memory Sinkã€‚è¾“å‡ºåˆ°å†…å­˜ä¸­ï¼Œä¾›è°ƒè¯•ä½¿ç”¨ã€‚


```python

```

**1ï¼Œè¾“å‡ºåˆ°Kafka Sink**

<!-- #region -->

ç¤ºèŒƒä»£ç å¦‚ä¸‹ï¼Œæ³¨æ„ï¼Œdf åº”å½“å…·å¤‡ä»¥ä¸‹åˆ—ï¼štopic, key å’Œ value.

```python
query = df.selectExpr("topic", "CAST(key AS STRING)", "CAST(value AS STRING)") \
  .writeStream()\
  .format("kafka")\
  .option("kafka.bootstrap.servers", "host1:port1,host2:port2")\
  .start()
```
<!-- #endregion -->

```python

```

**2ï¼Œè¾“å‡ºåˆ°File Sink**


```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")

query = dfstudents \
    .writeStream\
    .format("csv") \
    .option("checkpointLocation", "./data/checkpoint") \
    .option("path", "./data/students_csv") \
    .start()

time.sleep(5)
query.stop()

```

```python

```

**3, è¾“å‡ºåˆ°ForeachBatch Sink**


å¯¹äºæ¯ä¸€ä¸ªBatch,å¯ä»¥å½“åšä¸€ä¸ªStatic DataFrame è¿›è¡Œå¤„ç†ã€‚

```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")

def foreach_batch_function(df, epoch_id):
    print("epoch_id = ",epoch_id)
    df.show()
    print("rows = ",df.count())
    
query = dfstudents.writeStream.foreachBatch(foreach_batch_function).start()  

time.sleep(3)
query.stop()
```

```
epoch_id =  0
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 12| 75.5|
|Hanmeimei| 16| 90.0|
|     Lily| 15| 68.0|
|  Michael| 20| 70.5|
|     Andy| 17| 80.0|
|   Justin| 19| 87.0|
+---------+---+-----+

rows =  6
```

```python

```

**4, è¾“å‡ºåˆ°Console Sink**


å°†ç»“æœè¾“å‡ºåˆ°ç»ˆç«¯ï¼Œå¯¹äºjupyter ç¯å¢ƒè°ƒè¯•ï¼Œå¯èƒ½éœ€è¦åœ¨jupyter çš„ log æ—¥å¿—ä¸­å»æŸ¥çœ‹ã€‚

```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")

dfstudents.writeStream \
  .format("console") \
  .trigger(processingTime='2 seconds') \
  .start()
```

```
-------------------------------------------
Batch: 0
-------------------------------------------
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 12| 75.5|
|Hanmeimei| 16| 90.0|
|     Lily| 15| 68.0|
|  Michael| 20| 70.5|
|     Andy| 17| 80.0|
|   Justin| 19| 87.0|
+---------+---+-----+
```

```python

```

```python

```

```python

```

**5, è¾“å‡ºåˆ°Memory Sink**

```python
schema = T.StructType().add("name","string").add("age","integer").add("score","double")
dfstudents = spark.readStream.schema(schema).json("./data/students_json")


#è®¾ç½®çš„queryName å°†æˆä¸ºéœ€è¦æŸ¥è¯¢çš„è¡¨çš„åç§°
query = dfstudents \
    .writeStream \
    .queryName("dfstudents") \
    .outputMode("append") \
    .format("memory") \
    .start()

time.sleep(3)
query.stop()

dfstudents_static = spark.sql("select * from dfstudents")
dfstudents_static.show() 


```

```
+---------+---+-----+
|     name|age|score|
+---------+---+-----+
|    LiLei| 12| 75.5|
|Hanmeimei| 16| 90.0|
|     Lily| 15| 68.0|
|  Michael| 20| 70.5|
|     Andy| 17| 80.0|
|   Justin| 19| 87.0|
+---------+---+-----+
```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
