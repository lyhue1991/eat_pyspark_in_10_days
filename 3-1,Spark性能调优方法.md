# 3-1,Sparkæ€§èƒ½è°ƒä¼˜æ–¹æ³•



Sparkç¨‹åºå¯ä»¥å¿«å¦‚é—ªç”µâš¡ï¸ï¼Œä¹Ÿå¯ä»¥æ…¢å¦‚èœ—ç‰›ğŸŒã€‚

å®ƒçš„æ€§èƒ½å–å†³äºç”¨æˆ·ä½¿ç”¨å®ƒçš„æ–¹å¼ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œå¦‚æœæœ‰å¯èƒ½ï¼Œç”¨æˆ·åº”å½“å°½å¯èƒ½å¤šåœ°ä½¿ç”¨SparkSQLä»¥å–å¾—æ›´å¥½çš„æ€§èƒ½ã€‚

ä¸»è¦åŸå› æ˜¯SparkSQLæ˜¯ä¸€ç§å£°æ˜å¼ç¼–ç¨‹é£æ ¼ï¼ŒèƒŒåçš„è®¡ç®—å¼•æ“ä¼šè‡ªåŠ¨åšå¤§é‡çš„æ€§èƒ½ä¼˜åŒ–å·¥ä½œã€‚

åŸºäºRDDçš„Sparkçš„æ€§èƒ½è°ƒä¼˜å±äºå‘éå¸¸æ·±çš„é¢†åŸŸï¼Œå¹¶ä¸”å¾ˆå®¹æ˜“è¸©åˆ°ã€‚

æˆ‘ä»¬å°†ä»‹ç»Sparkè°ƒä¼˜åŸç†ï¼ŒSparkä»»åŠ¡ç›‘æ§ï¼Œä»¥åŠSparkè°ƒä¼˜æ¡ˆä¾‹ã€‚

æœ¬æ–‡å‚è€ƒäº†ä»¥ä¸‹æ–‡ç« ï¼š

ã€ŠSparkæ€§èƒ½ä¼˜åŒ–æŒ‡å—â€”â€”åŸºç¡€ç¯‡ã€‹ï¼šhttps://tech.meituan.com/2016/04/29/spark-tuning-basic.html

ã€ŠSparkæ€§èƒ½ä¼˜åŒ–æŒ‡å—â€”â€”é«˜çº§ç¯‡ã€‹ï¼š https://tech.meituan.com/2016/05/12/spark-tuning-pro.html

ã€Šspark-è°ƒèŠ‚executorå †å¤–å†…å­˜ã€‹ï¼šhttps://www.cnblogs.com/colorchild/p/12175328.html


```python
import findspark

#æŒ‡å®šspark_homeä¸ºåˆšæ‰çš„è§£å‹è·¯å¾„,æŒ‡å®špythonè·¯å¾„
spark_home = "/Users/liangyun/ProgramFiles/spark-3.0.1-bin-hadoop3.2"
python_path = "/Users/liangyun/anaconda3/bin/python"
findspark.init(spark_home,python_path)

import pyspark 
from pyspark.sql import SparkSession

#SparkSQLçš„è®¸å¤šåŠŸèƒ½å°è£…åœ¨SparkSessionçš„æ–¹æ³•æ¥å£ä¸­

spark = SparkSession.builder \
        .appName("test") \
        .config("master","local[4]") \
        .enableHiveSupport() \
        .getOrCreate()

sc = spark.sparkContext
```

```python

```

### ä¸€ï¼ŒSparkè°ƒä¼˜åŸç†

<!-- #region -->
å¯ä»¥ç”¨ä¸‹é¢ä¸‰ä¸ªå…¬å¼æ¥è¿‘ä¼¼ä¼°è®¡sparkä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´ã€‚


$$ ä»»åŠ¡æ‰§è¡Œæ—¶é—´ â‰ˆ \frac{ä»»åŠ¡è®¡ç®—æ€»æ—¶é—´ + shuffleæ€»æ—¶é—´ + GCåƒåœ¾å›æ”¶æ€»æ—¶é—´} {ä»»åŠ¡æœ‰æ•ˆå¹¶è¡Œåº¦}$$


$$ ä»»åŠ¡æœ‰æ•ˆå¹¶è¡Œåº¦ â‰ˆ \frac{min(ä»»åŠ¡å¹¶è¡Œåº¦ï¼Œ partitionåˆ†åŒºæ•°é‡)} {æ•°æ®å€¾æ–œåº¦\times è®¡ç®—å€¾æ–œåº¦} $$

$$ ä»»åŠ¡å¹¶è¡Œåº¦ â‰ˆ executoræ•°é‡ \times æ¯ä¸ªexecutorçš„coreæ•°é‡ $$


å¯ä»¥ç”¨ä¸‹é¢äºŒä¸ªå…¬å¼æ¥è¯´æ˜sparkåœ¨executorä¸Šçš„å†…å­˜åˆ†é…ã€‚

$$ executorç”³è¯·çš„å†…å­˜ â‰ˆ å †å†…å†…å­˜(å †å†…å†…å­˜ç”±å¤šä¸ªcoreå…±äº«) + å †å¤–å†…å­˜ $$

$$ å †å†…å†…å­˜ â‰ˆ storageå†…å­˜+executionå†…å­˜+otherå†…å­˜ $$


å¦‚æœç¨‹åºæ‰§è¡Œå¤ªæ…¢ï¼Œè°ƒä¼˜çš„é¡ºåºä¸€èˆ¬å¦‚ä¸‹ï¼š

1ï¼Œé¦–å…ˆè°ƒæ•´ä»»åŠ¡å¹¶è¡Œåº¦ï¼Œå¹¶è°ƒæ•´partitionåˆ†åŒºã€‚

2ï¼Œå°è¯•å®šä½å¯èƒ½çš„é‡å¤è®¡ç®—ï¼Œå¹¶ä¼˜åŒ–ä¹‹ã€‚

3ï¼Œå°è¯•å®šä½æ•°æ®å€¾æ–œé—®é¢˜æˆ–è€…è®¡ç®—å€¾æ–œé—®é¢˜å¹¶ä¼˜åŒ–ä¹‹ã€‚

4ï¼Œå¦‚æœshuffleè¿‡ç¨‹æç¤ºå †å¤–å†…å­˜ä¸è¶³ï¼Œè€ƒè™‘è°ƒé«˜å †å¤–å†…å­˜ã€‚

5ï¼Œå¦‚æœå‘ç”ŸOOMæˆ–è€…GCè€—æ—¶è¿‡é•¿ï¼Œè€ƒè™‘æé«˜executor-memoryæˆ–é™ä½executor-coreã€‚


ä»¥ä¸‹æ˜¯å¯¹ä¸Šè¿°å…¬å¼ä¸­æ¶‰åŠåˆ°çš„ä¸€äº›æ¦‚å¿µçš„åˆæ­¥è§£è¯»ã€‚


* ä»»åŠ¡è®¡ç®—æ€»æ—¶é—´ï¼šå‡è®¾ç”±ä¸€å°æ— é™å†…å­˜çš„åŒç­‰CPUé…ç½®çš„å•æ ¸æœºå™¨æ‰§è¡Œè¯¥ä»»åŠ¡ï¼Œæ‰€éœ€è¦çš„è¿è¡Œæ—¶é—´ã€‚é€šè¿‡ç¼“å­˜é¿å…é‡å¤è®¡ç®—ï¼Œé€šè¿‡mapPartitionsä»£æ›¿mapä»¥å‡å°‘è¯¸å¦‚è¿æ¥æ•°æ®åº“ï¼Œé¢„å¤„ç†å¹¿æ’­å˜é‡ç­‰é‡å¤è¿‡ç¨‹ï¼Œéƒ½æ˜¯å‡å°‘ä»»åŠ¡è®¡ç®—æ€»æ—¶é—´çš„ä¾‹å­ã€‚


* shuffleæ€»æ—¶é—´ï¼šä»»åŠ¡å› ä¸ºreduceByKeyï¼Œjoinï¼ŒsortByç­‰shuffleç±»ç®—å­ä¼šè§¦å‘shuffleæ“ä½œäº§ç”Ÿçš„ç£ç›˜è¯»å†™å’Œç½‘ç»œä¼ è¾“çš„æ€»æ—¶é—´ã€‚shuffleæ“ä½œçš„ç›®çš„æ˜¯å°†åˆ†å¸ƒåœ¨é›†ç¾¤ä¸­å¤šä¸ªèŠ‚ç‚¹ä¸Šçš„åŒä¸€ä¸ªkeyçš„æ•°æ®ï¼Œæ‹‰å–åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹ä¸Šï¼Œä»¥ä¾¿è®©ä¸€ä¸ªèŠ‚ç‚¹å¯¹åŒä¸€ä¸ªkeyçš„æ‰€æœ‰æ•°æ®è¿›è¡Œç»Ÿä¸€å¤„ç†ã€‚ shuffleè¿‡ç¨‹é¦–å…ˆæ˜¯å‰ä¸€ä¸ªstageçš„ä¸€ä¸ªshuffle writeå³å†™ç£ç›˜è¿‡ç¨‹ï¼Œä¸­é—´æ˜¯ä¸€ä¸ªç½‘ç»œä¼ è¾“è¿‡ç¨‹ï¼Œç„¶åæ˜¯åä¸€ä¸ªstageçš„ä¸€ä¸ªshuffle readå³è¯»ç£ç›˜è¿‡ç¨‹ã€‚shuffleè¿‡ç¨‹æ—¢åŒ…æ‹¬ç£ç›˜è¯»å†™ï¼ŒåˆåŒ…æ‹¬ç½‘ç»œä¼ è¾“ï¼Œéå¸¸è€—æ—¶ã€‚å› æ­¤å¦‚æœ‰å¯èƒ½ï¼Œåº”å½“é¿å…ä½¿ç”¨shuffleç±»ç®—å­ã€‚ä¾‹å¦‚ç”¨map+broadcastçš„æ–¹å¼ä»£æ›¿joinè¿‡ç¨‹ã€‚é€€è€Œæ±‚å…¶æ¬¡ï¼Œä¹Ÿå¯ä»¥åœ¨shuffleä¹‹å‰å¯¹ç›¸åŒkeyçš„æ•°æ®è¿›è¡Œå½’å¹¶ï¼Œå‡å°‘shuffleè¯»å†™å’Œä¼ è¾“çš„æ•°æ®é‡ã€‚æ­¤å¤–ï¼Œè¿˜å¯ä»¥åº”ç”¨ä¸€äº›è¾ƒä¸ºé«˜æ•ˆçš„shuffleç®—å­æ¥ä»£æ›¿ä½æ•ˆçš„shuffleç®—å­ã€‚ä¾‹å¦‚ç”¨reduceByKey/aggregateByKeyæ¥ä»£æ›¿groupByKeyã€‚æœ€åï¼Œshuffleåœ¨è¿›è¡Œç½‘ç»œä¼ è¾“çš„è¿‡ç¨‹ä¸­ä¼šé€šè¿‡nettyä½¿ç”¨JVMå †å¤–å†…å­˜ï¼Œsparkä»»åŠ¡ä¸­å¤§è§„æ¨¡æ•°æ®çš„shuffleå¯èƒ½ä¼šå¯¼è‡´å †å¤–å†…å­˜ä¸è¶³ï¼Œå¯¼è‡´ä»»åŠ¡æŒ‚æ‰ï¼Œè¿™æ—¶å€™éœ€è¦åœ¨é…ç½®æ–‡ä»¶ä¸­è°ƒå¤§å †å¤–å†…å­˜ã€‚


* GCåƒåœ¾å›æ”¶æ€»æ—¶é—´ï¼šå½“JVMä¸­executionå†…å­˜ä¸è¶³æ—¶ï¼Œä¼šå¯åŠ¨GCåƒåœ¾å›æ”¶è¿‡ç¨‹ã€‚æ‰§è¡ŒGCè¿‡ç¨‹æ—¶å€™ï¼Œç”¨æˆ·çº¿ç¨‹ä¼šç»ˆæ­¢ç­‰å¾…ã€‚å› æ­¤å¦‚æœexecutionå†…å­˜ä¸å¤Ÿå……åˆ†ï¼Œä¼šè§¦å‘è¾ƒå¤šçš„GCè¿‡ç¨‹ï¼Œæ¶ˆè€—è¾ƒå¤šçš„æ—¶é—´ã€‚åœ¨spark2.0ä¹‹åexcutionå†…å­˜å’Œstorageå†…å­˜æ˜¯ç»Ÿä¸€åˆ†é…çš„ï¼Œä¸å¿…è°ƒæ•´excutionå†…å­˜å æ¯”ï¼Œå¯ä»¥æé«˜executor-memoryæ¥é™ä½è¿™ç§å¯èƒ½ã€‚æˆ–è€…å‡å°‘executor-coresæ¥é™ä½è¿™ç§å¯èƒ½(è¿™ä¼šå¯¼è‡´ä»»åŠ¡å¹¶è¡Œåº¦çš„é™ä½)ã€‚


* ä»»åŠ¡æœ‰æ•ˆå¹¶è¡Œåº¦ï¼šä»»åŠ¡å®é™…ä¸Šå¹³å‡è¢«å¤šå°‘ä¸ªcoreæ‰§è¡Œã€‚å®ƒé¦–å…ˆå–å†³äºå¯ç”¨çš„coreæ•°é‡ã€‚å½“partitionåˆ†åŒºæ•°é‡å°‘äºå¯ç”¨çš„coreæ•°é‡æ—¶ï¼Œåªä¼šæœ‰partitionåˆ†åŒºæ•°é‡çš„coreæ‰§è¡Œä»»åŠ¡ï¼Œå› æ­¤ä¸€èˆ¬è®¾ç½®åˆ†åŒºæ•°æ˜¯å¯ç”¨coreæ•°é‡çš„2å€ä»¥ä¸Š20å€ä»¥ä¸‹ã€‚æ­¤å¤–ä»»åŠ¡æœ‰æ•ˆå¹¶è¡Œåº¦ä¸¥é‡å—åˆ°æ•°æ®å€¾æ–œå’Œè®¡ç®—å€¾æ–œçš„å½±å“ã€‚æœ‰æ—¶å€™æˆ‘ä»¬ä¼šçœ‹åˆ°99%çš„partitionä¸Šçš„æ•°æ®å‡ åˆ†é’Ÿå°±æ‰§è¡Œå®Œæˆäº†ï¼Œä½†æ˜¯æœ‰1%çš„partitionä¸Šçš„æ•°æ®å´è¦æ‰§è¡Œå‡ ä¸ªå°æ—¶ã€‚è¿™æ—¶å€™ä¸€èˆ¬æ˜¯å‘ç”Ÿäº†æ•°æ®å€¾æ–œæˆ–è€…è®¡ç®—å€¾æ–œã€‚è¿™ä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬è¯´ï¼Œä»»åŠ¡å®é™…ä¸Šæœ‰æ•ˆçš„å¹¶è¡Œåº¦ä¼šå¾ˆä½ï¼Œå› ä¸ºåœ¨åé¢çš„è¿™å‡ ä¸ªå°æ—¶çš„ç»å¤§éƒ¨åˆ†æ—¶é—´ï¼Œåªæœ‰å¾ˆå°‘çš„å‡ ä¸ªcoreåœ¨æ‰§è¡Œä»»åŠ¡ã€‚


* ä»»åŠ¡å¹¶è¡Œåº¦ï¼šä»»åŠ¡å¯ç”¨coreçš„æ•°é‡ã€‚å®ƒç­‰äºç”³è¯·åˆ°çš„executoræ•°é‡å’Œæ¯ä¸ªexecutorçš„coreæ•°é‡çš„ä¹˜ç§¯ã€‚å¯ä»¥åœ¨spark-submitæ—¶å€™ç”¨num-executorå’Œexecutor-coresæ¥æ§åˆ¶å¹¶è¡Œåº¦ã€‚æ­¤å¤–ï¼Œä¹Ÿå¯ä»¥å¼€å¯spark.dynamicAllocation.enabledæ ¹æ®ä»»åŠ¡è€—æ—¶åŠ¨æ€å¢å‡executoræ•°é‡ã€‚è™½ç„¶æé«˜executor-coresä¹Ÿèƒ½å¤Ÿæé«˜å¹¶è¡Œåº¦ï¼Œä½†æ˜¯å½“è®¡ç®—éœ€è¦å ç”¨è¾ƒå¤§çš„å­˜å‚¨æ—¶ï¼Œä¸å®œè®¾ç½®è¾ƒé«˜çš„executor-coresæ•°é‡ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´executorå†…å­˜ä¸è¶³å‘ç”Ÿå†…å­˜æº¢å‡ºOOMã€‚


* partitionåˆ†åŒºæ•°é‡ï¼šåˆ†åŒºæ•°é‡è¶Šå¤§ï¼Œå•ä¸ªåˆ†åŒºçš„æ•°æ®é‡è¶Šå°ï¼Œä»»åŠ¡åœ¨ä¸åŒçš„coreä¸Šçš„æ•°é‡åˆ†é…ä¼šè¶Šå‡åŒ€ï¼Œæœ‰åŠ©äºæå‡ä»»åŠ¡æœ‰æ•ˆå¹¶è¡Œåº¦ã€‚ä½†partitionæ•°é‡è¿‡å¤§ï¼Œä¼šå¯¼è‡´æ›´å¤šçš„æ•°æ®åŠ è½½æ—¶é—´ï¼Œä¸€èˆ¬è®¾ç½®åˆ†åŒºæ•°æ˜¯å¯ç”¨coreæ•°é‡çš„2å€ä»¥ä¸Š20å€ä»¥ä¸‹ã€‚å¯ä»¥åœ¨spark-submitä¸­ç”¨spark.default.parallelismæ¥æ§åˆ¶RDDçš„é»˜è®¤åˆ†åŒºæ•°é‡ï¼Œå¯ä»¥ç”¨spark.sql.shuffle.partitionsæ¥æ§åˆ¶SparkSQLä¸­ç»™shuffleè¿‡ç¨‹çš„åˆ†åŒºæ•°é‡ã€‚


* æ•°æ®å€¾æ–œåº¦ï¼šæ•°æ®å€¾æ–œæŒ‡çš„æ˜¯æ•°æ®é‡åœ¨ä¸åŒçš„partitionä¸Šåˆ†é…ä¸å‡åŒ€ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œshuffleç®—å­å®¹æ˜“äº§ç”Ÿæ•°æ®å€¾æ–œç°è±¡ï¼ŒæŸä¸ªkeyä¸Šèšåˆçš„æ•°æ®é‡å¯èƒ½ä¼šç™¾ä¸‡åƒä¸‡ä¹‹å¤šï¼Œè€Œå¤§éƒ¨åˆ†keyèšåˆçš„æ•°æ®é‡å´åªæœ‰å‡ åå‡ ç™¾ä¸ªã€‚ä¸€ä¸ªpartitionä¸Šè¿‡å¤§çš„æ•°æ®é‡ä¸ä»…éœ€è¦è€—è´¹å¤§é‡çš„è®¡ç®—æ—¶é—´ï¼Œè€Œä¸”å®¹æ˜“å‡ºç°OOMã€‚å¯¹äºæ•°æ®å€¾æ–œï¼Œä¸€ç§ç®€å•çš„ç¼“è§£æ–¹æ¡ˆæ˜¯å¢å¤§partitionåˆ†åŒºæ•°é‡ï¼Œä½†ä¸èƒ½ä»æ ¹æœ¬ä¸Šè§£å†³é—®é¢˜ã€‚ä¸€ç§è¾ƒå¥½çš„è§£å†³æ–¹æ¡ˆæ˜¯åˆ©ç”¨éšæœºæ•°æ„é€ æ•°é‡ä¸ºåŸå§‹keyæ•°é‡1000å€çš„ä¸­é—´keyã€‚å¤§æ¦‚æ­¥éª¤å¦‚ä¸‹ï¼Œåˆ©ç”¨1åˆ°1000çš„éšæœºæ•°å’Œå½“å‰keyç»„åˆæˆä¸­é—´keyï¼Œä¸­é—´keyçš„æ•°æ®å€¾æ–œç¨‹åº¦åªæœ‰åŸæ¥çš„1/1000, å…ˆå¯¹ä¸­é—´keyæ‰§è¡Œä¸€æ¬¡shuffleæ“ä½œï¼Œå¾—åˆ°ä¸€ä¸ªæ•°æ®é‡å°‘å¾—å¤šçš„ä¸­é—´ç»“æœï¼Œç„¶åå†å¯¹æˆ‘ä»¬å…³å¿ƒçš„åŸå§‹keyè¿›è¡Œshuffleï¼Œå¾—åˆ°ä¸€ä¸ªæœ€ç»ˆç»“æœã€‚


* è®¡ç®—å€¾æ–œåº¦ï¼šè®¡ç®—å€¾æ–œæŒ‡çš„æ˜¯ä¸åŒpartitionä¸Šçš„æ•°æ®é‡ç›¸å·®ä¸å¤§ï¼Œä½†æ˜¯è®¡ç®—è€—æ—¶ç›¸å·®å·¨å¤§ã€‚è€ƒè™‘è¿™æ ·ä¸€ä¸ªä¾‹å­ï¼Œæˆ‘ä»¬çš„RDDçš„æ¯ä¸€è¡Œæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæˆ‘ä»¬è¦è®¡ç®—æ¯ä¸€è¡Œä¸­è¿™ä¸ªåˆ—è¡¨ä¸­çš„æ•°ä¸¤ä¸¤ä¹˜ç§¯ä¹‹å’Œï¼Œè¿™ä¸ªè®¡ç®—çš„å¤æ‚åº¦æ˜¯å’Œåˆ—è¡¨é•¿åº¦çš„å¹³æ–¹æˆæ­£æ¯”çš„ï¼Œå› æ­¤å¦‚æœæœ‰ä¸€ä¸ªåˆ—è¡¨çš„é•¿åº¦æ˜¯å…¶å®ƒåˆ—è¡¨å¹³å‡é•¿åº¦çš„10å€ï¼Œé‚£ä¹ˆè®¡ç®—è¿™ä¸€è¡Œçš„æ—¶é—´å°†ä¼šæ˜¯å…¶å®ƒåˆ—è¡¨çš„100å€ï¼Œä»è€Œäº§ç”Ÿè®¡ç®—å€¾æ–œã€‚è®¡ç®—å€¾æ–œå’Œæ•°æ®å€¾æ–œçš„è¡¨ç°éå¸¸ç›¸ä¼¼ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°99%çš„partitionä¸Šçš„æ•°æ®å‡ åˆ†é’Ÿå°±æ‰§è¡Œå®Œæˆäº†ï¼Œä½†æ˜¯æœ‰1%çš„partitionä¸Šçš„æ•°æ®å´è¦æ‰§è¡Œå‡ ä¸ªå°æ—¶ã€‚è®¡ç®—å€¾æ–œå’Œshuffleæ— å…³ï¼Œåœ¨mapç«¯å°±å¯ä»¥å‘ç”Ÿã€‚è®¡ç®—å€¾æ–œå‡ºç°åï¼Œä¸€èˆ¬å¯ä»¥é€šè¿‡èˆå»æç«¯æ•°æ®æˆ–è€…æ”¹å˜è®¡ç®—æ–¹æ³•ä¼˜åŒ–æ€§èƒ½ã€‚


* å †å†…å†…å­˜ï¼šon-heap memory, å³Javaè™šæ‹Ÿæœºç›´æ¥ç®¡ç†çš„å­˜å‚¨ï¼Œç”±JVMè´Ÿè´£åƒåœ¾å›æ”¶GCã€‚ç”±å¤šä¸ªcoreå…±äº«ï¼Œcoreè¶Šå¤šï¼Œæ¯ä¸ªcoreå®é™…èƒ½ä½¿ç”¨çš„å†…å­˜è¶Šå°‘ã€‚coreè®¾ç½®å¾—è¿‡å¤§å®¹æ˜“å¯¼è‡´OOMï¼Œå¹¶ä½¿å¾—GCæ—¶é—´å¢åŠ ã€‚


* å †å¤–å†…å­˜ï¼šoff-heap memory, ä¸å—JVMç®¡ç†çš„å†…å­˜,  å¯ä»¥ç²¾ç¡®æ§åˆ¶ç”³è¯·å’Œé‡Šæ”¾, æ²¡æœ‰GCé—®é¢˜ã€‚ä¸€èˆ¬shuffleè¿‡ç¨‹åœ¨è¿›è¡Œç½‘ç»œä¼ è¾“çš„è¿‡ç¨‹ä¸­ä¼šé€šè¿‡nettyä½¿ç”¨åˆ°å †å¤–å†…å­˜ã€‚

<!-- #endregion -->

```python

```

### äºŒï¼ŒSparkä»»åŠ¡UIç›‘æ§


Sparkä»»åŠ¡å¯åŠ¨åï¼Œå¯ä»¥åœ¨æµè§ˆå™¨ä¸­è¾“å…¥ http://localhost:4040/ è¿›å…¥åˆ°spark web UI ç›‘æ§ç•Œé¢ã€‚

è¯¥ç•Œé¢ä¸­å¯ä»¥ä»å¤šä¸ªç»´åº¦ä»¥ç›´è§‚çš„æ–¹å¼éå¸¸ç»†ç²’åº¦åœ°æŸ¥çœ‹Sparkä»»åŠ¡çš„æ‰§è¡Œæƒ…å†µï¼ŒåŒ…æ‹¬ä»»åŠ¡è¿›åº¦ï¼Œè€—æ—¶åˆ†æï¼Œå­˜å‚¨åˆ†æï¼Œshuffleæ•°æ®é‡å¤§å°ç­‰ã€‚

æœ€å¸¸æŸ¥çœ‹çš„é¡µé¢æ˜¯ Stagesé¡µé¢å’ŒExcutorsé¡µé¢ã€‚

Jobsï¼š
æ¯ä¸€ä¸ªActionæ“ä½œå¯¹åº”ä¸€ä¸ªJobï¼Œä»¥Jobç²’åº¦æ˜¾ç¤ºApplicationè¿›åº¦ã€‚æœ‰æ—¶é—´è½´Timelineã€‚

![](./data/Spark-UI-jobs.png)

<br>
<br>

<!-- #region -->
**Stages**ï¼š
Jobåœ¨é‡åˆ°shuffleåˆ‡å¼€Stageï¼Œæ˜¾ç¤ºæ¯ä¸ªStageè¿›åº¦ï¼Œä»¥åŠshuffleæ•°æ®é‡ã€‚

![](./data/Spark-Stagesé¡µé¢.png)

<br>
<br>

å¯ä»¥ç‚¹å‡»æŸä¸ªStageè¿›å…¥è¯¦æƒ…é¡µï¼ŒæŸ¥çœ‹å…¶ä¸‹é¢æ¯ä¸ªTaskçš„æ‰§è¡Œæƒ…å†µä»¥åŠå„ä¸ªpartitionæ‰§è¡Œçš„è´¹æ—¶ç»Ÿè®¡ã€‚

![](./data/Spark-Stagesç»†ç²’åº¦.png)


Storage:  

ç›‘æ§cacheæˆ–è€…persistå¯¼è‡´çš„æ•°æ®å­˜å‚¨å¤§å°ã€‚


Environment:  
æ˜¾ç¤ºsparkå’Œscalaç‰ˆæœ¬ï¼Œä¾èµ–çš„å„ç§jaråŒ…åŠå…¶ç‰ˆæœ¬ã€‚


**Excutors** : 
ç›‘æ§å„ä¸ªExcutorsçš„å­˜å‚¨å’Œshuffleæƒ…å†µã€‚

![](./data/Spark-UI-executorsé¡µé¢.png)

SQL: 
æ˜¾ç¤ºå„ç§SQLå‘½ä»¤åœ¨é‚£äº›Jobsä¸­è¢«æ‰§è¡Œã€‚


<!-- #endregion -->

```python

```

### ä¸‰ï¼ŒSparkè°ƒä¼˜æ¡ˆä¾‹


ä¸‹é¢ä»‹ç»å‡ ä¸ªè°ƒä¼˜çš„å…¸å‹æ¡ˆä¾‹ï¼š

1ï¼Œèµ„æºé…ç½®ä¼˜åŒ–

2ï¼Œåˆ©ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—

3ï¼Œæ•°æ®å€¾æ–œè°ƒä¼˜ 

4ï¼Œbroadcast+mapä»£æ›¿join

5ï¼ŒreduceByKey/aggregateByKeyä»£æ›¿groupByKey


<!-- #region -->
**1ï¼Œèµ„æºé…ç½®ä¼˜åŒ–**

ä¸‹é¢æ˜¯ä¸€ä¸ªèµ„æºé…ç½®çš„ä¾‹å­ï¼š


ä¼˜åŒ–å‰ï¼š

```bash
#æäº¤pythonå†™çš„ä»»åŠ¡
spark-submit --master yarn \
--deploy-mode cluster \
--executor-memory 12G \
--driver-memory 12G \
--num-executors 100 \
--executor-cores 8 \
--conf spark.yarn.maxAppAttempts=2 \
--conf spark.task.maxFailures=10 \
--conf spark.stage.maxConsecutiveAttempts=10 \
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./anaconda3.zip/anaconda3/bin/python #æŒ‡å®šexcutorsçš„Pythonç¯å¢ƒ
--conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON = ./anaconda3.zip/anaconda3/bin/python  #clusteræ¨¡å¼æ—¶å€™è®¾ç½®
--archives viewfs:///user/hadoop-xxx/yyy/anaconda3.zip #ä¸Šä¼ åˆ°hdfsçš„Pythonç¯å¢ƒ
--files  data.csv,profile.txt
--py-files  pkg.py,tqdm.py
pyspark_demo.py 
```


ä¼˜åŒ–åï¼š

```bash
#æäº¤pythonå†™çš„ä»»åŠ¡
spark-submit --master yarn \
--deploy-mode cluster \
--executor-memory 12G \
--driver-memory 12G \
--num-executors 100 \
--executor-cores 2 \
--conf spark.yarn.maxAppAttempts=2 \
--conf spark.default.parallelism=1600 \
--conf spark.sql.shuffle.partitions=1600 \
--conf spark.memory.offHeap.enabled=true \
--conf spark.memory.offHeap.size=2g\
--conf spark.task.maxFailures=10 \
--conf spark.stage.maxConsecutiveAttempts=10 \
--conf spark.yarn.appMasterEnv.PYSPARK_PYTHON=./anaconda3.zip/anaconda3/bin/python #æŒ‡å®šexcutorsçš„Pythonç¯å¢ƒ
--conf spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON = ./anaconda3.zip/anaconda3/bin/python  #clusteræ¨¡å¼æ—¶å€™è®¾ç½®
--archives viewfs:///user/hadoop-xxx/yyy/anaconda3.zip #ä¸Šä¼ åˆ°hdfsçš„Pythonç¯å¢ƒ
--files  data.csv,profile.txt
--py-files  pkg.py,tqdm.py
pyspark_demo.py 
```

è¿™é‡Œä¸»è¦å‡å°äº† executor-coresæ•°é‡ï¼Œä¸€èˆ¬è®¾ç½®ä¸º1~4ï¼Œè¿‡å¤§çš„æ•°é‡å¯èƒ½ä¼šé€ æˆæ¯ä¸ªcoreè®¡ç®—å’Œå­˜å‚¨èµ„æºä¸è¶³äº§ç”ŸOOMï¼Œä¹Ÿä¼šå¢åŠ GCæ—¶é—´ã€‚
æ­¤å¤–ä¹Ÿå°†é»˜è®¤åˆ†åŒºæ•°è°ƒåˆ°äº†1600ï¼Œå¹¶è®¾ç½®äº†2Gçš„å †å¤–å†…å­˜ã€‚


<!-- #endregion -->

**2, åˆ©ç”¨ç¼“å­˜å‡å°‘é‡å¤è®¡ç®—**

```python
%%time
# ä¼˜åŒ–å‰:
import math 
rdd_x = sc.parallelize(range(0,2000000,3),3)
rdd_y = sc.parallelize(range(2000000,4000000,2),3)
rdd_z = sc.parallelize(range(4000000,6000000,2),3)
rdd_data = rdd_x.union(rdd_y).union(rdd_z).map(lambda x:math.tan(x))
s = rdd_data.reduce(lambda a,b:a+b+0.0)
n = rdd_data.count()
mean = s/n 
print(mean)

```

```
-1.889935655259299
CPU times: user 40.2 ms, sys: 12.4 ms, total: 52.6 ms
Wall time: 2.76 s
```

```python
%%time 
# ä¼˜åŒ–å: 
import math 
from  pyspark.storagelevel import StorageLevel
rdd_x = sc.parallelize(range(0,2000000,3),3)
rdd_y = sc.parallelize(range(2000000,4000000,2),3)
rdd_z = sc.parallelize(range(4000000,6000000,2),3)
rdd_data = rdd_x.union(rdd_y).union(rdd_z).map(lambda x:math.tan(x)).persist(StorageLevel.MEMORY_AND_DISK)

s = rdd_data.reduce(lambda a,b:a+b+0.0)
n = rdd_data.count()
mean = s/n 
rdd_data.unpersist()
print(mean)

```

```
-1.889935655259299
CPU times: user 40.5 ms, sys: 11.5 ms, total: 52 ms
Wall time: 2.18 s
```

```python

```

**3, æ•°æ®å€¾æ–œè°ƒä¼˜**

```python
%%time 
# ä¼˜åŒ–å‰: 
rdd_data = sc.parallelize(["hello world"]*1000000+["good morning"]*10000+["I love spark"]*10000)
rdd_word = rdd_data.flatMap(lambda x:x.split(" "))
rdd_one = rdd_word.map(lambda x:(x,1))
rdd_count = rdd_one.reduceByKey(lambda a,b:a+b+0.0)
print(rdd_count.collect()) 

```

```
[('good', 10000.0), ('hello', 1000000.0), ('spark', 10000.0), ('world', 1000000.0), ('love', 10000.0), ('morning', 10000.0), ('I', 10000.0)]
CPU times: user 285 ms, sys: 27.6 ms, total: 313 ms
Wall time: 2.74 s
```

```python
%%time 
# ä¼˜åŒ–å: 
import random 
rdd_data = sc.parallelize(["hello world"]*1000000+["good morning"]*10000+["I love spark"]*10000)
rdd_word = rdd_data.flatMap(lambda x:x.split(" "))
rdd_one = rdd_word.map(lambda x:(x,1))
rdd_mid_key = rdd_one.map(lambda x:(x[0]+"_"+str(random.randint(0,999)),x[1]))
rdd_mid_count = rdd_mid_key.reduceByKey(lambda a,b:a+b+0.0)
rdd_count = rdd_mid_count.map(lambda x:(x[0].split("_")[0],x[1])).reduceByKey(lambda a,b:a+b+0.0)
print(rdd_count.collect())  

#ä½œè€…æŒ‰ï¼šæ­¤å¤„ä»…ç¤ºèŒƒåŸç†ï¼Œå•æœºä¸Šè¯¥ä¼˜åŒ–æ–¹æ¡ˆéš¾ä»¥è·å¾—æ€§èƒ½ä¼˜åŠ¿

```

```
[('good', 10000.0), ('hello', 1000000.0), ('spark', 10000.0), ('world', 1000000.0), ('love', 10000.0), ('morning', 10000.0), ('I', 10000.0)]
CPU times: user 351 ms, sys: 51 ms, total: 402 ms
Wall time: 7 s
```

```python

```

**4, broadcast+mapä»£æ›¿join**


è¯¥ä¼˜åŒ–ç­–ç•¥ä¸€èˆ¬é™äºæœ‰ä¸€ä¸ªå‚ä¸joinçš„rddçš„æ•°æ®é‡ä¸å¤§çš„æƒ…å†µã€‚


```python
%%time 
# ä¼˜åŒ–å‰:

rdd_age = sc.parallelize([("LiLei",18),("HanMeimei",19),("Jim",17),("LiLy",20)])
rdd_gender = sc.parallelize([("LiLei","male"),("HanMeimei","female"),("Jim","male"),("LiLy","female")])
rdd_students = rdd_age.join(rdd_gender).map(lambda x:(x[0],x[1][0],x[1][1]))

print(rdd_students.collect())

```

```
[('LiLy', 20, 'female'), ('LiLei', 18, 'male'), ('HanMeimei', 19, 'female'), ('Jim', 17, 'male')]
CPU times: user 43.9 ms, sys: 11.6 ms, total: 55.6 ms
Wall time: 307 ms
```

```python
%%time 

# ä¼˜åŒ–å:
rdd_age = sc.parallelize([("LiLei",18),("HanMeimei",19),("Jim",17),("LiLy",20)])
rdd_gender = sc.parallelize([("LiLei","male"),("HanMeimei","female"),("Jim","male"),("LiLy","female")],2)
ages = rdd_age.collect()
broads = sc.broadcast(ages)

def get_age(it):
    result = []
    ages = dict(broads.value)
    for x in it:
        name = x[0]
        age = ages.get(name,0)
        result.append((x[0],age,x[1]))
    return iter(result)

rdd_students = rdd_gender.mapPartitions(get_age)

print(rdd_students.collect())

```

```
[('LiLei', 18, 'male'), ('HanMeimei', 19, 'female'), ('Jim', 17, 'male'), ('LiLy', 20, 'female')]
CPU times: user 14.3 ms, sys: 7.43 ms, total: 21.7 ms
Wall time: 86.3 ms
```

```python

```

**5ï¼ŒreduceByKey/aggregateByKeyä»£æ›¿groupByKey**


groupByKeyç®—å­æ˜¯ä¸€ä¸ªä½æ•ˆçš„ç®—å­ï¼Œå…¶ä¼šäº§ç”Ÿå¤§é‡çš„shuffleã€‚å…¶åŠŸèƒ½å¯ä»¥ç”¨reduceByKeyå’ŒaggreagateByKeyä»£æ›¿ï¼Œé€šè¿‡åœ¨æ¯ä¸ªpartitionå†…éƒ¨å…ˆåšä¸€æ¬¡æ•°æ®çš„åˆå¹¶æ“ä½œï¼Œå¤§å¤§å‡å°‘äº†shuffleçš„æ•°æ®é‡ã€‚


```python
%%time 
# ä¼˜åŒ–å‰:
rdd_students = sc.parallelize([("class1","LiLei"),("class2","HanMeimei"),("class1","Lucy"),
                               ("class1","Ann"),("class1","Jim"),("class2","Lily")])
rdd_names = rdd_students.groupByKey().map(lambda t:(t[0],list(t[1])))
names = rdd_names.collect()
print(names)

```

```
[('class1', ['LiLei', 'Lucy', 'Ann', 'Jim']), ('class2', ['HanMeimei', 'Lily'])]
CPU times: user 25.3 ms, sys: 7.32 ms, total: 32.6 ms
Wall time: 164 ms
```

```python
%%time 
# ä¼˜åŒ–å:
rdd_students = sc.parallelize([("class1","LiLei"),("class2","HanMeimei"),("class1","Lucy"),
                               ("class1","Ann"),("class1","Jim"),("class2","Lily")])
rdd_names = rdd_students.aggregateByKey([],lambda arr,name:arr+[name],lambda arr1,arr2:arr1+arr2)

names = rdd_names.collect()
print(names)
```

```
[('class1', ['LiLei', 'Lucy', 'Ann', 'Jim']), ('class2', ['HanMeimei', 'Lily'])]
CPU times: user 21.6 ms, sys: 6.63 ms, total: 28.3 ms
Wall time: 118 ms
```

```python

```

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"ç®—æ³•ç¾é£Ÿå±‹"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚

ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**sparkåŠ ç¾¤**ï¼ŒåŠ å…¥sparkå’Œå¤§æ•°æ®è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

![image.png](./data/ç®—æ³•ç¾é£Ÿå±‹äºŒç»´ç .jpg)
